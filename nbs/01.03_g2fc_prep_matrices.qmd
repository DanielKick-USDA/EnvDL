---
title: Data Prep
jupyter: python3
---

```{python}
import os
import tqdm

import numpy as np
import pandas as pd
pd.set_option('display.max_columns', None)
 
from sklearn import preprocessing # LabelEncoder
from sklearn.metrics import mean_squared_error # if squared=False; RMSE

from EnvDL.core import *
from EnvDL.dna import *
```

```{python}
cache_path = '../nbs_artifacts/01.03_g2fc_prep_matrices/'
ensure_dir_path_exists(dir_path = cache_path)
```

```{python}
load_from = '../nbs_artifacts/01.02_g2fc_imputation/'

meta = pd.read_csv(load_from+'meta.csv')

phno = pd.read_csv(load_from+'phno.csv')
soil = pd.read_csv(load_from+'soil.csv')
wthr = pd.read_csv(load_from+'wthr.csv')

cgmv = pd.read_csv(load_from+'cgmv.csv')
```

```{python}
temp = phno.groupby(['Env', 'Year', 'Hybrid', 'Replicate', 'Block', 'Plot']).count().reset_index()
assert 1 == np.max(temp.Yield_Mg_ha)
```

```{python}
mask = ((phno.Yield_Mg_ha.notna())) # This used to allow for missing yield so long as they were in 
# 2022. Now that the 2022 data is available they should be excluded.
phno = phno.loc[mask, :].reset_index(drop = True)
phno = phno.loc[:, ['Env', 'Year', 'Hybrid', 'Replicate', 'Block', 'Plot', 'Yield_Mg_ha']]
phno = phno.drop_duplicates().reset_index(drop = True).copy()
```



## Prep CVs

```{python}
# YMat[phno.Year == 2021]
```

## Indexing

### Filtered Phenotype and lookup
There are huge data savings to be had from keeping only deduplciated genomic data in memory in the GPU. 

```{python}
uniq_hybrids = list(set(phno.Hybrid))
uniq_hybrids[0:3]
```

```{python}
# check snp records and only keep phenotypes with snp data
snps_found = [exists_geno((taxa_to_filename(taxa = ith_hybrid))) for ith_hybrid in  uniq_hybrids]
```

```{python}
temp = pd.DataFrame(zip(uniq_hybrids, snps_found), columns=['Hybrid', 'SNPS'])
temp.loc[(temp.SNPS != True)]
```

```{python}
print('Genotypes flagged as missing that exist with different capitalization:')
missing_hybrids = temp.loc[(temp.SNPS != True), ['Hybrid']]
# quick check that some of these aren't just differing in capitalization
missing_hybrids_list = [e.replace('/', '__').lower() for e in list(missing_hybrids.Hybrid)]
print(
    [e for e in [e.lower for e in os.listdir('../data/zma/g2fc/genotypes/snps/')] if e in missing_hybrids_list]
)
```

```{python}
print(
    str(missing_hybrids.merge(phno, how = 'left').shape[0]
       )+' observations ('+str(round(100 * (missing_hybrids.merge(phno, how = 'left').shape[0]/phno.shape[0]), 3)
       )+'%) missing genotype.'
)
```

```{python}
# limit to phno with snp data
phno_geno = temp.loc[(temp.SNPS == True)].merge(phno, how = 'left')
phno_geno
```

```{python}
# restrict to observations with snp data
phno_geno = phno.merge(phno_geno.loc[:, ['Hybrid', 'SNPS']].drop_duplicates(), how  = 'left')
phno_geno = phno_geno.loc[(phno_geno.SNPS == True)].drop(columns = ['SNPS']).reset_index(drop=True).copy()
phno_geno
```

```{python}
assert phno_geno.Yield_Mg_ha.isna().sum() == 0
```

```{python}
# phno_geno.to_csv(cache_path+'phno_geno_filter.csv', index=False)

phno_geno = phno_geno.reset_index().rename(columns = {'index':'Phno_Idx'})
```











```{python}
Env_Idxs  = phno_geno.loc[:, ['Env']
                    ].drop_duplicates(
                    ).reset_index(drop = True
                    ).reset_index(
                    ).rename(columns = {'index':'Env_Idx'})
Geno_Idxs = phno_geno.loc[:, ['Hybrid']
                    ].drop_duplicates(
                    ).reset_index(drop = True
                    ).reset_index(
                    ).rename(columns = {'index':'Geno_Idx'})
```

```{python}
# This is key. I'm explictly adding an index column to fix frustrations jumping between sorted versions of the df.

phno_geno = phno_geno.merge(Env_Idxs).merge(Geno_Idxs).sort_values('Phno_Idx').reset_index(drop = True)
```

```{python}
phno_geno = phno_geno.loc[:, [ 'Env', 'Year', 'Hybrid', 'Yield_Mg_ha', 
                              'Replicate', 'Block', 'Plot', 
                              'Phno_Idx', 'Env_Idx', 'Geno_Idx']]

phno_geno
```

```{python}
phno_geno.to_csv(cache_path+'phno_geno.csv', index=False)
```

```{python}
# and overwrite phno to prevent incomplete data from creeping in.
phno = phno_geno.copy()
```

### Observation to Planting/Harvest DOY

```{python}
# use phno_index as lookup

mgmt_time = phno.merge(
    meta.loc[:, ['Env', 'Hybrid', 'Replicate', 'Block', 'Plot', 'Date_Planted','Date_Harvested']
       ].drop_duplicates(), how = 'left')

mgmt_time['DOY_Planted'] = round(mgmt_time['Date_Planted']).astype(int)
mgmt_time['DOY_Harvested'] = round(mgmt_time['Date_Harvested']).astype(int)

mgmt_time = mgmt_time.loc[:, ['DOY_Planted','DOY_Harvested']]
mgmt_time
```

```{python}
PlantHarvest = np.array(mgmt_time.loc[:, ])
np.save(cache_path+'PlantHarvestNames.npy', list(mgmt_time))
np.save(cache_path+'PlantHarvest.npy', PlantHarvest)
```

### Observation to Metadata

```{python}
meta_small = phno.loc[:, ['Env', 'Env_Idx']].drop_duplicates().merge(meta.loc[:, [
    'Env',
 'Pounds_Needed_Soil_Moisture',
 'Cover_beet',
 'Cover_corn',
 'Cover_cotton',
 'Cover_fallow',
 'Cover_lima',
 'Cover_peanut',
 'Cover_pumpkin',
 'Cover_rye',
 'Cover_sorghum',
 'Cover_soy',
 'Cover_wheat',
 'Pre_Chisel',
 'Pre_Cult',
 'Pre_Disc',
 'Pre_MinTill']].drop_duplicates(), how = 'left').sort_values('Env_Idx')

meta_small
```

```{python}
mgmtMat = np.array(meta_small.loc[:, ].drop(columns = ['Env', 'Env_Idx']))
np.save(cache_path+'mgmtMatNames.npy', list(meta_small.loc[:, ].drop(columns = ['Env', 'Env_Idx'])))
np.save(cache_path+'mgmtMat.npy', mgmtMat)
```

### Observation to Genotype

```{python}
uniq_geno = phno_geno.loc[:, ['Geno_Idx']].drop_duplicates().reset_index().rename(columns = {'index':'Is_Phno_Idx'})
obs_geno_lookup = phno_geno.loc[:, ['Phno_Idx', 'Geno_Idx']].merge(uniq_geno).sort_values('Phno_Idx').reset_index(drop = True).copy()
obs_geno_lookup
```

```{python}
# save out obs_geno_lookup
np.save(cache_path+'obs_geno_lookup.npy', np.asarray(obs_geno_lookup))
obs_geno_lookup.to_csv(cache_path+'obs_geno_lookup.csv', index=False)
```

### Observation to Environment

```{python}
uniq_env = phno_geno.loc[:, ['Env_Idx']].drop_duplicates().reset_index().rename(columns = {'index':'Is_Phno_Idx'})
obs_env_lookup = phno_geno.loc[:, ['Phno_Idx', 'Env_Idx']].merge(uniq_env).sort_values('Phno_Idx').reset_index(drop = True).copy()
obs_env_lookup
```

```{python}
# save out obs_env_lookup
np.save(cache_path+'obs_env_lookup.npy', np.asarray(obs_env_lookup))
obs_env_lookup.to_csv(cache_path+'obs_env_lookup.csv', index=False)
```

## Y

```{python}
YMat = np.array(phno_geno.Yield_Mg_ha)
np.save(cache_path+'YMat.npy', YMat)
```

## Genomic Data

### One Hot Encode G (Deduplicated)

```{python}
# If phno and GMat observations are 1:1, then GMat is 135793, 2250. After deduplicating this will become 4926, 2250. That's only 3.6%!
# Doing this removes several hybrids so it's actually a little smaller 4926, 2203
```

```{python}
obs_geno_lookup.head()
```

```{python}
temp = obs_geno_lookup.drop(columns = 'Phno_Idx').drop_duplicates().reset_index(drop=True)

mask = phno_geno.index.isin(list(obs_geno_lookup['Is_Phno_Idx'].drop_duplicates()))
temp = phno_geno.loc[mask, ]#'Hybrid']
temp = temp.reset_index(drop = True) # drop so that the index in the DataFrame and ndarray match
temp
```

```{python}
temp = pd.concat([temp, temp.Hybrid.str.split('/', expand=True)], axis=1
        ).rename(columns = {0:'P0', 1:'P1'})

uniq_parents = list(set(pd.concat([temp['P0'], temp['P1']])))
```

```{python}
if os.path.exists(cache_path+'GMat.npy'):
    GMat = np.load(cache_path+'GMat.npy')
else:
    GMat = np.zeros([temp.shape[0], len(uniq_parents)])

    for j in tqdm.tqdm(range(len(uniq_parents))):
        for parent in ['P0', 'P1']:
            mask = (temp[parent] == uniq_parents[j]) 
            GMat[temp.loc[mask, ].index, j] += 1
            
    np.save(cache_path+'GMat.npy', GMat)
```

```{python}
np.save(cache_path+'GMatNames.npy', uniq_parents)
```

```{python}
# confirm there are two parents encoded for each observation
assert 2 == np.min(np.sum(GMat, axis = 1))
```

### Nucleotides

```{python}
# Convert IUPAC codes into one hot
# https://www.bioinformatics.org/sms/iupac.html
IUPAC = ['unk', 'A', 'C', 'G', 'T', 'K', 'M', 'N', 'R', 'S', 'W', 'Y']
IUPAC = dict(zip(IUPAC, [i for i in range(len(IUPAC))])) # begin with unk.
IUPAC
```

```{python}
put_cached_result(cache_path+'ACGT_OneHot_dict.pkl', IUPAC)
```

```{python}
if os.path.exists(cache_path+'ACGT_OneHot.npy'):
#     ACGT = np.load(cache_path+'ACGT_OneHot.npy')
    pass
else:
    temp = obs_geno_lookup.drop(columns = 'Phno_Idx').drop_duplicates().reset_index(drop=True)

    # use the first Hybrid to figure out what the length of the sequence is.
    i = 0
    phno_idx = temp.loc[i, 'Is_Phno_Idx']
    ith_hybrid = phno.loc[phno_idx, 'Hybrid']
    geno_seq_len = len(get_geno(taxa_to_filename(taxa = ith_hybrid))[1:])    

    # setup ndarray to hold data
    ACGT = np.ndarray(shape = (temp.shape[0],        # obs
                               geno_seq_len,         # values
                               len(IUPAC.keys())+1)) # channels

    # This is inefficient but needs only to be run once.
    # go over observations, channels, fill in values

    for i in tqdm.tqdm(temp.index): 
        phno_idx = temp.loc[i, 'Is_Phno_Idx']
        ith_hybrid = phno.loc[phno_idx, 'Hybrid']
        res = get_geno(taxa_to_filename(taxa = ith_hybrid)) 
        res = res[1:] # drop taxa

        res = [e.strip('\n') for e in res] # remove end of line

        for key in IUPAC.keys():
            IUPAC_mask = [True if e == key else False for e in res]
            ACGT[i, 
                 IUPAC_mask, 
                 IUPAC[key]] = 1  

    # Swap axes to match pytorch convention
    ACGT = np.swapaxes(ACGT, 1, 2)

    np.save(cache_path+'ACGT_OneHot.npy', ACGT)
```

### Nucleotide percents

```{python}
if os.path.exists(cache_path+'ACGT.npy'):
#     ACGT = np.load(cache_path+'ACGT.npy')
    pass
else:
    temp = obs_geno_lookup.drop(columns = 'Phno_Idx').drop_duplicates().reset_index(drop=True)

    
    # use the first Hybrid to figure out what the length of the sequence is.
    i = 0
    phno_idx = temp.loc[i, 'Is_Phno_Idx']
    ith_hybrid = phno.loc[phno_idx, 'Hybrid']
    geno_seq_len = len(get_geno(taxa_to_filename(taxa = ith_hybrid))[1:])    
    
    # setup ndarray to hold data
    ACGT = np.ndarray(shape = (temp.shape[0], geno_seq_len, 4))

    for i in tqdm.tqdm(temp.index):
        phno_idx = temp.loc[i, 'Is_Phno_Idx']
        ith_hybrid = phno.loc[phno_idx, 'Hybrid']
        res = get_geno(taxa_to_filename(taxa = ith_hybrid)) 
        res = res[1:] # drop taxa
        res = list_to_ACGT(in_seq = res)
        ACGT[i, :, :] = res[None, :, :]
        
    # Swap axes to match pytorch convention
    # (4926, 125891, 4)
    ACGT = np.swapaxes(ACGT, 1, 2)

    # set missings to 0 
    ACGT[np.isnan(ACGT)] = 0

    np.save(cache_path+'ACGT.npy', ACGT)
```

### Hilbert Nucleotide percents

```{python}
if os.path.exists(cache_path+'ACGT_hilb.npy'):
#     ACGT_hilb = np.load(cache_path+'ACGT_hilb.npy')
    pass
else:
    ACGT_hilb = np_3d_to_hilbert( np.swapaxes(ACGT, 1, 2) ) # swap channels back to dim 2 before running

    # ACGT_hilb.shape
    # (4926, 256, 512, 4)
    # Pytorch standard has channels second
    ACGT_hilb = np.swapaxes(ACGT_hilb, 1, 3)
    ACGT_hilb = np.swapaxes(ACGT_hilb, 2, 3)

    # set missings to 0
    ACGT_hilb[np.isnan(ACGT_hilb)] = 0

    np.save(cache_path+'ACGT_hilb.npy', ACGT_hilb)
```

## Environmental Data

### Make Environment to Observation Lookup 

```{python}
obs_env_lookup_small = obs_env_lookup.drop(columns = 'Phno_Idx'
                                    ).drop_duplicates(
                                    ).sort_values('Env_Idx'
                                    ).reset_index(drop = True)

obs_env_lookup_small.head()
```

### Make S Matrix

```{python}
SMat = phno.loc[:, ['Env']].merge(soil.drop(columns = ['Unnamed: 0', 'Year'])).drop(columns = ['Env'])
SMatNames = list(SMat)
SMat = np.array(SMat)
```

```{python}
SMat.shape
```

```{python}
SMatSmall = np.zeros([
    obs_env_lookup_small.shape[0], # unique environments
    len(SMatNames)                 # Cin
])
```

```{python}
for i in obs_env_lookup_small.index:
    ii = obs_env_lookup_small.loc[(obs_env_lookup_small.Env_Idx == i), 'Is_Phno_Idx']
    ii = int(ii)
    SMatSmall[i, ] = SMat[ii, ]
```

```{python}
# overwrite with much smaller version (rows from 135793 -> 236)
SMat = SMatSmall.copy()
```

### Make W Matrix

```{python}
# Input: (N,Cin,Lin)(N,Cin,Lin) or (Cin,Lin)(Cin,Lin)
wthr.DOY = wthr.DOY.astype(int)
```

```{python}
WMatNames = list(wthr.drop(columns = ['Unnamed: 0', 'Env', 'Year', 'Date', 'DOY']))
WMat = np.zeros([                  # Pytorch uses
    obs_env_lookup_small.shape[0], # N
    len(WMatNames),                # Cin
    np.max(wthr.DOY)               # Lin
])
```

```{python}
for i in tqdm.tqdm(obs_env_lookup_small.index):
    ii = obs_env_lookup_small.loc[(obs_env_lookup_small.Env_Idx == i), 'Is_Phno_Idx']
    ii = int(ii)

    ith_Env = phno_geno.loc[(phno_geno.index == ii), 'Env']
    ith_Env = list(ith_Env)[0]

    # selected data is transposed to match correct shape
    wthr_mask = (wthr.Env == ith_Env)
    WMat[i, :, :] = wthr.loc[wthr_mask, 
                       ].sort_values('DOY'
                       ).drop(columns = ['Unnamed: 0', 'Env', 
                                         'Year', 'Date', 'DOY']).T
```

### Hilbert W Matrix

```{python}
# # Demo
# import plotly.express as px
# temp = np_3d_to_hilbert( WMat[:, :, :].swapaxes(1, 2) )
# print(temp.shape)
# i = 8
# print(WMatNames[i])
# px.imshow(temp[0, :, :, i])
```

```{python}
if os.path.exists(cache_path+'WMat_hilb.npy'):
#     WMat_hilb = np.load(cache_path+'WMat_hilb.npy')
    pass
else:
    WMat_hilb = np_3d_to_hilbert( np.swapaxes(WMat, 1, 2) ) # swap channels back to dim 2 before running

    # WMat_hilb.shape
    # (4926, 256, 512, 4)
    # Pytorch standard has channels second
    WMat_hilb = np.swapaxes(WMat_hilb, 1, 3)
    WMat_hilb = np.swapaxes(WMat_hilb, 2, 3)

    # set missings to 0
    WMat_hilb[np.isnan(WMat_hilb)] = 0

    np.save(cache_path+'WMat_hilb.npy', WMat_hilb)
```

## Prep CGMV?

```{python}
cgmv_Env = phno_geno.loc[:, ['Env', 'Env_Idx']
                   ].drop_duplicates(
                   ).merge(cgmv
                   ).sort_values('Env_Idx'
                   ).reset_index(
                   ).drop(columns = ['index', 'Env', 'Env_Idx', 'Unnamed: 0', 'Year'])

MMatNames = list(cgmv_Env)
MMat = np.asarray(cgmv_Env)
```

# Save data
This will streamline model generation. I'll just need to load these files in and can directly begin modeling.

```{python}
np.save(cache_path+'SMatNames.npy', SMatNames)
np.save(cache_path+'WMatNames.npy', WMatNames)
np.save(cache_path+'MMatNames.npy', MMatNames)
```

```{python}
np.save(cache_path+'SMat.npy', SMat)
np.save(cache_path+'WMat.npy', WMat)
np.save(cache_path+'MMat.npy', MMat)
```

