{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aadccc0",
   "metadata": {},
   "source": [
    "# Y~G ACGT Probabilities Hilbert Curve 2d\n",
    "\n",
    "> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c283bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from EnvDL.core import ensure_dir_path_exists \n",
    "from EnvDL.dlfn import g2fc_datawrapper, BigDataset, plDNN_general\n",
    "from EnvDL.dlfn import ResNet2d, BasicBlock2d\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F # F.mse_loss\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.loggers import TensorBoardLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ec5d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run settings: \n",
    "max_epoch = 10\n",
    "batch_size = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99263a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu_num = 0\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if use_gpu_num in [0, 1]: \n",
    "    torch.cuda.set_device(use_gpu_num)\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4600ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_path = '../nbs_artifacts/02.24_g2fc_G_ACGT_Hilbert_conv2d/'\n",
    "ensure_dir_path_exists(dir_path = cache_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517af1f8",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8bfa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = g2fc_datawrapper()\n",
    "X.set_split()\n",
    "X.load_all(name_list = ['obs_geno_lookup', 'YMat', 'ACGT_hilb',], store=True) \n",
    "\n",
    "X.calc_cs('YMat', version = 'np', filter = 'val:train')\n",
    "X.calc_cs('ACGT_hilb',            filter = 'val:train', filter_lookup= 'obs_geno_lookup')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a6c56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataloader = DataLoader(BigDataset(\n",
    "    lookups_are_filtered = False,\n",
    "    lookup_obs  = X.get('val:train',       ops_string='asarray from_numpy             '),\n",
    "    lookup_geno = X.get('obs_geno_lookup', ops_string='asarray from_numpy             '),\n",
    "    y =           X.get('YMat',            ops_string='asarray from_numpy float cuda:0')[:, None],\n",
    "    G =           X.get('ACGT_hilb',       ops_string='        from_numpy float cuda:0'),\n",
    "    G_type = 'hilbert'\n",
    "    ),\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "\n",
    "validation_dataloader = DataLoader(BigDataset(\n",
    "    lookups_are_filtered = False,\n",
    "    lookup_obs = X.get('val:test',         ops_string='asarray from_numpy             '),\n",
    "    lookup_geno = X.get('obs_geno_lookup', ops_string='asarray from_numpy             '),\n",
    "    y =          X.get('YMat',             ops_string='asarray from_numpy float cuda:0')[:, None],\n",
    "    G =          X.get('ACGT_hilb',        ops_string='        from_numpy float cuda:0'),\n",
    "    G_type = 'hilbert'\n",
    "    ),\n",
    "    batch_size = batch_size,\n",
    "    shuffle = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492ad404",
   "metadata": {},
   "source": [
    "## Test Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f60b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ResNet2d(\n",
    "#         block = BasicBlock2d, #: Type[Union[BasicBlock, Bottleneck]],\n",
    "#         layers = [2, 2, 2, 2], #: List[int],\n",
    "#         # num_classes: int = 1000,\n",
    "#         zero_init_residual = False,\n",
    "#         groups = 1,\n",
    "#         width_per_group = 64,\n",
    "#         replace_stride_with_dilation = None,\n",
    "#         norm_layer = None,\n",
    "#         input_channels = 4\n",
    "#     )\n",
    "\n",
    "# DNNG = plDNN_general(model)     \n",
    "# optimizer = DNNG.configure_optimizers()\n",
    "\n",
    "# logger = TensorBoardLogger(\"tb_logs\", name=\"g-acgt-hilb-res-4rep2-from-pytorch\")\n",
    "# trainer = pl.Trainer(max_epochs=max_epoch, logger=logger)\n",
    "\n",
    "# trainer.fit(model=DNNG, train_dataloaders=training_dataloader, val_dataloaders=validation_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934b4000",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = ResNet2d(\n",
    "        block = BasicBlock2d, #: Type[Union[BasicBlock, Bottleneck]],\n",
    "        layers = [4 for i in range(4)], #: List[int],\n",
    "        # num_classes: int = 1000,\n",
    "        zero_init_residual = False,\n",
    "        groups = 1,\n",
    "        width_per_group = 64,\n",
    "        replace_stride_with_dilation = None,\n",
    "        norm_layer = None,\n",
    "        input_channels = 4\n",
    "    )\n",
    "\n",
    "DNNG = plDNN_general(model)     \n",
    "optimizer = DNNG.configure_optimizers()\n",
    "\n",
    "logger = TensorBoardLogger(\"tb_logs\", name=\"g-acgt-hilb-res-4rep4-from-pytorch\")\n",
    "trainer = pl.Trainer(max_epochs=max_epoch, logger=logger)\n",
    "\n",
    "trainer.fit(model=DNNG, train_dataloaders=training_dataloader, val_dataloaders=validation_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167ed13a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a60818d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009d1294",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ce8e3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb44be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xin = torch.randn((50, 4, 256, 512))\n",
    "\n",
    "# model = nn.Sequential(\n",
    "#     nn.Conv2d(4, 8, kernel_size=3, stride=2, padding=1),\n",
    "#     nn.BatchNorm2d(8),\n",
    "#     nn.Conv2d(8, 8, 3, 2, 1),\n",
    "#     nn.BatchNorm2d(8),\n",
    "#     nn.Conv2d(8, 8, 3, 2, 1),\n",
    "#     nn.BatchNorm2d(8),\n",
    "#     nn.Conv2d(8, 8, 3, 2, 1),\n",
    "#     nn.BatchNorm2d(8),\n",
    "#     nn.Conv2d(8, 8, 3, 2, 1),\n",
    "#     nn.BatchNorm2d(8),\n",
    "#     nn.Conv2d(8, 8, 3, 2, 1),\n",
    "#     nn.BatchNorm2d(8),\n",
    "#     nn.Conv2d(8, 8, 3, 2, 1),\n",
    "#     nn.Conv2d(8, 8, 3, 2, 1),\n",
    "#     nn.BatchNorm2d(8),\n",
    "#     nn.AdaptiveAvgPool2d((1,1)),\n",
    "#     nn.Flatten(),\n",
    "#     nn.Linear(8, 1)\n",
    "# )\n",
    "\n",
    "# model(xin).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df37efca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_epoch = 10\n",
    "# DNNG = plDNN_ACGT(model)     \n",
    "# optimizer = DNNG.configure_optimizers()\n",
    "\n",
    "# logger = TensorBoardLogger(\"tb_logs\", name=\"g-acgt-hilb-no-res-8deep-batch-norm\")\n",
    "# trainer = pl.Trainer(max_epochs=max_epoch, logger=logger)\n",
    "\n",
    "# trainer.fit(model=DNNG, train_dataloaders=training_dataloader, val_dataloaders=validation_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d95c590",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e1d976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from the Fixup paper's repository \n",
    "# # https://github.com/hongyi-zhang/Fixup/blob/master/cifar/models/fixup_resnet_cifar.py\n",
    "# def conv3x3(in_planes, out_planes, stride=1):\n",
    "#     \"\"\"3x3 convolution with padding\"\"\"\n",
    "#     return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "#                      padding=1, bias=False)\n",
    "\n",
    "\n",
    "# class FixupBasciBlock(nn.Module):\n",
    "#     expansion = 1\n",
    "\n",
    "#     def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "#         super(FixupBasicBlock, self).__init__()\n",
    "#         # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "#         self.bias1a = nn.Parameter(torch.zeros(1))\n",
    "#         self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "#         self.bias1b = nn.Parameter(torch.zeros(1))\n",
    "#         self.relu = nn.ReLU(inplace=True)\n",
    "#         self.bias2a = nn.Parameter(torch.zeros(1))\n",
    "#         self.conv2 = conv3x3(planes, planes)\n",
    "#         self.scale = nn.Parameter(torch.ones(1))\n",
    "#         self.bias2b = nn.Parameter(torch.zeros(1))\n",
    "#         self.downsample = downsample\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         identity = x\n",
    "\n",
    "#         out = self.conv1(x + self.bias1a)\n",
    "#         out = self.relu(out + self.bias1b)\n",
    "\n",
    "#         out = self.conv2(out + self.bias2a)\n",
    "#         out = out * self.scale + self.bias2b\n",
    "\n",
    "#         if self.downsample is not None:\n",
    "#             identity = self.downsample(x + self.bias1a)\n",
    "#             identity = torch.cat((identity, torch.zeros_like(identity)), 1)\n",
    "\n",
    "#         out += identity\n",
    "#         out = self.relu(out)\n",
    "\n",
    "#         return out\n",
    "\n",
    "\n",
    "# class FixupResNet(nn.Module):\n",
    "\n",
    "#     def __init__(self, block, layers, num_classes=10):\n",
    "#         super(FixupResNet, self).__init__()\n",
    "#         self.num_layers = sum(layers)\n",
    "#         self.inplanes = 16\n",
    "#         # self.conv1 = conv3x3(3, 16)\n",
    "#         self.conv1 = conv3x3(4, 16)\n",
    "#         self.bias1 = nn.Parameter(torch.zeros(1))\n",
    "#         self.relu = nn.ReLU(inplace=True)\n",
    "#         self.layer1 = self._make_layer(block, 16, layers[0])\n",
    "#         self.layer2 = self._make_layer(block, 32, layers[1], stride=2)\n",
    "#         self.layer3 = self._make_layer(block, 64, layers[2], stride=2)\n",
    "#         self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "#         self.bias2 = nn.Parameter(torch.zeros(1))\n",
    "#         self.fc = nn.Linear(64, num_classes)\n",
    "\n",
    "#         for m in self.modules():\n",
    "#             if isinstance(m, FixupBasicBlock):\n",
    "#                 nn.init.normal_(m.conv1.weight, mean=0, std=np.sqrt(2 / (m.conv1.weight.shape[0] * np.prod(m.conv1.weight.shape[2:]))) * self.num_layers ** (-0.5))\n",
    "#                 nn.init.constant_(m.conv2.weight, 0)\n",
    "#             elif isinstance(m, nn.Linear):\n",
    "#                 nn.init.constant_(m.weight, 0)\n",
    "#                 nn.init.constant_(m.bias, 0)\n",
    "\n",
    "#     def _make_layer(self, block, planes, blocks, stride=1):\n",
    "#         downsample = None\n",
    "#         if stride != 1:\n",
    "#             downsample = nn.AvgPool2d(1, stride=stride)\n",
    "\n",
    "#         layers = []\n",
    "#         layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "#         self.inplanes = planes\n",
    "#         for _ in range(1, blocks):\n",
    "#             layers.append(block(planes, planes))\n",
    "\n",
    "#         return nn.Sequential(*layers)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.relu(x + self.bias1)\n",
    "\n",
    "#         x = self.layer1(x)\n",
    "#         x = self.layer2(x)\n",
    "#         x = self.layer3(x)\n",
    "\n",
    "#         x = self.avgpool(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         x = self.fc(x + self.bias2)\n",
    "\n",
    "#         return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fixup_resnet20(**kwargs):\n",
    "#     \"\"\"Constructs a Fixup-ResNet-20 model.\n",
    "\n",
    "#     \"\"\"\n",
    "#     model = FixupResNet(FixupBasicBlock, [3, 3, 3], **kwargs)\n",
    "#     return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc728b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = fixup_resnet20(num_classes=1).to('cuda')\n",
    "# model(next(iter(training_dataloader))[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSUV_(model, next(iter(training_dataloader))[0]) # woah! I was not expecting this to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b504ed87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Module for training subnetworks.\n",
    "# class plDNN_ACGT(pl.LightningModule):\n",
    "#     def __init__(self, mod):\n",
    "#         super().__init__()\n",
    "#         self.mod = mod\n",
    "        \n",
    "#     def training_step(self, batch, batch_idx):\n",
    "#         g_i, y_i = batch\n",
    "#         # pred, out = self.mod(g_i)\n",
    "#         pred = self.mod(g_i)\n",
    "#         loss = F.mse_loss(pred, y_i)\n",
    "#         self.log(\"train_loss\", loss)\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             weight_list=[(name, param) for name, param in model.named_parameters() if name.split('.')[-1] == 'weight']\n",
    "#             for l in weight_list:\n",
    "#                 self.log((\"train_mean\"+l[0]), l[1].mean())\n",
    "#                 self.log((\"train_std\"+l[0]), l[1].std())        \n",
    "#         return(loss)\n",
    "        \n",
    "#     def validation_step(self, batch, batch_idx):\n",
    "#         g_i, y_i = batch\n",
    "#         # pred, out = self.mod(g_i)\n",
    "#         pred = self.mod(g_i)\n",
    "#         loss = F.mse_loss(pred, y_i)\n",
    "#         self.log('val_loss', loss)        \n",
    "     \n",
    "#     def configure_optimizers(self, **kwargs):\n",
    "#         optimizer = torch.optim.Adam(self.parameters(), **kwargs)\n",
    "#         return optimizer    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_epoch = 10\n",
    "# DNNG = plDNN_ACGT(model)     \n",
    "# optimizer = DNNG.configure_optimizers()\n",
    "\n",
    "# logger = TensorBoardLogger(\"tb_logs\", name=\"g-acgt-hilb-res-f20\")\n",
    "# trainer = pl.Trainer(max_epochs=max_epoch, logger=logger)\n",
    "\n",
    "# trainer.fit(model=DNNG, train_dataloaders=training_dataloader, val_dataloaders=validation_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788ba1e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf76c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class NeuralNetwork(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(NeuralNetwork, self).__init__()    \n",
    "\n",
    "# #         def Linear_block(in_size, out_size, drop_pr):\n",
    "# #             block = nn.Sequential(\n",
    "# #                 nn.Linear(in_size, out_size),\n",
    "# #                 nn.ReLU(),\n",
    "# #                 nn.Dropout(drop_pr)\n",
    "# #             )\n",
    "# #             return(block)         \n",
    "        \n",
    "        \n",
    "# #         def Conv1D_Max_block(in_channels, out_channels, kernel_size, stride):\n",
    "# #             block = nn.Sequential(\n",
    "# #                 nn.Conv1d(\n",
    "# #                     in_channels= in_channels, # second channel\n",
    "# #                     out_channels= out_channels,\n",
    "# #                     kernel_size= kernel_size,\n",
    "# #                     stride= stride\n",
    "# #                 ), \n",
    "# #                 nn.MaxPool1d((kernel_size,), stride=stride)\n",
    "# #             )\n",
    "# #             return(block)\n",
    "        \n",
    "#         self.x_network = nn.Sequential(\n",
    "#             nn.Conv2d(\n",
    "#                     in_channels= 4, \n",
    "#                     out_channels= 4,\n",
    "#                     kernel_size= (3, 3),\n",
    "#                     stride= 2,\n",
    "#                     padding = 1,\n",
    "#                     bias = True\n",
    "#                 ),\n",
    "#             nn.Conv2d(\n",
    "#                     in_channels= 4, \n",
    "#                     out_channels= 4,\n",
    "#                     kernel_size= (3, 3),\n",
    "#                     stride= 2,\n",
    "#                     padding = 1,\n",
    "#                     bias = True\n",
    "#                 ),\n",
    "#             nn.Conv2d(\n",
    "#                     in_channels= 4, \n",
    "#                     out_channels= 4,\n",
    "#                     kernel_size= (3, 3),\n",
    "#                     stride= 2,\n",
    "#                     padding = 1,\n",
    "#                     bias = True\n",
    "#                 ),\n",
    "#             nn.Conv2d(\n",
    "#                     in_channels= 4, \n",
    "#                     out_channels= 4,\n",
    "#                     kernel_size= (3, 3),\n",
    "#                     stride= 2,\n",
    "#                     padding = 1,\n",
    "#                     bias = True\n",
    "#                 ),\n",
    "#             nn.Conv2d(\n",
    "#                     in_channels= 4, \n",
    "#                     out_channels= 4,\n",
    "#                     kernel_size= (3, 3),\n",
    "#                     stride= 2,\n",
    "#                     padding = 1,\n",
    "#                     bias = True\n",
    "#                 )\n",
    "#         )\n",
    "        \n",
    "#         self.x_pred = nn.Sequential(\n",
    "#             nn.Flatten(),            \n",
    "#             nn.Linear(512, 1)\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         out = self.x_network(x)\n",
    "#         pred = self.x_pred(out)\n",
    "#         return pred, out\n",
    "\n",
    "# model = NeuralNetwork().to(device)\n",
    "\n",
    "# # model(next(iter(training_dataloader))[0])[0].shape\n",
    "\n",
    "# # torch.Size([50, 4, 256, 512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8883eaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSUV_(model, next(iter(training_dataloader))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae76ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Module for training subnetworks.\n",
    "# class plDNN_ACGT(pl.LightningModule):\n",
    "#     def __init__(self, mod):\n",
    "#         super().__init__()\n",
    "#         self.mod = mod\n",
    "        \n",
    "#     def training_step(self, batch, batch_idx):\n",
    "#         g_i, y_i = batch\n",
    "#         pred, out = self.mod(g_i)\n",
    "#         loss = F.mse_loss(pred, y_i)\n",
    "#         self.log(\"train_loss\", loss)\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             weight_list=[(name, param) for name, param in model.named_parameters() if name.split('.')[-1] == 'weight']\n",
    "#             for l in weight_list:\n",
    "#                 self.log((\"train_mean\"+l[0]), l[1].mean())\n",
    "#                 self.log((\"train_std\"+l[0]), l[1].std())        \n",
    "#         return(loss)\n",
    "        \n",
    "#     def validation_step(self, batch, batch_idx):\n",
    "#         g_i, y_i = batch\n",
    "#         pred, out = self.mod(g_i)\n",
    "#         loss = F.mse_loss(pred, y_i)\n",
    "#         self.log('val_loss', loss)        \n",
    "     \n",
    "#     def configure_optimizers(self, **kwargs):\n",
    "#         optimizer = torch.optim.Adam(self.parameters(), **kwargs)\n",
    "#         return optimizer    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a09fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_epoch = 200\n",
    "# DNNG = plDNN_ACGT(model)     \n",
    "# optimizer = DNNG.configure_optimizers()\n",
    "\n",
    "# logger = TensorBoardLogger(\"tb_logs\", name=\"g-acgt-hilb-res\")\n",
    "# trainer = pl.Trainer(max_epochs=max_epoch, logger=logger)\n",
    "\n",
    "# trainer.fit(model=DNNG, train_dataloaders=training_dataloader, val_dataloaders=validation_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952cfd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(DNNG.mod, cache_path+'g-acgt-hilb'+'.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
