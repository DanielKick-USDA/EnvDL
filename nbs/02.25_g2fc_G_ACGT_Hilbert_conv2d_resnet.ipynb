{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6356970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacky way to schedule. Here I'm setting these to sleep until the gpus should be free.\n",
    "# At the end of the notebooks  os._exit(00) will kill the kernel freeing the gpu. \n",
    "#                          Hours to wait\n",
    "# import time; time.sleep( 6 * (60*60))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aadccc0",
   "metadata": {},
   "source": [
    "# G only ACGT Probabilities Hilbert Conv1D\n",
    "\n",
    "> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc2f33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "pio.templates.default = \"plotly_white\"\n",
    "\n",
    "import hilbertcurve\n",
    "from hilbertcurve.hilbertcurve import HilbertCurve\n",
    "\n",
    "from EnvDL.core import * # includes remove_matching_files\n",
    "from EnvDL.dna  import *\n",
    "from EnvDL.dlfn import * # includes LSUV_\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# import os\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# import plotly.express as px\n",
    "# import plotly.io as pio\n",
    "# pio.templates.default = \"plotly_white\"\n",
    "\n",
    "# import hilbertcurve\n",
    "# from hilbertcurve.hilbertcurve import HilbertCurve\n",
    "\n",
    "# from EnvDL.core import * # includes remove_matching_files\n",
    "# from EnvDL.dna import *\n",
    "# from EnvDL.dlfn import *\n",
    "\n",
    "# from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54470e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "use_gpu_num = 0\n",
    "\n",
    "# Imports --------------------------------------------------------------------\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import torch.nn.functional as F # F.mse_loss\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if use_gpu_num in [0, 1]: \n",
    "    torch.cuda.set_device(use_gpu_num)\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4600ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_path = '../nbs_artifacts/02.24_g2fc_G_ACGT_Hilbert_conv2d/'\n",
    "ensure_dir_path_exists(dir_path = cache_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517af1f8",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e59a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_from = '../nbs_artifacts/01.03_g2fc_prep_matrices/'\n",
    "# phno = pd.read_csv(load_from+'phno.csv')\n",
    "phno_geno = pd.read_csv(load_from+'phno_geno.csv')\n",
    "phno = phno_geno\n",
    "\n",
    "obs_geno_lookup = np.load(load_from+'obs_geno_lookup.npy') # Phno_Idx\tGeno_Idx\tIs_Phno_Idx\n",
    "YMat = np.load(load_from+'YMat.npy')\n",
    "# GMat = np.load(load_from+'GMat.npy')\n",
    "# ACGT_OneHot = np.load(load_from+'ACGT_OneHot.npy')\n",
    "# ACGT = np.load(load_from+'ACGT.npy')\n",
    "ACGT_hilb = np.load(load_from+'ACGT_hilb.npy')\n",
    "# SMat = np.load(load_from+'SMat3.npy')\n",
    "# WMat = np.load(load_from+'WMat3.npy')\n",
    "# MMat = np.load(load_from+'MMat3.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b79c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can't reduce size because there are missing values in the hilbert curve.\n",
    "# reduce Size of ACGT ----\n",
    "# Since these are probabilities we can immediately save 1/4 of the space needed by using ACG instead of ACGT\n",
    "# ACGT_hilb = ACGT_hilb[:, 0:-1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea0cacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dataloader_batch_size = 8 #16 #64\n",
    "# # run_epochs = 200\n",
    "\n",
    "# use_gpu_num = 0\n",
    "\n",
    "# # Imports --------------------------------------------------------------------\n",
    "# import torch\n",
    "# from torch.utils.data import Dataset\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torch import nn\n",
    "\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# if use_gpu_num in [0, 1]: \n",
    "#     torch.cuda.set_device(use_gpu_num)\n",
    "# print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9753c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02205e69",
   "metadata": {},
   "source": [
    "### Create train/test validate indicies from json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f93d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rng = np.random.default_rng(9230473) # note, must use rng.shuffle(arr) below for this to take effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a91e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pct_cumsum_thresh = .9\n",
    "\n",
    "# # make a df to aid in creating train/test splits\n",
    "# # the plan is to shuffle the rows of the df, calculate the cumulative sum of the percents obs, then \n",
    "# # the entries above and below a given percent will be the train/test.\n",
    "# obs_per_hybrid = phno.assign(n = 1).groupby('Hybrid').count().reset_index()\n",
    "# obs_per_hybrid = obs_per_hybrid.loc[:, ['Hybrid', 'n']]\n",
    "# obs_per_hybrid['pct'] = obs_per_hybrid.n / obs_per_hybrid.n.sum()\n",
    "# obs_per_hybrid['pct_cumsum'] = np.nan\n",
    "# obs_per_hybrid['random_order'] = 0\n",
    "\n",
    "# obs_per_hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ca1bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # fill in the random values to sort on\n",
    "# arr = np.arange(obs_per_hybrid.shape[0])\n",
    "# rng.shuffle(arr)\n",
    "# obs_per_hybrid.random_order = arr\n",
    "\n",
    "# obs_per_hybrid = obs_per_hybrid.sort_values('random_order')\n",
    "# obs_per_hybrid['pct_cumsum'] = obs_per_hybrid.pct.cumsum()\n",
    "# obs_per_hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bce9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert back into phno indices\n",
    "# train_hybrids = list(obs_per_hybrid.loc[(obs_per_hybrid.pct_cumsum <= pct_cumsum_thresh), 'Hybrid'])\n",
    "# test_hybrids  = list(obs_per_hybrid.loc[(obs_per_hybrid.pct_cumsum >  pct_cumsum_thresh), 'Hybrid'])\n",
    "\n",
    "# train_idx = phno.loc[(phno.Hybrid.isin(train_hybrids)), ].index\n",
    "# test_idx  = phno.loc[(phno.Hybrid.isin(test_hybrids)), ].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d0163e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [len(train_idx), len(test_idx)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbaedf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm all observation idxs are have genomic information\n",
    "# assert [] == [e for e in list(train_idx)+list(test_idx) if e not in obs_geno_lookup[:, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a639405",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf2a56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_from = '../nbs_artifacts/01.06_g2fc_cluster_genotypes/'\n",
    "\n",
    "split_info = read_split_info(\n",
    "    load_from = '../nbs_artifacts/01.06_g2fc_cluster_genotypes/',\n",
    "    json_prefix = '2023:9:5:12:8:26')\n",
    "\n",
    "temp = phno.copy()\n",
    "temp[['Female', 'Male']] = temp['Hybrid'].str.split('/', expand = True)\n",
    "\n",
    "test_dict = find_idxs_split_dict(\n",
    "    obs_df = temp, \n",
    "    split_dict = split_info['test'][0]\n",
    ")\n",
    "\n",
    "temp = temp.loc[test_dict['train_idx'], ] # restrict before re-aplying\n",
    "\n",
    "val_dict = find_idxs_split_dict(\n",
    "    obs_df = temp, \n",
    "    split_dict = split_info['validate'][0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1facc312",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = val_dict['train_idx']\n",
    "test_idx  = val_dict['test_idx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f07df15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm all observation idxs are have genomic information\n",
    "assert [] == [e for e in list(train_idx)+list(test_idx) if e not in obs_geno_lookup[:, 0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d986a5b3",
   "metadata": {},
   "source": [
    "## One Hot Encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ee1d4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6a7c6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04d9ece",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02004189",
   "metadata": {},
   "outputs": [],
   "source": [
    "YMat_cs = calc_cs(YMat[train_idx])\n",
    "y_cs = apply_cs(YMat, YMat_cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15ee06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataloader = DataLoader(\n",
    "    ACGTDataset(y = torch.from_numpy(y_cs[train_idx])[:, None].to(torch.float), \n",
    "                G = torch.from_numpy(ACGT_hilb).to(torch.float), \n",
    "                idx_original = torch.from_numpy(np.array(train_idx)),\n",
    "                idx_lookup   = torch.from_numpy(np.asarray(obs_geno_lookup)),\n",
    "                use_gpu_num = 0,\n",
    "                device = 'cuda'\n",
    "               ),\n",
    "    batch_size = 50,\n",
    "    shuffle = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08eaaff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataloader = DataLoader(\n",
    "    ACGTDataset(y = torch.from_numpy(y_cs[test_idx])[:, None].to(torch.float), \n",
    "                G = torch.from_numpy(ACGT_hilb).to(torch.float), \n",
    "                idx_original = torch.from_numpy(np.array(test_idx)),\n",
    "                idx_lookup   = torch.from_numpy(np.asarray(obs_geno_lookup)),\n",
    "                use_gpu_num = 0,\n",
    "                device = 'cuda'\n",
    "               ),\n",
    "    batch_size = 50,\n",
    "    shuffle = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b8b1fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 4, 256, 512])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(training_dataloader))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9156df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3281c8c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfb88bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xin' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m ResNet2d(\n\u001b[1;32m      2\u001b[0m         block \u001b[38;5;241m=\u001b[39m BasicBlock2d, \u001b[38;5;66;03m#: Type[Union[BasicBlock, Bottleneck]],\u001b[39;00m\n\u001b[1;32m      3\u001b[0m         layers \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m], \u001b[38;5;66;03m#: List[int],\u001b[39;00m\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;66;03m# num_classes: int = 1000,\u001b[39;00m\n\u001b[1;32m      5\u001b[0m         zero_init_residual \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m      6\u001b[0m         groups \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m      7\u001b[0m         width_per_group \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m,\n\u001b[1;32m      8\u001b[0m         replace_stride_with_dilation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m      9\u001b[0m         norm_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m     )(xin)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'xin' is not defined"
     ]
    }
   ],
   "source": [
    "ResNet2d(\n",
    "        block = BasicBlock2d, #: Type[Union[BasicBlock, Bottleneck]],\n",
    "        layers = [2, 2, 2, 2], #: List[int],\n",
    "        # num_classes: int = 1000,\n",
    "        zero_init_residual = False,\n",
    "        groups = 1,\n",
    "        width_per_group = 64,\n",
    "        replace_stride_with_dilation = None,\n",
    "        norm_layer = None\n",
    "    )(xin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda0c864",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kickd/miniconda3/envs/fastai/lib/python3.11/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/kickd/miniconda3/envs/fastai/lib/python3.11/si ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Missing logger folder: tb_logs/g-acgt-hilb-res-4rep2-from-pytorch\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name | Type     | Params\n",
      "----------------------------------\n",
      "0 | mod  | ResNet2d | 11.2 M\n",
      "----------------------------------\n",
      "11.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.721    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a24fe819869d41c69dd914aee9bd7d89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kickd/miniconda3/envs/fastai/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "/home/kickd/miniconda3/envs/fastai/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "/home/kickd/miniconda3/envs/fastai/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2606c19f798e4f5b8d90102640473966",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c71ed2f29fb477c941731bd1eca2300",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ca2e1e083d843f693b60c847c850a90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c45de0abc42147aebc66fc0c63be248e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11d2b8694c46431a83b31fc5463573cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ae50482f43348628aa37c7638eedb9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0171c0a12f884113b6a510c75e343d13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be2213b0784a40028d5317c6fe74890f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d851becf8b3046c48d341f7353954b2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7936a7f31c304bbc88aa0c8c23fa5c91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ba49418effb433bb4fa64aab2d5a0ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "model = ResNet2d(\n",
    "        block = BasicBlock2d, #: Type[Union[BasicBlock, Bottleneck]],\n",
    "        layers = [2, 2, 2, 2], #: List[int],\n",
    "        # num_classes: int = 1000,\n",
    "        zero_init_residual = False,\n",
    "        groups = 1,\n",
    "        width_per_group = 64,\n",
    "        replace_stride_with_dilation = None,\n",
    "        norm_layer = None\n",
    "    )\n",
    "\n",
    "max_epoch = 10\n",
    "DNNG = plDNN_ACGT(model)     \n",
    "optimizer = DNNG.configure_optimizers()\n",
    "\n",
    "logger = TensorBoardLogger(\"tb_logs\", name=\"g-acgt-hilb-res-4rep2-from-pytorch\")\n",
    "trainer = pl.Trainer(max_epochs=max_epoch, logger=logger)\n",
    "\n",
    "trainer.fit(model=DNNG, train_dataloaders=training_dataloader, val_dataloaders=validation_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2c8213",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kickd/miniconda3/envs/fastai/lib/python3.11/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/kickd/miniconda3/envs/fastai/lib/python3.11/si ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: tb_logs/g-acgt-hilb-res-4rep4-from-pytorch\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name | Type     | Params\n",
      "----------------------------------\n",
      "0 | mod  | ResNet2d | 23.7 M\n",
      "----------------------------------\n",
      "23.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "23.7 M    Total params\n",
      "94.886    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d32268c36c441328ae3fc30a8ea7294",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kickd/miniconda3/envs/fastai/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "/home/kickd/miniconda3/envs/fastai/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "/home/kickd/miniconda3/envs/fastai/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28aeff603af04a519c4e10661135a38b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba35c1f83ad94cb38c16a2edc4baeb7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6dfabcc620349dd9b09ce2c64a1b315",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae85d81bfbd846a68a4f39c06adbda05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "490598fbd86a4d53a85b742507e782bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6658e8cf687424b812744fcdaefeda4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a95dbde0650491c8a7f89084c71e9f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "616b8446a89b44408bd5d659764bc99e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f76b78c6c1fd45d887808edc6ecee5af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a234c56038a49df8ddf50e2c4ef2325",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be9025535bb44e0bb5585bd530b2117b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "model = ResNet2d(\n",
    "        block = BasicBlock2d, #: Type[Union[BasicBlock, Bottleneck]],\n",
    "        layers = [4 for i in range(4)], #: List[int],\n",
    "        # num_classes: int = 1000,\n",
    "        zero_init_residual = False,\n",
    "        groups = 1,\n",
    "        width_per_group = 64,\n",
    "        replace_stride_with_dilation = None,\n",
    "        norm_layer = None\n",
    "    )\n",
    "\n",
    "max_epoch = 10\n",
    "DNNG = plDNN_ACGT(model)     \n",
    "optimizer = DNNG.configure_optimizers()\n",
    "\n",
    "logger = TensorBoardLogger(\"tb_logs\", name=\"g-acgt-hilb-res-4rep4-from-pytorch\")\n",
    "trainer = pl.Trainer(max_epochs=max_epoch, logger=logger)\n",
    "\n",
    "trainer.fit(model=DNNG, train_dataloaders=training_dataloader, val_dataloaders=validation_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbd4bdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a60818d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009d1294",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ce8e3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb44be2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 1])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xin = torch.randn((50, 4, 256, 512))\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(4, 8, kernel_size=3, stride=2, padding=1),\n",
    "    nn.BatchNorm2d(8),\n",
    "    nn.Conv2d(8, 8, 3, 2, 1),\n",
    "    nn.BatchNorm2d(8),\n",
    "    nn.Conv2d(8, 8, 3, 2, 1),\n",
    "    nn.BatchNorm2d(8),\n",
    "    nn.Conv2d(8, 8, 3, 2, 1),\n",
    "    nn.BatchNorm2d(8),\n",
    "    nn.Conv2d(8, 8, 3, 2, 1),\n",
    "    nn.BatchNorm2d(8),\n",
    "    nn.Conv2d(8, 8, 3, 2, 1),\n",
    "    nn.BatchNorm2d(8),\n",
    "    nn.Conv2d(8, 8, 3, 2, 1),\n",
    "    nn.Conv2d(8, 8, 3, 2, 1),\n",
    "    nn.BatchNorm2d(8),\n",
    "    nn.AdaptiveAvgPool2d((1,1)),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(8, 1)\n",
    ")\n",
    "\n",
    "model(xin).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df37efca",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epoch = 10\n",
    "DNNG = plDNN_ACGT(model)     \n",
    "optimizer = DNNG.configure_optimizers()\n",
    "\n",
    "logger = TensorBoardLogger(\"tb_logs\", name=\"g-acgt-hilb-no-res-8deep-batch-norm\")\n",
    "trainer = pl.Trainer(max_epochs=max_epoch, logger=logger)\n",
    "\n",
    "trainer.fit(model=DNNG, train_dataloaders=training_dataloader, val_dataloaders=validation_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d95c590",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e1d976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the Fixup paper's repository \n",
    "# https://github.com/hongyi-zhang/Fixup/blob/master/cifar/models/fixup_resnet_cifar.py\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "\n",
    "class FixupBasciBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(FixupBasicBlock, self).__init__()\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        self.bias1a = nn.Parameter(torch.zeros(1))\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bias1b = nn.Parameter(torch.zeros(1))\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.bias2a = nn.Parameter(torch.zeros(1))\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.scale = nn.Parameter(torch.ones(1))\n",
    "        self.bias2b = nn.Parameter(torch.zeros(1))\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x + self.bias1a)\n",
    "        out = self.relu(out + self.bias1b)\n",
    "\n",
    "        out = self.conv2(out + self.bias2a)\n",
    "        out = out * self.scale + self.bias2b\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x + self.bias1a)\n",
    "            identity = torch.cat((identity, torch.zeros_like(identity)), 1)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class FixupResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=10):\n",
    "        super(FixupResNet, self).__init__()\n",
    "        self.num_layers = sum(layers)\n",
    "        self.inplanes = 16\n",
    "        # self.conv1 = conv3x3(3, 16)\n",
    "        self.conv1 = conv3x3(4, 16)\n",
    "        self.bias1 = nn.Parameter(torch.zeros(1))\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.layer1 = self._make_layer(block, 16, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 32, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 64, layers[2], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.bias2 = nn.Parameter(torch.zeros(1))\n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, FixupBasicBlock):\n",
    "                nn.init.normal_(m.conv1.weight, mean=0, std=np.sqrt(2 / (m.conv1.weight.shape[0] * np.prod(m.conv1.weight.shape[2:]))) * self.num_layers ** (-0.5))\n",
    "                nn.init.constant_(m.conv2.weight, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.constant_(m.weight, 0)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1:\n",
    "            downsample = nn.AvgPool2d(1, stride=stride)\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(planes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x + self.bias1)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x + self.bias2)\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixup_resnet20(**kwargs):\n",
    "    \"\"\"Constructs a Fixup-ResNet-20 model.\n",
    "\n",
    "    \"\"\"\n",
    "    model = FixupResNet(FixupBasicBlock, [3, 3, 3], **kwargs)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc728b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fixup_resnet20(num_classes=1).to('cuda')\n",
    "model(next(iter(training_dataloader))[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSUV_(model, next(iter(training_dataloader))[0]) # woah! I was not expecting this to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b504ed87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module for training subnetworks.\n",
    "class plDNN_ACGT(pl.LightningModule):\n",
    "    def __init__(self, mod):\n",
    "        super().__init__()\n",
    "        self.mod = mod\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        g_i, y_i = batch\n",
    "        # pred, out = self.mod(g_i)\n",
    "        pred = self.mod(g_i)\n",
    "        loss = F.mse_loss(pred, y_i)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            weight_list=[(name, param) for name, param in model.named_parameters() if name.split('.')[-1] == 'weight']\n",
    "            for l in weight_list:\n",
    "                self.log((\"train_mean\"+l[0]), l[1].mean())\n",
    "                self.log((\"train_std\"+l[0]), l[1].std())        \n",
    "        return(loss)\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        g_i, y_i = batch\n",
    "        # pred, out = self.mod(g_i)\n",
    "        pred = self.mod(g_i)\n",
    "        loss = F.mse_loss(pred, y_i)\n",
    "        self.log('val_loss', loss)        \n",
    "     \n",
    "    def configure_optimizers(self, **kwargs):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), **kwargs)\n",
    "        return optimizer    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epoch = 10\n",
    "DNNG = plDNN_ACGT(model)     \n",
    "optimizer = DNNG.configure_optimizers()\n",
    "\n",
    "logger = TensorBoardLogger(\"tb_logs\", name=\"g-acgt-hilb-res-f20\")\n",
    "trainer = pl.Trainer(max_epochs=max_epoch, logger=logger)\n",
    "\n",
    "trainer.fit(model=DNNG, train_dataloaders=training_dataloader, val_dataloaders=validation_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094466d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788ba1e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf76c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()    \n",
    "\n",
    "#         def Linear_block(in_size, out_size, drop_pr):\n",
    "#             block = nn.Sequential(\n",
    "#                 nn.Linear(in_size, out_size),\n",
    "#                 nn.ReLU(),\n",
    "#                 nn.Dropout(drop_pr)\n",
    "#             )\n",
    "#             return(block)         \n",
    "        \n",
    "        \n",
    "#         def Conv1D_Max_block(in_channels, out_channels, kernel_size, stride):\n",
    "#             block = nn.Sequential(\n",
    "#                 nn.Conv1d(\n",
    "#                     in_channels= in_channels, # second channel\n",
    "#                     out_channels= out_channels,\n",
    "#                     kernel_size= kernel_size,\n",
    "#                     stride= stride\n",
    "#                 ), \n",
    "#                 nn.MaxPool1d((kernel_size,), stride=stride)\n",
    "#             )\n",
    "#             return(block)\n",
    "        \n",
    "        self.x_network = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                    in_channels= 4, \n",
    "                    out_channels= 4,\n",
    "                    kernel_size= (3, 3),\n",
    "                    stride= 2,\n",
    "                    padding = 1,\n",
    "                    bias = True\n",
    "                ),\n",
    "            nn.Conv2d(\n",
    "                    in_channels= 4, \n",
    "                    out_channels= 4,\n",
    "                    kernel_size= (3, 3),\n",
    "                    stride= 2,\n",
    "                    padding = 1,\n",
    "                    bias = True\n",
    "                ),\n",
    "            nn.Conv2d(\n",
    "                    in_channels= 4, \n",
    "                    out_channels= 4,\n",
    "                    kernel_size= (3, 3),\n",
    "                    stride= 2,\n",
    "                    padding = 1,\n",
    "                    bias = True\n",
    "                ),\n",
    "            nn.Conv2d(\n",
    "                    in_channels= 4, \n",
    "                    out_channels= 4,\n",
    "                    kernel_size= (3, 3),\n",
    "                    stride= 2,\n",
    "                    padding = 1,\n",
    "                    bias = True\n",
    "                ),\n",
    "            nn.Conv2d(\n",
    "                    in_channels= 4, \n",
    "                    out_channels= 4,\n",
    "                    kernel_size= (3, 3),\n",
    "                    stride= 2,\n",
    "                    padding = 1,\n",
    "                    bias = True\n",
    "                )\n",
    "        )\n",
    "        \n",
    "        self.x_pred = nn.Sequential(\n",
    "            nn.Flatten(),            \n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.x_network(x)\n",
    "        pred = self.x_pred(out)\n",
    "        return pred, out\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "\n",
    "# model(next(iter(training_dataloader))[0])[0].shape\n",
    "\n",
    "# torch.Size([50, 4, 256, 512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8883eaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "LSUV_(model, next(iter(training_dataloader))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae76ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module for training subnetworks.\n",
    "class plDNN_ACGT(pl.LightningModule):\n",
    "    def __init__(self, mod):\n",
    "        super().__init__()\n",
    "        self.mod = mod\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        g_i, y_i = batch\n",
    "        pred, out = self.mod(g_i)\n",
    "        loss = F.mse_loss(pred, y_i)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            weight_list=[(name, param) for name, param in model.named_parameters() if name.split('.')[-1] == 'weight']\n",
    "            for l in weight_list:\n",
    "                self.log((\"train_mean\"+l[0]), l[1].mean())\n",
    "                self.log((\"train_std\"+l[0]), l[1].std())        \n",
    "        return(loss)\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        g_i, y_i = batch\n",
    "        pred, out = self.mod(g_i)\n",
    "        loss = F.mse_loss(pred, y_i)\n",
    "        self.log('val_loss', loss)        \n",
    "     \n",
    "    def configure_optimizers(self, **kwargs):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), **kwargs)\n",
    "        return optimizer    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a09fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epoch = 200\n",
    "DNNG = plDNN_ACGT(model)     \n",
    "optimizer = DNNG.configure_optimizers()\n",
    "\n",
    "logger = TensorBoardLogger(\"tb_logs\", name=\"g-acgt-hilb-res\")\n",
    "trainer = pl.Trainer(max_epochs=max_epoch, logger=logger)\n",
    "\n",
    "trainer.fit(model=DNNG, train_dataloaders=training_dataloader, val_dataloaders=validation_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952cfd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(DNNG.mod, cache_path+'g-acgt-hilb'+'.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
