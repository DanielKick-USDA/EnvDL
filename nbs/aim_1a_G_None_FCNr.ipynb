{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "source": [
    "# Fully Connected Genomic Model (No filtering) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy  as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from   torch import nn\n",
    "import torch.nn.functional as F\n",
    "from   torch.utils.data import Dataset\n",
    "from   torch.utils.data import DataLoader\n",
    "\n",
    "from EnvDL.dlfn import BigDataset, plDNN_general\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from   lightning.pytorch.loggers import CSVLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataG2F.core import get_data\n",
    "from dataG2F.qol  import ensure_dir_path_exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run settings: \n",
    "max_epoch  = 200\n",
    "batch_size = 48\n",
    "batch_size = 256\n",
    "\n",
    "max_epoch  = 20\n",
    "\n",
    "# run settings\n",
    "params_run = {\n",
    "    'max_epoch': 200,\n",
    "    'batch_size': 48,\n",
    "    'batch_size': 256,\n",
    "    'max_epoch': 20,   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data settings\n",
    "params_data = {\n",
    "    'y_var': 'Yield_Mg_ha',\n",
    "    'y_resid': 'None', # None, Env, Geno\n",
    "    'y_resid_strat': 'None', # None, naive_mean, filter_mean, ...\n",
    "    'holdout_parents': [\n",
    "        # Testers from 2020-2021\n",
    "        'PHZ51', \n",
    "        # 'PHP02', \n",
    "        # 'PHK76'\n",
    "\n",
    "    ],    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this file I define params later. I've included it here to gurantee that we can merge other params dicts into it.\n",
    "params = {\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_setting(settings_dict, key):\n",
    "#     if key in settings_dict.keys():\n",
    "#         return settings_dict[key]\n",
    "#     else:\n",
    "#         return None\n",
    "    \n",
    "# get_setting(settings_data, 'holdout_parents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_var = params_data['y_var']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_path = '../nbs_artifacts/aim_1a_G_None_FCN/'\n",
    "save_prefix = [e for e in cache_path.split('/') if e != ''][-1]\n",
    "#TODO append info on the residual to the save prefix name?\n",
    "\n",
    "ensure_dir_path_exists(dir_path = cache_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "use_gpu_num = 0\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if use_gpu_num in [0, 1]: \n",
    "    torch.cuda.set_device(use_gpu_num)\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_geno_lookup          = get_data('obs_geno_lookup')\n",
    "phno                     = get_data('phno')\n",
    "acgt                     = get_data('ACGT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten data\n",
    "acgt = acgt.reshape(4926, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make holdout sets\n",
    "# create a mask for parent genotype\n",
    "def  mask_parent(df_FM, holdout = 'PHZ51'):\n",
    "    holdout=   holdout.upper()\n",
    "    mask_F = tmp.F.str.upper() == holdout\n",
    "    mask_M = tmp.M.str.upper() == holdout\n",
    "    mask = (mask_F | mask_M)\n",
    "    return mask\n",
    "\n",
    "\n",
    "holdout_parents = params_data['holdout_parents']\n",
    "\n",
    "tmp = phno.loc[:, ['Env', 'Year', 'Hybrid']]\n",
    "tmp[['F', 'M']] = tmp['Hybrid'].str.split('/', n=1, expand=True)\n",
    "\n",
    "mask = pd.concat([\n",
    "    mask_parent(df_FM=tmp, \n",
    "                holdout=e) for e in holdout_parents], axis=1)\n",
    "\n",
    "test_idx  = mask.loc[(mask[0]),  ].index\n",
    "train_idx = mask.loc[(~mask[0]), ].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert y to residual if needed\n",
    "\n",
    "if params_data['y_resid'] == 'None':\n",
    "    pass\n",
    "else:\n",
    "    if params_data['y_resid_strat'] == 'naive_mean':\n",
    "        # use only data in the training set (especially since testers will be more likely to be found across envs)\n",
    "        # get enviromental means, subtract from observed value\n",
    "        tmp = phno.loc[train_idx, ]\n",
    "        env_mean = tmp.groupby(['Env_Idx']\n",
    "                     ).agg(Env_Mean = (y_var, 'mean')\n",
    "                     ).reset_index()\n",
    "        tmp = phno.merge(env_mean)\n",
    "        tmp.loc[:, y_var] = tmp.loc[:, y_var] - tmp.loc[:, 'Env_Mean']\n",
    "        phno = tmp.drop(columns='Env_Mean')\n",
    "\n",
    "    if params_data['y_resid_strat'] == 'filter_mean':\n",
    "        # for adjusting to environment we could use _all_ observations but ideally we will use the same set of genotypes across all observations\n",
    "        def minimum_hybrids_for_env(tmp = phno.loc[:, ['Env', 'Year', 'Hybrid']],\n",
    "                                    year = 2014):\n",
    "            # Within each year what hybrids are most common?\n",
    "            tmp = tmp.loc[(tmp.Year == year), ].groupby(['Env', 'Hybrid']).count().reset_index().sort_values('Year')\n",
    "\n",
    "            all_envs = set(tmp.Env)\n",
    "            # if we filter on the number of sites a hybrid is planted at, what is the largest number of sites we can ask for before we lose a location?\n",
    "            # site counts for sets which contain all envs\n",
    "            i = max([i for i in list(set(tmp.Year)) if len(set(tmp.loc[(tmp.Year >= i), 'Env'])) == len(all_envs)])\n",
    "\n",
    "            before = len(set(tmp.loc[:, 'Hybrid']))\n",
    "            after  = len(set(tmp.loc[(tmp.Year >= i), 'Hybrid']))\n",
    "            print(f'Reducing {year} hybrids from {before} to {after} ({round(100*after/before)}%).')\n",
    "            tmp = tmp.loc[(tmp.Year >= i), ['Env', 'Hybrid']].reset_index(drop=True)\n",
    "            return tmp\n",
    "\n",
    "\n",
    "        tmp = phno.loc[:, ['Env', 'Year', 'Hybrid']]\n",
    "        filter_hybrids = [minimum_hybrids_for_env(tmp = phno.loc[:, ['Env', 'Year', 'Hybrid']], year = i) \n",
    "                          for i in list(set(phno.Year)) ]\n",
    "        env_mean = pd.concat(filter_hybrids).merge(phno, how = 'left')\n",
    "\n",
    "        env_mean = env_mean.groupby(['Env_Idx']\n",
    "                          ).agg(Env_Mean = (y_var, 'mean')\n",
    "                          ).reset_index()\n",
    "\n",
    "        tmp = phno.merge(env_mean)\n",
    "        tmp.loc[:, y_var] = tmp.loc[:, y_var] - tmp.loc[:, 'Env_Mean']\n",
    "        phno = tmp.drop(columns='Env_Mean')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# center and y value data\n",
    "assert 0 == phno.loc[:, y_var].isna().sum()\n",
    "\n",
    "y = phno.loc[:, y_var]\n",
    "# use train index to prevent information leakage\n",
    "y_c = y[train_idx].mean()\n",
    "y_s = y[train_idx].std()\n",
    "\n",
    "y = (y - y_c)/y_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataloader = DataLoader(BigDataset(\n",
    "    lookups_are_filtered = False,\n",
    "    lookup_obs  = torch.from_numpy(np.array(train_idx)), \n",
    "    lookup_geno = torch.from_numpy(obs_geno_lookup),\n",
    "    y =           torch.from_numpy(y.to_numpy()).to(torch.float32)[:, None],\n",
    "    G =           torch.from_numpy(acgt).to(torch.float32),\n",
    "    G_type = 'raw',\n",
    "    send_batch_to_gpu = 'cuda:0'\n",
    "    ),\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "validation_dataloader = DataLoader(BigDataset(\n",
    "    lookups_are_filtered = False,\n",
    "    lookup_obs  = torch.from_numpy(np.array(test_idx)), \n",
    "    lookup_geno = torch.from_numpy(obs_geno_lookup),\n",
    "    y =           torch.from_numpy(y.to_numpy()).to(torch.float32)[:, None],\n",
    "    G =           torch.from_numpy(acgt).to(torch.float32),\n",
    "    G_type = 'raw',\n",
    "    send_batch_to_gpu = 'cuda:0'\n",
    "    ),\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Parameters of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convenince wrapper to fill in for R's seq or x:y notation\n",
    "def linrange(start, stop):\n",
    "    import numpy as np\n",
    "    diff = start - stop\n",
    "    res = np.linspace(start, stop, abs(diff)+1).astype(int)\n",
    "    return res "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2048, 1024, 512, 256, 128, 64, 32, 16]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[2**i for i in linrange(11, 4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This one is designed to go from (3x125891) -> ~1258.91  -> ~125.891 -> ~12.5891 -> 1\n",
    "layer_sizes = [1024, 128, 12]\n",
    "layer_drops = [0.1 for e in layer_sizes]\n",
    "\n",
    "num_layers = len(layer_sizes)\n",
    "\n",
    "params = {\n",
    "    'num_layers':num_layers,\n",
    "    f\"in_1_of_{num_layers}\": (4 * 125891)\n",
    "}\n",
    "\n",
    "for i in range(num_layers):\n",
    "    params[f\"out_{ i + 1}_of_{num_layers}\"] = layer_sizes[i]\n",
    "    params[f\"drop_{ i + 1}_of_{num_layers}\"] = layer_drops[i]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_layers': 3,\n",
       " 'in_1_of_3': 503564,\n",
       " 'out_1_of_3': 1024,\n",
       " 'drop_1_of_3': 0.1,\n",
       " 'out_2_of_3': 128,\n",
       " 'drop_2_of_3': 0.1,\n",
       " 'out_3_of_3': 12,\n",
       " 'drop_3_of_3': 0.1}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([256, 1]), torch.Size([256, 503564])]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[e.shape for e in next(iter(training_dataloader))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from EnvDL.dlfn import Linear_res_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A quirk of this is that to get only a single layer the length of the input tensor must be passed in. for 2+ I'll figure it out.\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, parameterization):\n",
    "        super(NeuralNetwork, self).__init__()            \n",
    "        module_list = []\n",
    "\n",
    "        max_layer = parameterization['num_layers']\n",
    "        for i in range(max_layer):\n",
    "            if i  == 0:\n",
    "                name_in = f\"in_{i+1}_of_{max_layer}\"\n",
    "            else:\n",
    "                name_in = f\"out_{i}_of_{max_layer}\"\n",
    "            name_out = f\"out_{i+1}_of_{max_layer}\"\n",
    "            name_drop= f\"drop_{i+1}_of_{max_layer}\"\n",
    "\n",
    "            if i == 0:\n",
    "                module_list += [nn.Flatten()]\n",
    "            \n",
    "\n",
    "            module_list += [\n",
    "                Linear_res_block(\n",
    "                    in_size  = parameterization[name_in], \n",
    "                    out_size = parameterization[name_out], \n",
    "                    drop_pr  = parameterization[name_drop])]\n",
    "            \n",
    "            if (i+1) == max_layer:\n",
    "                module_list += [nn.Linear(parameterization[name_out], 1)]\n",
    "                \n",
    "        self.x_network = nn.ModuleList(module_list)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        for mod in self.x_network:\n",
    "            if mod == self.x_network[-1]:\n",
    "                out = x # get the penultimate layer's outputs for later\n",
    "            x = mod(x)\n",
    "        \n",
    "        pred = x\n",
    "        return pred#, out\n",
    "\n",
    "# model = NeuralNetwork(\n",
    "#     parameterization = parameterization).to(device)\n",
    "\n",
    "# model(next(iter(training_dataloader))[0][0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(parameterization = params).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1307],\n",
       "        [-0.1577],\n",
       "        [-0.1549],\n",
       "        [-0.1290],\n",
       "        [-0.1295]], device='cuda:0', grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model(next(iter(training_dataloader))[1])[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine parameter info\n",
    "params['params_data'] = params_data\n",
    "params['params_run']  = params_run\n",
    "\n",
    "\n",
    "# params_data\n",
    "# params_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name | Type          | Params\n",
      "---------------------------------------\n",
      "0 | mod  | NeuralNetwork | 517 M \n",
      "---------------------------------------\n",
      "517 M     Trainable params\n",
      "0         Non-trainable params\n",
      "517 M     Total params\n",
      "2,071.663 Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad313b76daaa4130999f2a0118074d14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "522579e4aba84bb294fbe2598cb8fd2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kickd/miniconda3/envs/fastai/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "DNN = plDNN_general(model)  \n",
    "\n",
    "optimizer = DNN.configure_optimizers()\n",
    "\n",
    "logger = CSVLogger(\"nifa_tb\", name=save_prefix)\n",
    "logger.log_hyperparams(params=params)\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=max_epoch, logger=logger)\n",
    "\n",
    "trainer.fit(model=DNN, train_dataloaders=training_dataloader, val_dataloaders=validation_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
