---
title: Benchmark DNNCO
jupyter: python3
---

```{python}
# changed optimizer, batch size, numEpoch
dataloader_batch_size = 112 #1000
run_epochs = 364
run_epochs = 20
epochs_run = 0
```

```{python}
# import time
# time.sleep(30*60)
```


> 


```{python}
import os, json, re

import numpy as np
import pandas as pd
pd.set_option('display.max_columns', None)

import plotly.express as px
import plotly.io as pio
pio.templates.default = "plotly_white"

import hilbertcurve
from hilbertcurve.hilbertcurve import HilbertCurve

from EnvDL.core import * # includes remove_matching_files
from EnvDL.dna import *
from EnvDL.dlfn import *

from tqdm import tqdm
```

```{python}
# dataloader_batch_size = 8 #16 #64
# run_epochs = 200

use_gpu_num = 0

# Imports --------------------------------------------------------------------
import torch
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
from torch import nn
import torch.nn.functional as F # F.mse_loss

import lightning.pytorch as pl
from lightning.pytorch.loggers import TensorBoardLogger


device = "cuda" if torch.cuda.is_available() else "cpu"
if use_gpu_num in [0, 1]: 
    torch.cuda.set_device(use_gpu_num)
print(f"Using {device} device")
```

```{python}
cache_path = '../nbs_artifacts/01.90_g2fc_benchmark_DNNCO_Learner/'
ensure_dir_path_exists(dir_path = cache_path)
```

## Load data

```{python}
load_from = '../nbs_artifacts/01.03_g2fc_prep_matrices/'
phno_geno = pd.read_csv(load_from+'phno_geno.csv')
phno = phno_geno

obs_env_lookup  = np.load(load_from+'obs_env_lookup.npy')
obs_geno_lookup = np.load(load_from+'obs_geno_lookup.npy') # Phno_Idx	Geno_Idx	Is_Phno_Idx
YMat = np.load(load_from+'YMat.npy')
SMat = np.load(load_from+'SMat.npy')
WMat = np.load(load_from+'WMat.npy')
# PlantHarvestNames = np.load(load_from+'PlantHarvestNames.npy') 
PlantHarvest = np.load(load_from+'PlantHarvest.npy')  # 'DOY_Planted', 'DOY_Harvested'
```

### Screeplot

```{python}
load_from = '../data/zma/g2fc/genotypes_G3_filter/'

GScree = pd.read_table(load_from+'Eigenvalues_Imputed_5_Genotype_Data_All_Years_G3_Hz_0.0675.txt',
                     sep = '\t',
                     low_memory = False)
GScree

fig = px.line(GScree, x = 'PC', y = 'cumulative proportion', log_x=True)
pr_bins = [(2+i)/10 for i in range(8)]
for i in range(len(pr_bins)-1):
    x_start = GScree.loc[(GScree['cumulative proportion'] >= pr_bins[i]), 'PC'].min()
    x_stop = GScree.loc[(GScree['cumulative proportion'] >= pr_bins[i+1]), 'PC'].min()
   
    if i%2 == 1:
        fig.add_vrect(x0=x_start, 
                      x1=x_stop, 
                      fillcolor="green", opacity=0.25, line_width=0)
    else:
        fig.add_vrect(x0=x_start, 
                      x1=x_stop, 
                      fillcolor="white", opacity=0.25, line_width=0)
fig.show()
```

### PCA Transformed Data

```{python}
GMat = pd.read_table(load_from+'PC_Imputed_5_Genotype_Data_All_Years_G3_Hz_0.0675.txt',
                     sep = '\t',
                     skiprows=2,
                    low_memory = False)
GMat.shape # two more entries than the data I've been using.
```

```{python}
# need to check taxa match
assert 0 == len([e for e in list(set(phno_geno['Hybrid'])) if e not in list(GMat['Taxa'])])
```

```{python}
# Get the desired genotypes in the desired order
temp = pd.concat([phno_geno['Hybrid'], pd.DataFrame(obs_geno_lookup)], axis = 1)
temp = temp.loc[:, ['Hybrid', 1]].drop_duplicates().sort_values(1).reset_index(drop = True)
temp
```

```{python}
# Reorder PCA data, then convert to numpy
GMat = temp.merge(GMat.rename(columns = {'Taxa':'Hybrid', 'index':'Gpca_Idx'})).drop(columns = 1)
GMatHybrid = GMat['Hybrid']
GMat = GMat.loc[:, [i for i in list(GMat) if i != 'Hybrid']]
```

```{python}
GMat = np.asarray(GMat)
```

```{python}
# We should now be able to use obs_geno_lookup to lookup PCA values.
assert GMat.shape[0] == len(set(obs_geno_lookup[:, 1]))
```


### Custom Dataloader for G S W

```{python}
# ACGTDataset
```

```{python}
# obs_geno_lookup
# YMat
# SMat
# WMat
# GMat
```

```{python}
window_start = PlantHarvest[:, 0]-76 # 75 before planting date
window_end   = PlantHarvest[:, 0]+212 # 212 after (288 total per W.npy in paper)
window_end - window_start
```

```{python}
# the increased range of planting dates causes the window to extend beyond the target year. 
```

```{python}
window_start.min()
```

```{python}
window_end.max() - 364 #pd.to_datetime() is 0 indexed
```

```{python}
window_start = PlantHarvest[:, 0]-60
window_end   = PlantHarvest[:, 0]+194
window_end - window_start
```

```{python}
assert window_start.min() >= 0 
```

```{python}
assert window_end.max() <= 364 #pd.to_datetime() is 0 indexed
```


```{python}
WMat.shape
```

```{python}
# obs_env_lookup  = np.load(load_from+'obs_env_lookup.npy')
# obs_geno_lookup = np.load(load_from+'obs_geno_lookup.npy') # Phno_Idx	Geno_Idx	Is_Phno_Idx
(obs_env_lookup.shape,
obs_geno_lookup.shape)
```



```{python}
class GSWDataset(Dataset): # for any G containing matix with many (phno) to one (geno)
    def __init__(self, 
                 y, 
                 G, # not on gpu
                 S,
                 W,
                 
                 planting, # should be the vector so [:, 0]
                 days_before_planting,
                 days_after_planting,
                 
                 idx_original,
                 idx_lookup_geno,
                 idx_lookup_env,
                 
                 transform = None, target_transform = None,
                 device = 'cuda',
                 **kwargs 
                ):

        self.device = device
        self.y = y 
        self.G = G
        self.S = S
        self.W = W
        
        self.window_start = planting - days_before_planting
        self.window_end   = planting + days_after_planting
        
        self.idx_original = idx_original
#         self.idx_lookup = idx_lookup
        self.idx_lookup_geno = idx_lookup_geno
        self.idx_lookup_env  = idx_lookup_env
        
        self.transform = transform
        self.target_transform = target_transform    
        
    def __len__(self):
        return len(self.y)
    
    def __getitem__(self, idx):
        y_idx =self.y[idx]
            
        #                 |array containing correct index in deduplicated g 
        #                 |               index in phno    
        uniq_g_idx = self.idx_lookup_geno[
            self.idx_original[idx], 1]
        g_idx = self.G[uniq_g_idx, :]
        
        
        #                 |array containing correct index in deduplicated w 
        #                 |               index in phno    
        uniq_env_idx = self.idx_lookup_env[self.idx_original[idx], 1]
        s_idx = self.S[uniq_env_idx, :]
        w_idx = self.W[uniq_env_idx, :, self.window_start[idx]:self.window_end[idx]]
        
        # send all to gpu        
        if (self.device != 'cpu'):
            if y_idx.device.type == 'cpu':
                y_idx = y_idx.to(self.device) 
                
            if g_idx.device.type == 'cpu':
                g_idx = g_idx.to(self.device) 
                
            if s_idx.device.type == 'cpu':
                s_idx = s_idx.to(self.device)                 
                
            if w_idx.device.type == 'cpu':
                w_idx = w_idx.to(self.device)         
        
        if self.transform:
            y_idx = self.transform(y_idx)
            g_idx = self.transform(g_idx)
            s_idx = self.transform(s_idx)
            w_idx = self.transform(w_idx)
            
        return y_idx, g_idx, s_idx, w_idx
    
# training_dataloader = DataLoader(
#     GSWDataset(
#         y = torch.from_numpy(YMat[[0, 1], ]), 
#         G = torch.from_numpy(GMat), 
#         S = torch.from_numpy(SMat),
#         W = torch.from_numpy(WMat),
#         planting = PlantHarvest[:, 0],
#         days_before_planting = 60,
#         days_after_planting = 194,
#         idx_original = torch.from_numpy(np.array([0, 1]))[:, None],
#         idx_lookup_geno = obs_geno_lookup,
#         idx_lookup_env = obs_env_lookup,
#         device = 'cuda'),
#     batch_size = 1,
#     shuffle = True
# )
# next(iter(training_dataloader))
```




```{python}
## Create train/test validate indicies from json
load_from = '../nbs_artifacts/01.06_g2fc_cluster_genotypes/'
```

```{python}
def read_json(json_path):
    with open(json_path, 'r') as fp:
        dat = json.load(fp)
    return(dat)

def read_split_info(
    load_from = '../nbs_artifacts/01.06_g2fc_cluster_genotypes/',
    json_prefix = '2023:9:5:12:8:26'):
    jsons = [e for e in os.listdir(load_from) if re.match('^'+json_prefix+'.+\.json$', e)]
    vals = [e for e in jsons if re.match('.+val\d+\.json$', e)]
    vals.sort()
    out = {}
    out['test'] = [read_json(json_path = load_from+json_prefix+'-test.json')]
    out['test_file'] = [json_prefix+'-test.json']
    out['validate'] = [read_json(json_path = load_from+val) for val in vals]
    out['validate_files'] = [val for val in vals]
    return(out)

def find_idxs_split_dict(
    obs_df, # assumes presence of Year, Female, Male
    split_dict # from read_split_info() output. Should be a test of validate dict.
):

    temp = obs_df
    test_mask = ((temp.Year.isin(split_dict['test_years'])) & 
                 ((temp.Female.isin(split_dict['test_parents'])) |
                  (temp.Male.isin(split_dict['test_parents']))))
    temp['Split'] = ''
    temp.loc[test_mask, 'Split'] = 'Test'

    train_mask = (~(temp.Year.isin(split_dict['test_years'])) & 
                 (~((temp.Female.isin(split_dict['test_parents'])) |
                  (temp.Male.isin(split_dict['test_parents'])))))
    temp.loc[train_mask, 'Split'] = 'Train'

    temp_test  = (temp.Split == 'Test') # should be the same as with the mask above
    temp_train = (temp.Split == 'Train') # should be the same as with the mask above

    # Confirm that there's no overlap in parents or years
    temp_test_parents  = set(temp.loc[temp_test, 'Female']+temp.loc[temp_test, 'Male'])
    temp_train_parents = set(temp.loc[temp_train, 'Female']+temp.loc[temp_train, 'Male'])

    temp_test_years  = set(temp.loc[temp_test, 'Year'])
    temp_train_years = set(temp.loc[temp_train, 'Year'])

    assert [] == [e for e in temp_test_parents if e in temp_train_parents]
    assert [] == [e for e in temp_train_parents if e in temp_test_parents]
    assert [] == [e for e in temp_test_years if e in temp_train_years]
    assert [] == [e for e in temp_train_years if e in temp_test_years]

    return({
        'test_idx': temp.loc[test_mask, ].index, 
        'train_idx': temp.loc[train_mask, ].index} )
```

```{python}
split_info = read_split_info(
    load_from = '../nbs_artifacts/01.06_g2fc_cluster_genotypes/',
    json_prefix = '2023:9:5:12:8:26')
```

```{python}
temp = phno.copy()
temp[['Female', 'Male']] = temp['Hybrid'].str.split('/', expand = True)
```

```{python}
test_dict = find_idxs_split_dict(
    obs_df = temp, 
    split_dict = split_info['test'][0]
)
# test_dict
```

```{python}
# since this is applying predefined model structure no need for validation.
# This is included for my future reference when validation is needed.
temp = temp.loc[test_dict['train_idx'], ] # restrict before re-aplying

val_dict = find_idxs_split_dict(
    obs_df = temp, 
    split_dict = split_info['validate'][0]
)
# val_dict
```

```{python}
test_dict
```

```{python}
train_idx = test_dict['train_idx']
test_idx  = test_dict['test_idx']
```

```{python}
# confirm all observation idxs are have genomic information
assert [] == [e for e in list(train_idx)+list(test_idx) if e not in obs_geno_lookup[:, 0]]
```

```{python}
YMat_cs = calc_cs(YMat[train_idx])
y_cs = apply_cs(YMat, YMat_cs)
```

```{python}
SMat_cs = calc_cs(SMat[list(set(obs_env_lookup[train_idx, 1])), ])
s_cs = apply_cs(SMat, SMat_cs)

WMat_cs = calc_cs(WMat[list(set(obs_env_lookup[train_idx, 1])), ])
w_cs = apply_cs(WMat, WMat_cs)
```

```{python}
GMat_cs = calc_cs(GMat[list(set(obs_geno_lookup[train_idx, 1])), ])
g_cs = apply_cs(GMat, GMat_cs)
```

```{python}
training_dataloader = DataLoader(
    GSWDataset(
        y = torch.from_numpy(y_cs[train_idx])[:, None].to(torch.float32), #torch.from_numpy(YMat[[0, 1], ]), 
        G = torch.from_numpy(g_cs).to(torch.float32), 
        S = torch.from_numpy(s_cs).to(torch.float32),
        W = torch.from_numpy(w_cs).to(torch.float32),
        planting = PlantHarvest[:, 0],
        days_before_planting = 60,
        days_after_planting = 194,
        idx_original = torch.from_numpy(np.array(train_idx)),#[:, None],
        idx_lookup_geno = obs_geno_lookup,
        idx_lookup_env = obs_env_lookup,
        device = 'cuda'),
    batch_size = dataloader_batch_size,
    shuffle = True
)
# next(iter(training_dataloader))

testing_dataloader = DataLoader(
    GSWDataset(
        y = torch.from_numpy(y_cs[test_idx])[:, None].to(torch.float32), #torch.from_numpy(YMat[[0, 1], ]), 
        G = torch.from_numpy(g_cs).to(torch.float32), 
        S = torch.from_numpy(s_cs).to(torch.float32),
        W = torch.from_numpy(w_cs).to(torch.float32),
        planting = PlantHarvest[:, 0],
        days_before_planting = 60,
        days_after_planting = 194,
        idx_original = torch.from_numpy(np.array(test_idx)),#[:, None],
        idx_lookup_geno = obs_geno_lookup,
        idx_lookup_env = obs_env_lookup,
        device = 'cuda'),
    batch_size = dataloader_batch_size,
    shuffle = True
)
# next(iter(testing_dataloader))
```

```{python}
# training_dataloader = DataLoader(
#     GSWDataset(
#         y = torch.from_numpy(y_cs[train_idx])[:, None].to(torch.float32), #torch.from_numpy(YMat[[0, 1], ]), 
#         G = torch.from_numpy(GMat).to(torch.float32), 
#         S = torch.from_numpy(SMat).to(torch.float32),
#         W = torch.from_numpy(WMat).to(torch.float32),
#         planting = PlantHarvest[:, 0],
#         days_before_planting = 60,
#         days_after_planting = 194,
#         idx_original = torch.from_numpy(np.array(train_idx)),#[:, None],
#         idx_lookup_geno = obs_geno_lookup,
#         idx_lookup_env = obs_env_lookup,
#         device = 'cuda'),
#     batch_size = 1000,
#     shuffle = True
# )
# # next(iter(training_dataloader))

# testing_dataloader = DataLoader(
#     GSWDataset(
#         y = torch.from_numpy(y_cs[test_idx])[:, None].to(torch.float32), #torch.from_numpy(YMat[[0, 1], ]), 
#         G = torch.from_numpy(GMat).to(torch.float32), 
#         S = torch.from_numpy(SMat).to(torch.float32),
#         W = torch.from_numpy(WMat).to(torch.float32),
#         planting = PlantHarvest[:, 0],
#         days_before_planting = 60,
#         days_after_planting = 194,
#         idx_original = torch.from_numpy(np.array(test_idx)),#[:, None],
#         idx_lookup_geno = obs_geno_lookup,
#         idx_lookup_env = obs_env_lookup,
#         device = 'cuda'),
#     batch_size = 1000,
#     shuffle = True
# )
# next(iter(testing_dataloader))
```

```{python}
y_idx, g_idx, s_idx, w_idx = next(iter(training_dataloader))
```

## Use Pytorch Lightning to Train each Model

```{python}
def Linear_block(in_size, out_size, drop_pr):
    block = nn.Sequential(
        nn.Linear(in_size, out_size),
        nn.ReLU(),
        nn.Dropout(drop_pr)
    )
    return(block) 

def Conv1D_x2_Max_block(in_channels, out_channels, kernel_size, stride, maxpool_size):
    block = nn.Sequential(
        nn.Conv1d(
            in_channels= in_channels, # second channel
            out_channels= out_channels,
            kernel_size= kernel_size,
            stride= stride
        ), 
        nn.Conv1d(
            in_channels= out_channels, 
            out_channels= out_channels,
            kernel_size= kernel_size,
            stride= stride
        ), 
        nn.BatchNorm1d(out_channels),
        nn.MaxPool1d((maxpool_size), stride=stride)
    )
    return(block)
```

```{python}
class DNNCO_G(nn.Module):
    def __init__(self):
        super(DNNCO_G, self).__init__()    
        
        self.g_network =nn.Sequential(
            Linear_block(in_size= 4881, out_size= 83, drop_pr= 0.163923177),
            Linear_block(in_size= 83,   out_size= 133, drop_pr= 0.230663142)
        ) 
        self.g_pred =nn.Sequential(
            nn.Linear(133, 1)
        ) 
        
        
    def forward(self, g, s, w):
        g_out = self.g_network(g)
        g_pred = self.g_pred(g_out)
        return g_pred, g_out
    
# [e.shape for e in DNNCO_G().to(device)(g_idx, s_idx, w_idx)]    
```

```{python}
class DNNCO_S(nn.Module):
    def __init__(self):
        super(DNNCO_S, self).__init__()    
        
        self.s_network =nn.Sequential(
            Linear_block(in_size= 23, out_size= 38, drop_pr= 0.148724301),
            Linear_block(in_size= 38, out_size= 13, drop_pr= 0.276340999),
            Linear_block(in_size= 13, out_size= 45, drop_pr= 0.005434164),
            Linear_block(in_size= 45, out_size= 29, drop_pr= 0.173380695),
            Linear_block(in_size= 29, out_size= 4,  drop_pr= 0.),
            Linear_block(in_size= 4,  out_size= 4,  drop_pr= 0.),
            Linear_block(in_size= 4,  out_size= 4,  drop_pr= 0.)
        )  
        self.s_pred =nn.Sequential(
            nn.Linear(4, 1)
        ) 
        
    def forward(self, g, s, w):
        s_out = self.s_network(s)
        s_pred = self.s_pred(s_out)
        return s_pred, s_out
    
# [e.shape for e in DNNCO_S().to(device)(g_idx, s_idx, w_idx)]
```

```{python}
class DNNCO_W(nn.Module):
    def __init__(self):
        super(DNNCO_W, self).__init__()  
        
        self.w_network =nn.Sequential(
            Conv1D_x2_Max_block(
                in_channels = 16, 
                out_channels = 433, 
                kernel_size = 3,
                maxpool_size = 2,
                stride = 1),
            Conv1D_x2_Max_block(
                in_channels = 433, 
                out_channels = 436, 
                kernel_size = 3,
                maxpool_size = 2,
                stride = 1),
            Conv1D_x2_Max_block(
                in_channels = 436, 
                out_channels = 52, 
                kernel_size = 3,
                maxpool_size = 2,
                stride = 1),
            Conv1D_x2_Max_block(
                in_channels = 52, 
                out_channels = 163, 
                kernel_size = 3,
                maxpool_size = 2,
                stride = 1),
            Conv1D_x2_Max_block(
                in_channels = 163, 
                out_channels = 400, 
                kernel_size = 3,
                maxpool_size = 2,
                stride = 1),
            Conv1D_x2_Max_block(
                in_channels = 400, 
                out_channels = 294, 
                kernel_size = 3,
                maxpool_size = 2,
                stride = 1)
        )
        
        self.w_flatten =nn.Flatten()

        self.w_pred =nn.Sequential(
            nn.Linear(65856, 1)
        )
        
    def forward(self, g, s, w):
        w_out = self.w_network(w)
        w_out = self.w_flatten(w_out)
        w_pred = self.w_pred(w_out)
        return w_pred, w_out
    
# [e.shape for e in DNNCO_W().to(device)(g_idx, s_idx, w_idx)]
```

```{python}
# [[e.shape for e in DNNCO_G().to(device)(g_idx, s_idx, w_idx)],
#  [e.shape for e in DNNCO_S().to(device)(g_idx, s_idx, w_idx)],
#  [e.shape for e in DNNCO_W().to(device)(g_idx, s_idx, w_idx)]]
```

```{python}
class DNNCO_X(nn.Module):
    def __init__(self):
        super(DNNCO_X, self).__init__()  
        
        self.x_network =nn.Sequential(
            Linear_block(in_size= 65993, out_size= 152, drop_pr= 0.18658661),
            Linear_block(in_size= 152,   out_size= 207, drop_pr= 0.289893588),
            Linear_block(in_size= 207,   out_size= 206, drop_pr= 0.004841293),
            Linear_block(in_size= 206,   out_size= 188, drop_pr= 0.198121953),
            Linear_block(in_size= 188,   out_size= 44,  drop_pr= 0.243027717),
            Linear_block(in_size= 44,    out_size= 1,   drop_pr= 0.0)
        )  
        
    def forward(self, g, s, w):        
        x_pred = self.x_network(torch.concat([g, s, w], axis = 1))
        return x_pred, None
    
# DNNCO_X().to(device)(
#     DNNCO_G().to(device)(g_idx, s_idx, w_idx)[1],
#     DNNCO_S().to(device)(g_idx, s_idx, w_idx)[1],
#     DNNCO_W().to(device)(g_idx, s_idx, w_idx)[1])[0].shape   
```

```{python}
# Module for training subnetworks.
class plDNNCO_subnet(pl.LightningModule):
    def __init__(self, mod):
        super().__init__()
        self.mod = mod
    def training_step(self, batch, batch_idx):
        # train loop
        y_i, g_i, s_i, w_i = batch
        pred, out = self.mod(g_i, s_i, w_i)
        # print(y_i.shape, pred.shape)
        loss = F.mse_loss(pred, y_i)
        self.log("train_loss", loss)
        return(loss)
        
#     def configure_optimizers(self):
#         optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)
#         return optimizer
    
    def configure_optimizers(self, **kwargs):
        optimizer = torch.optim.Adam(self.parameters(), **kwargs)
        return optimizer    
    
    
# DNNG = plDNNCO_G(DNNCO_G().to(device))

# optimizer = DNNG.configure_optimizers()

# for batch_idx, batch in tqdm(enumerate(training_dataloader)):
#     loss = DNNG.training_step(batch, batch_idx)
#     loss.backward()
#     optimizer.step()
#     optimizer.zero_grad()    
```

```{python}
# Module for training Interaction networks. 
# This version uses the subnetworks so it would be good for predicting new data but this could be
# reformulated to instead use predictions from the subnetworks which would be more preformant.
class plDNNCO_X(pl.LightningModule):
    def __init__(self, mod, Gmod, Smod, Wmod):
        super().__init__()
        self.mod = mod
        self.Gmod = Gmod
        self.Smod = Smod
        self.Wmod = Wmod
    def training_step(self, batch, batch_idx):
        # train loop
        y_i, g_i, s_i, w_i = batch
        
        with torch.no_grad():
            _, g_out = self.Gmod(g_i, s_i, w_i)            
            _, s_out = self.Smod(g_i, s_i, w_i)            
            _, w_out = self.Wmod(g_i, s_i, w_i)            
        
        pred, out = self.mod(g_out, s_out, w_out)
        # print(y_i.shape, pred.shape)
        loss = F.mse_loss(pred, y_i)
        self.log("train_loss", loss)
        return(loss)
        
    def configure_optimizers(self, **kwargs):
        optimizer =  torch.optim.Adam(self.parameters(), **kwargs)
        return optimizer
    
```

### Train with logger

```{python}
# Table3 in G3 paper https://academic.oup.com/g3journal/article/13/4/jkad006/6982634
adam_params ={
          #lr,     beta1,    beta2,    batch_size, max_epoch
    'g':  [0.0001, 0.953368, 0.985947,  96,  12 ],
    's':  [0.01,   0.928472, 0.997516, 176, 199],
    'w':  [0.0001, 0.903649, 0.929582, 240, 629],
    'co': [0.01,   0.98752,  0.972311, 112, 364],
    'so': [0.001,  0.975893, 0.994607, 192, 711]}
```

```{python}
subnet = 'g'                                                         # 0. Update
lr, beta1, beta2, batch_size, max_epoch = adam_params[subnet]
max_epoch = 2 #FIXME
training_dataloader = DataLoader(
    GSWDataset(
        y = torch.from_numpy(y_cs[train_idx])[:, None].to(torch.float32), #torch.from_numpy(YMat[[0, 1], ]), 
        G = torch.from_numpy(g_cs).to(torch.float32), 
        S = torch.from_numpy(s_cs).to(torch.float32),
        W = torch.from_numpy(w_cs).to(torch.float32),
        planting = PlantHarvest[:, 0],
        days_before_planting = 60,
        days_after_planting = 194,
        idx_original = torch.from_numpy(np.array(train_idx)),#[:, None],
        idx_lookup_geno = obs_geno_lookup,
        idx_lookup_env = obs_env_lookup,
        device = 'cuda'),
    batch_size = batch_size,
    shuffle = True
)

DNNG = plDNNCO_subnet(DNNCO_G().to(device))                          # 1. Update
optimizer = DNNG.configure_optimizers(lr = lr, betas=(beta1, beta2)) # 2. Update

logger = TensorBoardLogger("tb_logs", name="dnnco-g")                # 3. Update
trainer = pl.Trainer(max_epochs=max_epoch, logger=logger)

trainer.fit(model=DNNG, train_dataloaders=training_dataloader)       # 4. Update
```

```{python}
subnet = 's'                                                         # 0. Update
lr, beta1, beta2, batch_size, max_epoch = adam_params[subnet]
max_epoch = 2 #FIXME
training_dataloader = DataLoader(
    GSWDataset(
        y = torch.from_numpy(y_cs[train_idx])[:, None].to(torch.float32), #torch.from_numpy(YMat[[0, 1], ]), 
        G = torch.from_numpy(g_cs).to(torch.float32), 
        S = torch.from_numpy(s_cs).to(torch.float32),
        W = torch.from_numpy(w_cs).to(torch.float32),
        planting = PlantHarvest[:, 0],
        days_before_planting = 60,
        days_after_planting = 194,
        idx_original = torch.from_numpy(np.array(train_idx)),#[:, None],
        idx_lookup_geno = obs_geno_lookup,
        idx_lookup_env = obs_env_lookup,
        device = 'cuda'),
    batch_size = batch_size,
    shuffle = True
)

DNNS = plDNNCO_subnet(DNNCO_S().to(device))                          # 1. Update
optimizer = DNNS.configure_optimizers(lr = lr, betas=(beta1, beta2)) # 2. Update

logger = TensorBoardLogger("tb_logs", name="dnnco-s")                # 3. Update
trainer = pl.Trainer(max_epochs=max_epoch, logger=logger)

trainer.fit(model=DNNS, train_dataloaders=training_dataloader)       # 4. Update
```

```{python}
subnet = 'w'                                                         # 0. Update
lr, beta1, beta2, batch_size, max_epoch = adam_params[subnet]
max_epoch = 2 #FIXME
training_dataloader = DataLoader(
    GSWDataset(
        y = torch.from_numpy(y_cs[train_idx])[:, None].to(torch.float32), #torch.from_numpy(YMat[[0, 1], ]), 
        G = torch.from_numpy(g_cs).to(torch.float32), 
        S = torch.from_numpy(s_cs).to(torch.float32),
        W = torch.from_numpy(w_cs).to(torch.float32),
        planting = PlantHarvest[:, 0],
        days_before_planting = 60,
        days_after_planting = 194,
        idx_original = torch.from_numpy(np.array(train_idx)),#[:, None],
        idx_lookup_geno = obs_geno_lookup,
        idx_lookup_env = obs_env_lookup,
        device = 'cuda'),
    batch_size = batch_size,
    shuffle = True
)

DNNW = plDNNCO_subnet(DNNCO_W().to(device))                          # 1. Update
optimizer = DNNW.configure_optimizers(lr = lr, betas=(beta1, beta2)) # 2. Update

logger = TensorBoardLogger("tb_logs", name="dnnco-w")                # 3. Update
trainer = pl.Trainer(max_epochs=max_epoch, logger=logger)

trainer.fit(model=DNNW, train_dataloaders=training_dataloader)       # 4. Update
```

```{python}
# Now freeze layers to be extra sure they are not updated
for param in DNNG.mod.parameters():
    param.requires_grad = False
    
for param in DNNS.mod.parameters():
    param.requires_grad = False
    
for param in DNNW.mod.parameters():
    param.requires_grad = False
```

```{python}
# DNNS = plDNNCO_subnet(DNNCO_S().to(device))
# optimizer = DNNS.configure_optimizers()
# logger = TensorBoardLogger("tb_logs", name="dnnco-sonly")
# trainer = pl.Trainer(#limit_train_batches=100, 
#                      max_epochs=10, logger=logger)
# trainer.fit(model=DNNS, train_dataloaders=training_dataloader)
```

```{python}
# DNNW = plDNNCO_subnet(DNNCO_W().to(device))
# optimizer = DNNW.configure_optimizers()
# logger = TensorBoardLogger("tb_logs", name="dnnco-wonly")
# trainer = pl.Trainer(#limit_train_batches=100, 
#                      max_epochs=10, logger=logger)
# trainer.fit(model=DNNW, train_dataloaders=training_dataloader)
```

```{python}
subnet = 'co'                                                        # 0. Update
lr, beta1, beta2, batch_size, max_epoch = adam_params[subnet]
max_epoch = 2 #FIXME
training_dataloader = DataLoader(
    GSWDataset(
        y = torch.from_numpy(y_cs[train_idx])[:, None].to(torch.float32), #torch.from_numpy(YMat[[0, 1], ]), 
        G = torch.from_numpy(g_cs).to(torch.float32), 
        S = torch.from_numpy(s_cs).to(torch.float32),
        W = torch.from_numpy(w_cs).to(torch.float32),
        planting = PlantHarvest[:, 0],
        days_before_planting = 60,
        days_after_planting = 194,
        idx_original = torch.from_numpy(np.array(train_idx)),#[:, None],
        idx_lookup_geno = obs_geno_lookup,
        idx_lookup_env = obs_env_lookup,
        device = 'cuda'),
    batch_size = batch_size,
    shuffle = True
)

# DNNW = plDNNCO_subnet(DNNCO_W().to(device))                          # 1. Update
# optimizer = DNNW.configure_optimizers(lr = lr, betas=(beta1, beta2)) # 2. Update

# logger = TensorBoardLogger("tb_logs", name="dnnco-w")                # 3. Update
# trainer = pl.Trainer(max_epochs=max_epoch, logger=logger)

# trainer.fit(model=DNNW, train_dataloaders=training_dataloader)       # 4. Update


DNNX = plDNNCO_X(DNNCO_X().to(device), Gmod = DNNG.mod, Smod = DNNS.mod, Wmod = DNNW.mod)
optimizer = DNNX.configure_optimizers(lr = lr, betas=(beta1, beta2))

logger = TensorBoardLogger("tb_logs", name="dnnco-x")
trainer = pl.Trainer(max_epochs=max_epoch, logger=logger)

trainer.fit(model=DNNX, train_dataloaders=training_dataloader)
```


```{python}
DNNX = plDNNCO_X(DNNCO_X().to(device), Gmod = DNNG.mod, Smod = DNNS.mod, Wmod = DNNW.mod)
optimizer = DNNS.configure_optimizers()
logger = TensorBoardLogger("tb_logs", name="dnnco-x2")
trainer = pl.Trainer(#limit_train_batches=100, 
                     max_epochs=10, logger=logger)
trainer.fit(model=DNNX, train_dataloaders=training_dataloader)
```


```{python}
# from lightning.pytorch import Trainer

# logger = TensorBoardLogger("tb_logs", name="dnnco-gonly")
# # trainer = Trainer(logger=logger)
# # train the model (hint: here are some helpful Trainer arguments for rapid idea iteration)
# trainer = pl.Trainer(#limit_train_batches=100, 
#                      max_epochs=10, logger=logger)
# trainer.fit(model=DNNG, train_dataloaders=training_dataloader)
```
















```{python}
def train_loop_yx(dataloader, model, loss_fn, optimizer, silent = False):
    import torch
    from torch.utils.data import Dataset
    from torch.utils.data import DataLoader
    size = len(dataloader.dataset)
    for batch, (y_i, g_i, s_i, w_i) in enumerate(dataloader):
        # Compute prediction and loss
        pred = model(g_i, s_i, w_i )
        loss = loss_fn(pred, y_i)

        # Backpropagation
#         torch.autograd.set_detect_anomaly(True)
        optimizer.zero_grad()
#         loss.backward(retain_graph=True)
        loss.backward()
        optimizer.step()

        if batch % 100 == 0:
            loss, current = loss.item(), batch * len(y_i)
            if not silent:
                print(f"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]")
```


```{python}
def train_error_yx(dataloader, model, loss_fn, silent = False):
    import torch
    from torch.utils.data import Dataset
    from torch.utils.data import DataLoader
    
    size = len(dataloader.dataset)
    num_batches = len(dataloader)
    train_loss = 0

    with torch.no_grad():
        for y_i, g_i, s_i, w_i in dataloader:
            pred = model(g_i, s_i, w_i)
            train_loss += loss_fn(pred, y_i).item() # <----------------------
            
    train_loss /= num_batches
    return(train_loss)
```

```{python}
def test_loop_yx(dataloader, model, loss_fn, silent = False):
    import torch
    from torch.utils.data import Dataset
    from torch.utils.data import DataLoader
    
    size = len(dataloader.dataset)
    num_batches = len(dataloader)
    test_loss = 0

    with torch.no_grad():
        for y_i, g_i, s_i, w_i in dataloader:
            pred = model(g_i, s_i, w_i)                
            test_loss += loss_fn(pred, y_i).item() # <-----------------------

    test_loss /= num_batches
    if not silent:
        print(f"Test Error: Avg loss: {test_loss:>8f}")
    return(test_loss)
```

```{python}
def train_nn_yx(
    cache_path,
    training_dataloader,
    testing_dataloader,
    model,
    learning_rate = 1e-3,
    batch_size = 64,
    epochs = 500,
    model_prefix = 'model',
    save_pt = False
):
    import numpy as np
    import pandas as pd
    import torch
    from torch import nn
    from tqdm import tqdm
    
    # Initialize the loss function
    loss_fn = nn.MSELoss()

#     optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas= [0.98752, 0.972311])

    loss_df = pd.DataFrame([i for i in range(epochs)], columns = ['Epoch'])
    loss_df['TrainMSE'] = np.nan
    loss_df['TestMSE']  = np.nan

    for t in tqdm(range(epochs)):        
#         print(f"Epoch {t+1}\n-------------------------------")
        train_loop_yx(training_dataloader, model, loss_fn, optimizer, silent = True)

        loss_df.loc[loss_df.index == t, 'TrainMSE'
                   ] = train_error_yx(training_dataloader, model, loss_fn, silent = True)
        
        loss_df.loc[loss_df.index == t, 'TestMSE'
                   ] = test_loop_yx(testing_dataloader, model, loss_fn, silent = True)
        
        if (t+1)%5 == 0: # Cache in case training is interupted. 
            # print(loss_df.loc[loss_df.index == t, ['TrainMSE', 'TestMSE']])
            if save_pt:
                torch.save(model.state_dict(), 
                           cache_path+'/'+model_prefix+'_'+str(t)+'_'+str(epochs)+'.pt') # convention is to use .pt or .pth
        
    return([model, loss_df])
```


```{python}
# model, loss_df = train_nn_yx(
#     cache_path,
#     training_dataloader,
#     training_dataloader, # For demo, the training and testing data are the same.
#     model,
#     learning_rate = 1e-3,
#     batch_size = dataloader_batch_size,
#     epochs = (run_epochs - epochs_run)
# )

# px.line(loss_df, x = 'Epoch', y = 'TrainMSE')
```

```{python}
def yhat_loop_yx(dataloader, model):
    import numpy as np
    import pandas as pd
    import torch
    
    size = len(dataloader.dataset)
    num_batches = len(dataloader)
    
    y_true = np.array([])
    y_pred = np.array([])
    
    with torch.no_grad():
        for y_i, g_i, s_i, w_i in dataloader:
            yhat_i = model(g_i, s_i, w_i)
            y_pred = np.append(y_pred, np.array(yhat_i.cpu()))
            y_true = np.append(y_true, np.array(y_i.cpu()))
    
    out = np.concatenate([y_true[:, None], y_pred[:, None]], axis = 1) 
    out = pd.DataFrame(out, columns = ['y_true', 'y_pred'])
    return(out)
```


```{python}
[os.unlink(cache_path+e) for e in os.listdir(cache_path)]
```

```{python}
# dataloader_batch_size = 50
# run_epochs = 100

# don't run if either of these exist because there may be cases where we want the results but not the model
if not os.path.exists(cache_path+'/model.pt'): 
    # Shared setup (train from scratch and load latest)
    model = NeuralNetwork()

    # find the biggest model to save
    saved_models = os.listdir(cache_path)
    saved_models = [e for e in saved_models if re.match('model*', e)]

    if saved_models == []:
        epochs_run = 0
    else:
        saved_models = [e for e in saved_models if e != 'model.pt']
        # if there are saved models reload and resume training
        saved_models_numbers = [int(e.replace('model_', ''
                                    ).replace('.pt', ''
                                    ).split('_')[0]) for e in saved_models]
        # saved_models
        epochs_run = max(saved_models_numbers)+1 # add 1 to account for 0 index
        latest_model = [e for e in saved_models if re.match(
            '^model_'+str(epochs_run-1)+'_.*\.pt$', e)][0] # subtract 1 to convert back
        model.load_state_dict(torch.load(cache_path+latest_model))
        print('Resuming Training: '+str(epochs_run)+'/'+str(run_epochs)+' epochs run.')
    
    model.to(device)    

    model, loss_df = train_nn_yx(
        cache_path,
        training_dataloader,
        testing_dataloader,
        model,
        learning_rate = 1e-3,
        batch_size = dataloader_batch_size,
        epochs = (run_epochs - epochs_run)
    )
    
    # experimental outputs:
    # 1. Model
    torch.save(model.state_dict(), cache_path+'/model.pt') # convention is to use .pt or .pth

    # 2. loss_df    
    # If this is resuming training, load and extend the existing loss dataframe
    if os.path.exists(cache_path+'/loss_df.csv'):
        loss_df_on_disk = pd.read_csv(cache_path+'/loss_df.csv')
        epoch_offset = 1 + loss_df_on_disk['Epoch'].max()
        loss_df['Epoch'] = loss_df['Epoch'] + epoch_offset
        loss_df = pd.concat([loss_df_on_disk, loss_df])
    loss_df.to_csv(cache_path+'/loss_df.csv', index=False)  
    
    # 3. predictions 
    yhats = pd.concat([
        yhat_loop_yx(testing_dataloader, model).assign(Split = 'Test'),
        yhat_loop_yx(training_dataloader, model).assign(Split = 'Train')], axis = 0)

    yhats.to_csv(cache_path+'/yhats.csv', index=False)
```


```{python}
estimate_iterations(sec_per_it = 84)
```

```{python}
# remove_matching_files(
#     cache_path,
#     match_regex_list = ['model\.pt'],
#     dry_run = False
# )
```

### Standard Visualizations

```{python}
scale_dict = {'y1':YMat_cs}
import plotly.graph_objects as go
```

```{python}
naieve_yhat = training_dataloader.dataset.y.mean()

naieve_MSE_Train = reverse_cs( 
    np.array(((naieve_yhat - training_dataloader.dataset.y)**2)).mean(),
    scale_dict['y1']
)

naieve_MSE_Test = reverse_cs( 
    np.array(((naieve_yhat - testing_dataloader.dataset.y)**2)).mean(),
    scale_dict['y1']
)

naieve_MSE_Train, naieve_MSE_Test



loss_df = pd.read_csv(cache_path+'/loss_df.csv')

loss_df.TrainMSE = reverse_cs(loss_df.TrainMSE, scale_dict['y1'])
loss_df.TestMSE  = reverse_cs(loss_df.TestMSE , scale_dict['y1'])


fig = go.Figure()
fig.add_trace(go.Scatter(x=loss_df.Epoch, y=loss_df.TestMSE,
                    mode='lines', name='Test'))
fig.add_trace(go.Scatter(x=loss_df.Epoch, y=loss_df.TrainMSE,
                    mode='lines', name='Train'))

fig.add_trace(go.Scatter(x=loss_df.Epoch, y=[naieve_MSE_Test  for e in range(len(loss_df.Epoch))], 
                         mode='lines', name='Naieve Test'))
fig.add_trace(go.Scatter(x=loss_df.Epoch, y=[naieve_MSE_Train for e in range(len(loss_df.Epoch))], 
                         mode='lines', name='Naieve Train'))
fig.show()
```

```{python}
yhats = pd.read_csv(cache_path+'/yhats.csv')

# px.scatter(yhats, x = 'y_true', y = 'y_pred', color = 'Split')
```

```{python}
yhats.y_true = reverse_cs(yhats.y_true, scale_dict['y1'])
yhats.y_pred = reverse_cs(yhats.y_pred, scale_dict['y1'])

# px.scatter(yhats, x = 'y_true', y = 'y_pred', color = 'Split', trendline="ols")
```

```{python}
yhats['Error'] = yhats.y_pred - yhats.y_true

px.histogram(yhats, x = 'Error', color = 'Split',
             marginal="box", # can be `rug`, `violin`
             nbins= 50)
```

```{python}
# os._exit(00)
```

