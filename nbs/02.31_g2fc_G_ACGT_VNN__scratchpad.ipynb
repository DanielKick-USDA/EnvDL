{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aadccc0",
   "metadata": {},
   "source": [
    "# G only KEGG based network architecture\n",
    "\n",
    "> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "392ee9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from EnvDL.core import ensure_dir_path_exists \n",
    "from EnvDL.dlfn import g2fc_datawrapper, BigDataset, plDNN_general\n",
    "from EnvDL.dlfn import ResNet2d, BasicBlock2d\n",
    "from EnvDL.dlfn import LSUV_\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F # F.mse_loss\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.loggers import TensorBoardLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9f4f7771",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from graphviz import Digraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "302bf05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run settings: \n",
    "max_epoch = 2\n",
    "batch_size = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ce01a01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "use_gpu_num = 0\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if use_gpu_num in [0, 1]: \n",
    "    torch.cuda.set_device(use_gpu_num)\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b6fba3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_path = '../nbs_artifacts/02.31_g2fc_G_ACGT_VNN__scratchpad/'\n",
    "ensure_dir_path_exists(dir_path = cache_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4f56cc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings:\n",
    "run_baseline = False\n",
    "run_alternate = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6573be8",
   "metadata": {},
   "source": [
    "## Demonstration Version\n",
    "\n",
    "New classes to help prepare network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "50ce10dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([200, 4, 3]), torch.Size([200, 4, 2])]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drawing on example version \n",
    "\n",
    "n_obs = 100 # 100 obs for each group\n",
    "y_true = torch.from_numpy(np.concatenate([\n",
    "        np.zeros((n_obs, )),\n",
    "        np.ones( (n_obs, ))], 0)) + .1* torch.rand(2*n_obs,)\n",
    "\n",
    "input_tensor_dict = {\n",
    "    'in1': torch.from_numpy(np.concatenate([\n",
    "        np.zeros((n_obs, 4, 3)),\n",
    "        np.ones( (n_obs, 4, 3))], 0)),\n",
    "    'in2': torch.from_numpy(np.concatenate([\n",
    "        np.zeros((n_obs, 4, 2)),  \n",
    "        np.ones( (n_obs, 4, 2))], 0))}\n",
    "\n",
    "x_list_temp = [input_tensor_dict[key].to(torch.float) for key in input_tensor_dict.keys()]\n",
    "[e.shape for e in x_list_temp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7bf7292e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kegg_connections = {\n",
    "    'y_hat':['d', 'c'],\n",
    "    'd':['b'],\n",
    "    'c':['a'],\n",
    "    'b':['in2', 'a'],\n",
    "    'a':['in1'],\n",
    "    'in1': [],\n",
    "    'in2': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "87cf434b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VNNHelper():\n",
    "    def __init__(self, edge_dict, all_values_are_nodes = True) -> None:\n",
    "        self.edge_dict = edge_dict.copy()\n",
    "        # handles updating all the lists of node groups (imp, out, edge)\n",
    "        self._setup_wrapper(all_values_are_nodes)\n",
    "\n",
    "        self.node_props = {}\n",
    "        for e in self.node_keys_vals['all_keys']:\n",
    "            self.node_props[e] = {}\n",
    "\n",
    "    def _setup_wrapper(self, all_values_are_nodes):\n",
    "        if all_values_are_nodes:\n",
    "            self._init_missing_nodes_as_input()\n",
    "        self.node_keys_vals = self._find_uniq_keys_values(input_dict = self.edge_dict)\n",
    "        self.nodes_inp = self._find_nodes_inp(all_key_value_dict = self.node_keys_vals)\n",
    "        self.nodes_out = self._find_top_nodes(all_key_value_dict = self.node_keys_vals)\n",
    "        self.nodes_edge= self._find_nodes_edge(all_key_value_dict= self.node_keys_vals, nodes_inp= self.nodes_inp, nodes_out = self.nodes_out)\n",
    "        # sets self.dependancy_order\n",
    "        self._get_run_order()\n",
    "\n",
    "    # CRUD operations on node proprty dictionary\n",
    "    def node_prop_create(self, name):\n",
    "        self.node_props[name] = {}\n",
    "        \n",
    "    def node_prop_read_keys(self, name):\n",
    "        return self.node_props[name].keys()\n",
    "\n",
    "    def node_prop_read_values(self, name, key):\n",
    "        return self.node_props[name][key]\n",
    "\n",
    "    def node_prop_update(self, name, key, value):\n",
    "        self.node_props[name][key] = value\n",
    "\n",
    "    def node_prop_delete(self, name, key=None):\n",
    "        if key == None:\n",
    "            del self.node_props[name]\n",
    "        else:\n",
    "            del self.node_props[name][key]\n",
    "\n",
    "\n",
    "    def _find_uniq_keys_values(self, input_dict):\n",
    "        \"\"\"\n",
    "        Building a Neural Net from an arbitrary graph\n",
    "        start by finding the top level -- all those keys which are theselves not values\n",
    "        helper function to get all keys and all value from a dict. Useful for when keys don't have unique values.\n",
    "        \"\"\"\n",
    "        all_keys = list(input_dict.keys())\n",
    "        all_values = []\n",
    "        for e in all_keys:\n",
    "            all_values.extend(input_dict[e])\n",
    "        all_values = list(set(all_values))\n",
    "\n",
    "        return({'all_keys': all_keys, 'all_values': all_values})\n",
    "\n",
    "    def _find_top_nodes(self, all_key_value_dict):\n",
    "        \"\"\"\n",
    "        Find order that nodes in the graph should be called to have all dependencies run when they are called.\n",
    "        find the dependancies for run order from many dependancies to none\n",
    "        wrapper function to find the nodes that aren't any other nodes dependancies.\n",
    "        \"\"\"\n",
    "        return([e for e in all_key_value_dict['all_keys'] if e not in all_key_value_dict['all_values']])\n",
    "    \n",
    "    def _init_missing_nodes_as_input(self):\n",
    "        node_keys_vals = self._find_uniq_keys_values(input_dict = self.edge_dict)\n",
    "        add_these_nodes = [e for e in node_keys_vals['all_values'] if e not in node_keys_vals['all_keys']]\n",
    "        for e in add_these_nodes:\n",
    "            self.edge_dict[e] = []\n",
    "\n",
    "    def _find_nodes_inp(self, all_key_value_dict):\n",
    "        # \"\"\"\n",
    "        # wrapper function to find the input nodes. They don't occur in the keys and thus won't be added to the list otherwise.\n",
    "        # another way to do this would have been to \n",
    "        # \"\"\"\n",
    "        # return([e for e in all_key_value_dict['all_values'] if e not in all_key_value_dict['all_keys']])\n",
    "        return [e for e in all_key_value_dict['all_keys'] if self.edge_dict[e] == []]\n",
    "\n",
    "    def _find_nodes_edge(self, all_key_value_dict, nodes_inp, nodes_out):\n",
    "        return [e for e in all_key_value_dict['all_keys'] if e not in nodes_inp+nodes_out]\n",
    "\n",
    "    def append_output_node(self, node_name):\n",
    "        if node_name in self.edge_dict.keys():\n",
    "            pass\n",
    "        else:\n",
    "            self.edge_dict[node_name] = self.nodes_out\n",
    "            self._setup_wrapper()\n",
    "            self.node_prop_create(name = node_name)\n",
    "\n",
    "    def _get_run_order(self, max_iter = 1000):\n",
    "        temp = self.edge_dict.copy()\n",
    "        dependancy_order = []\n",
    "        # Then iterate\n",
    "        for _ in range(max_iter): \n",
    "            top_nodes = self._find_top_nodes(all_key_value_dict = self._find_uniq_keys_values(input_dict = temp))\n",
    "            if top_nodes == []:\n",
    "                break\n",
    "            else:\n",
    "                dependancy_order += top_nodes    \n",
    "                # remove nodes from the graph that are at the 'top' level and haven't already been removed\n",
    "                for key in [e for e in dependancy_order if e in temp.keys()]:\n",
    "                    temp.pop(key)\n",
    "\n",
    "        # reverse to get the order that the nodes should be called\n",
    "        dependancy_order.reverse()                \n",
    "        self.dependancy_order = dependancy_order\n",
    "\n",
    "\n",
    "    def set_node_props(self, key, node_val_dict):\n",
    "        for name, val in node_val_dict:\n",
    "            self.node_prop_update(name = name, key=key, value=val)\n",
    "    \n",
    "    def calc_edge_inp(self):\n",
    "        for name in self.nodes_edge + self.nodes_out: \n",
    "            inp_size = sum([self.node_prop_read_values(name = e, key= 'out') for e in self.edge_dict[name]])\n",
    "            self.node_prop_update(name = name, key='inp', value=inp_size)     \n",
    "\n",
    "    def mk_digraph(self, include = ['node_name', 'inp_size', 'out_size']):\n",
    "        dot = ''\n",
    "        dot = Digraph()\n",
    "        for key in self.node_props.keys():\n",
    "            key_label = []\n",
    "\n",
    "            if 'node_name' in include: key_label += [key]\n",
    "            if 'inp_size' in include: key_label += ['In  '+str(self.node_props[key]['inp'])]\n",
    "            if 'out_size' in include: key_label += ['Out '+str(self.node_props[key]['out'])]\n",
    "            \n",
    "            if len(key_label) == 0:\n",
    "                key_label = ''\n",
    "            else:\n",
    "                key_label = '\\n'.join(key_label)\n",
    "\n",
    "            dot.node(key, key_label)\n",
    "            for value in self.edge_dict[key]:\n",
    "                # edge takes a head/tail whereas edges takes name pairs concatednated (A, B -> AB)in a list\n",
    "                dot.edge(value, key)    \n",
    "        return dot\n",
    "    \n",
    "\n",
    "myvnn = VNNHelper(edge_dict = kegg_connections)\n",
    "\n",
    "myvnn.nodes_inp[0:10]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2879322e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.50.0 (0)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"187pt\" height=\"527pt\"\n",
       " viewBox=\"0.00 0.00 187.02 526.77\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 522.77)\">\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-522.77 183.02,-522.77 183.02,4 -4,4\"/>\n",
       "<!-- y_hat -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>y_hat</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"89.01\" cy=\"-37.48\" rx=\"41.02\" ry=\"37.45\"/>\n",
       "<text text-anchor=\"middle\" x=\"89.01\" y=\"-48.78\" font-family=\"Times,serif\" font-size=\"14.00\">y_hat</text>\n",
       "<text text-anchor=\"middle\" x=\"89.01\" y=\"-33.78\" font-family=\"Times,serif\" font-size=\"14.00\">In &#160;10</text>\n",
       "<text text-anchor=\"middle\" x=\"89.01\" y=\"-18.78\" font-family=\"Times,serif\" font-size=\"14.00\">Out 1</text>\n",
       "</g>\n",
       "<!-- d -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>d</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"138.01\" cy=\"-148.43\" rx=\"39.7\" ry=\"37.45\"/>\n",
       "<text text-anchor=\"middle\" x=\"138.01\" y=\"-159.73\" font-family=\"Times,serif\" font-size=\"14.00\">d</text>\n",
       "<text text-anchor=\"middle\" x=\"138.01\" y=\"-144.73\" font-family=\"Times,serif\" font-size=\"14.00\">In &#160;5</text>\n",
       "<text text-anchor=\"middle\" x=\"138.01\" y=\"-129.73\" font-family=\"Times,serif\" font-size=\"14.00\">Out 5</text>\n",
       "</g>\n",
       "<!-- d&#45;&gt;y_hat -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>d&#45;&gt;y_hat</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M122.78,-113.55C118.21,-103.41 113.15,-92.14 108.36,-81.49\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"111.53,-80.02 104.24,-72.33 105.15,-82.89 111.53,-80.02\"/>\n",
       "</g>\n",
       "<!-- c -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>c</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"41.01\" cy=\"-148.43\" rx=\"39.7\" ry=\"37.45\"/>\n",
       "<text text-anchor=\"middle\" x=\"41.01\" y=\"-159.73\" font-family=\"Times,serif\" font-size=\"14.00\">c</text>\n",
       "<text text-anchor=\"middle\" x=\"41.01\" y=\"-144.73\" font-family=\"Times,serif\" font-size=\"14.00\">In &#160;5</text>\n",
       "<text text-anchor=\"middle\" x=\"41.01\" y=\"-129.73\" font-family=\"Times,serif\" font-size=\"14.00\">Out 5</text>\n",
       "</g>\n",
       "<!-- c&#45;&gt;y_hat -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>c&#45;&gt;y_hat</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M55.94,-113.55C60.32,-103.61 65.18,-92.58 69.79,-82.12\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"73.1,-83.27 73.93,-72.71 66.7,-80.45 73.1,-83.27\"/>\n",
       "</g>\n",
       "<!-- b -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>b</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"138.01\" cy=\"-259.38\" rx=\"41.02\" ry=\"37.45\"/>\n",
       "<text text-anchor=\"middle\" x=\"138.01\" y=\"-270.68\" font-family=\"Times,serif\" font-size=\"14.00\">b</text>\n",
       "<text text-anchor=\"middle\" x=\"138.01\" y=\"-255.68\" font-family=\"Times,serif\" font-size=\"14.00\">In &#160;10</text>\n",
       "<text text-anchor=\"middle\" x=\"138.01\" y=\"-240.68\" font-family=\"Times,serif\" font-size=\"14.00\">Out 5</text>\n",
       "</g>\n",
       "<!-- b&#45;&gt;d -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>b&#45;&gt;d</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M138.01,-221.8C138.01,-213.63 138.01,-204.85 138.01,-196.32\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"141.51,-196.1 138.01,-186.1 134.51,-196.1 141.51,-196.1\"/>\n",
       "</g>\n",
       "<!-- a -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>a</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"41.01\" cy=\"-370.34\" rx=\"39.7\" ry=\"37.45\"/>\n",
       "<text text-anchor=\"middle\" x=\"41.01\" y=\"-381.64\" font-family=\"Times,serif\" font-size=\"14.00\">a</text>\n",
       "<text text-anchor=\"middle\" x=\"41.01\" y=\"-366.64\" font-family=\"Times,serif\" font-size=\"14.00\">In &#160;5</text>\n",
       "<text text-anchor=\"middle\" x=\"41.01\" y=\"-351.64\" font-family=\"Times,serif\" font-size=\"14.00\">Out 5</text>\n",
       "</g>\n",
       "<!-- a&#45;&gt;c -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>a&#45;&gt;c</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M41.01,-332.53C41.01,-295.07 41.01,-236.89 41.01,-196.2\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"44.51,-195.99 41.01,-185.99 37.51,-195.99 44.51,-195.99\"/>\n",
       "</g>\n",
       "<!-- a&#45;&gt;b -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>a&#45;&gt;b</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M66.24,-340.99C78.27,-327.48 92.81,-311.16 105.64,-296.74\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"108.53,-298.76 112.57,-288.96 103.31,-294.1 108.53,-298.76\"/>\n",
       "</g>\n",
       "<!-- in2 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>in2</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"138.01\" cy=\"-370.34\" rx=\"39.7\" ry=\"37.45\"/>\n",
       "<text text-anchor=\"middle\" x=\"138.01\" y=\"-381.64\" font-family=\"Times,serif\" font-size=\"14.00\">in2</text>\n",
       "<text text-anchor=\"middle\" x=\"138.01\" y=\"-366.64\" font-family=\"Times,serif\" font-size=\"14.00\">In &#160;8</text>\n",
       "<text text-anchor=\"middle\" x=\"138.01\" y=\"-351.64\" font-family=\"Times,serif\" font-size=\"14.00\">Out 5</text>\n",
       "</g>\n",
       "<!-- in2&#45;&gt;b -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>in2&#45;&gt;b</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M138.01,-332.75C138.01,-324.58 138.01,-315.8 138.01,-307.27\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"141.51,-307.06 138.01,-297.06 134.51,-307.06 141.51,-307.06\"/>\n",
       "</g>\n",
       "<!-- in1 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>in1</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"41.01\" cy=\"-481.29\" rx=\"41.02\" ry=\"37.45\"/>\n",
       "<text text-anchor=\"middle\" x=\"41.01\" y=\"-492.59\" font-family=\"Times,serif\" font-size=\"14.00\">in1</text>\n",
       "<text text-anchor=\"middle\" x=\"41.01\" y=\"-477.59\" font-family=\"Times,serif\" font-size=\"14.00\">In &#160;12</text>\n",
       "<text text-anchor=\"middle\" x=\"41.01\" y=\"-462.59\" font-family=\"Times,serif\" font-size=\"14.00\">Out 5</text>\n",
       "</g>\n",
       "<!-- in1&#45;&gt;a -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>in1&#45;&gt;a</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M41.01,-443.71C41.01,-435.53 41.01,-426.75 41.01,-418.23\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"44.51,-418.01 41.01,-408.01 37.51,-418.01 44.51,-418.01\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7fa250482e90>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myvnn = VNNHelper(edge_dict = kegg_connections)\n",
    "\n",
    "# init input node sizes\n",
    "inp_tensor_name_to_size = zip(\n",
    "    ['in1', 'in2'], \n",
    "    [int(torch.prod(torch.tensor(e.shape)[1:])) for e in x_list_temp])\n",
    "\n",
    "myvnn.set_node_props(key = 'inp', node_val_dict = inp_tensor_name_to_size)\n",
    "\n",
    "# init node output sizes\n",
    "myvnn.set_node_props(key = 'out', node_val_dict = zip(myvnn.nodes_inp, [5 for e in myvnn.nodes_inp]))\n",
    "myvnn.set_node_props(key = 'out', node_val_dict = zip(myvnn.nodes_edge,[5 for e in myvnn.nodes_edge]))\n",
    "myvnn.set_node_props(key = 'out', node_val_dict = zip(myvnn.nodes_out, [1 for e in myvnn.nodes_out]))\n",
    "\n",
    "\n",
    "# options should be controlled by node_props\n",
    "myvnn.set_node_props(key = 'flatten', node_val_dict = zip(\n",
    "    myvnn.nodes_inp, \n",
    "    [True for e in myvnn.nodes_inp]))\n",
    "\n",
    "myvnn.set_node_props(key = 'reps', node_val_dict = zip(\n",
    "    myvnn.nodes_out+myvnn.nodes_inp+myvnn.nodes_edge, \n",
    "    [1 for e in myvnn.nodes_out+myvnn.nodes_inp+myvnn.nodes_edge]))\n",
    "\n",
    "# init dropout \n",
    "myvnn.set_node_props(key = 'drop', node_val_dict = zip(\n",
    "    myvnn.nodes_out+myvnn.nodes_inp+myvnn.nodes_edge, \n",
    "    [0.0 for e in myvnn.nodes_out+myvnn.nodes_inp+myvnn.nodes_edge]))\n",
    "\n",
    "# init edge node input size (propagate forward input/edge outpus)\n",
    "myvnn.calc_edge_inp()\n",
    "\n",
    "myvnn.mk_digraph(include = ['node_name', 'inp_size', 'out_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "39d21dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Linear_block(in_size, out_size, drop_pr, block_reps):\n",
    "    block_list = []\n",
    "    for i in range(block_reps):\n",
    "        if i == 0:\n",
    "            block_list += [\n",
    "                nn.Linear(in_size, out_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(drop_pr)]\n",
    "        else:\n",
    "            block_list += [\n",
    "                nn.Linear(out_size, out_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(drop_pr)]\n",
    "\n",
    "    block = nn.ModuleList(block_list)\n",
    "    return(block)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8cc7ec1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisableNeuralNetwork(nn.Module):\n",
    "    def __init__(self, \n",
    "                 node_props, \n",
    "                 Linear_block,\n",
    "                 dependancy_order,\n",
    "                 edge_dict,\n",
    "                 node_to_inp_num_dict\n",
    "                ):\n",
    "        super(VisableNeuralNetwork, self).__init__()\n",
    "        # Store nodes in dict\n",
    "        layer_dict = {}\n",
    "        for key in node_props.keys():\n",
    "            node_list = []\n",
    "            if 'flatten' in node_props[key]:\n",
    "                node_list += [nn.Flatten()]\n",
    "            node_list += [nn.Flatten()]\n",
    "            #TODO change linear block instead of conditioning on node name.\n",
    "            if key != 'y_hat':\n",
    "                node_list += [Linear_block(\n",
    "                    in_size=node_props[key]['inp'], \n",
    "                    out_size=node_props[key]['out'], \n",
    "                    drop_pr=node_props[key]['drop'],\n",
    "                    block_reps=node_props[key]['reps'])]\n",
    "            else:\n",
    "                node_list += [nn.Linear(node_props[key]['inp'], node_props[key]['out'])]\n",
    "\n",
    "            layer_dict[key] = nn.ModuleList(node_list)\n",
    "\n",
    "        self.layer_dict = nn.ModuleDict(layer_dict)\n",
    "        \n",
    "        self.dependancy_order = dependancy_order\n",
    "        self.edge_dict = edge_dict\n",
    "        self.node_to_inp_num_dict = node_to_inp_num_dict\n",
    "\n",
    "    def forward(self, x):\n",
    "        temp_res_dict = {}\n",
    "        for key in self.dependancy_order:\n",
    "            \n",
    "            # if the node depends on raw inputs, get them.\n",
    "            if key in self.node_to_inp_num_dict:\n",
    "                xin = [ x[self.node_to_inp_num_dict[key]] ]\n",
    "\n",
    "            # if the node depends on inputs that have been stored in the lookup dict\n",
    "            if self.edge_dict[key] != []:\n",
    "                if xin == None:\n",
    "                    xin = []\n",
    "                xin += [temp_res_dict[e] for e in self.edge_dict[key]]\n",
    "                \n",
    "            # join all input tensors.\n",
    "            xin = torch.concat(xin, axis = 1)\n",
    "\n",
    "            for l in self.layer_dict[key]:\n",
    "                if type(l) == torch.nn.modules.container.ModuleList:\n",
    "                    for ll in l:            \n",
    "                        xin = ll(xin)\n",
    "                else:\n",
    "                    xin = l(xin)\n",
    "                    \n",
    "            temp_res_dict[key] = xin\n",
    "            xin = None \n",
    "        return temp_res_dict[self.dependancy_order[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "bb3db872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4001],\n",
       "        [0.4001],\n",
       "        [0.4001],\n",
       "        [0.4001],\n",
       "        [0.4001]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = VisableNeuralNetwork(\n",
    "    node_props = myvnn.node_props,\n",
    "    Linear_block = Linear_block,\n",
    "    edge_dict = myvnn.edge_dict,\n",
    "    dependancy_order = myvnn.dependancy_order,\n",
    "    node_to_inp_num_dict = {'in1': 0, 'in2': 1}\n",
    ")\n",
    "model(x_list_temp)[0:5]\n",
    "# model.layer_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b574d4b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff84bdd0",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547c1e1f",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9c52e21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = g2fc_datawrapper()\n",
    "# X.set_split()\n",
    "# X.load_all(name_list = ['obs_geno_lookup', 'YMat', 'KEGG_slices',], store=True) \n",
    "\n",
    "# X.calc_cs('YMat', version = 'np', filter = 'val:train')\n",
    "\n",
    "\n",
    "# ACGT_gene_slice_list = X.get('KEGG_slices',     ops_string='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7f9e7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87bfb7bb",
   "metadata": {},
   "source": [
    "## Generate Graph for DNN (Refactored Original)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6afc90",
   "metadata": {},
   "source": [
    "### Functions to process KEGG entries into a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5b924e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Building a Neural Net from an arbitrary graph\n",
    "# start by finding the top level -- all those keys which are theselves not values\n",
    "# helper function to get all keys and all value from a dict. Useful for when keys don't have unique values.\n",
    "def find_uniq_keys_values(input_dict):\n",
    "    all_keys = list(input_dict.keys())\n",
    "    all_values = []\n",
    "    for e in all_keys:\n",
    "        all_values.extend(input_dict[e])\n",
    "    all_values = list(set(all_values))\n",
    "\n",
    "    return({'all_keys': all_keys,\n",
    "           'all_values': all_values})\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6dd645ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Find order that nodes in the graph should be called to have all dependencies run when they are called.\n",
    "# find the dependancies for run order from many dependancies to none\n",
    "# wrapper function to find the nodes that aren't any other nodes dependancies.\n",
    "def find_top_nodes(all_key_value_dict):\n",
    "    return([e for e in all_key_value_dict['all_keys'] if e not in all_key_value_dict['all_values']])\n",
    "# wrapper function to find the input nodes. They don't occur in the keys and thus won't be added to the list otherwise.\n",
    "# another way to do this would have been to \n",
    "def find_input_nodes(all_key_value_dict):\n",
    "    return([e for e in all_key_value_dict['all_values'] if e not in all_key_value_dict['all_keys']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4bae9662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOD is list of dicts.\n",
    "# def kegg_brite_LOD_to_connections(\n",
    "def kegg_connections_build(\n",
    "        n_genes, \n",
    "        kegg_gene_brite):\n",
    "    \"\"\"\n",
    "    The goal here is to have a dict with each node and a list of it's children. \n",
    "    For example, the graph\n",
    "    a--b--d\n",
    "    |-c--e\n",
    "    Would be parsed into     \n",
    "    {'a':['b', 'c'],\n",
    "    'b':['d'],\n",
    "    'c':['e']}\n",
    "    \"\"\"\n",
    "    kegg_connections = {}\n",
    "    # for all genes in list\n",
    "    for i in tqdm(range(n_genes)): \n",
    "        temp = kegg_gene_brite[i]['BRITE']['BRITE_PATHS']\n",
    "        # clean up to make sure that there are no \":\" characters. These can mess up graphviz\n",
    "        temp = [[temp[j][i].replace(':', '-') for i in range(len(temp[j])) ] for j in range(len(temp))]\n",
    "        # all paths through graph associated with a gene\n",
    "        for j in range(len(temp)):\n",
    "            # steps of the path through the graph\n",
    "            for k in range(len(temp[j])-1):\n",
    "                \n",
    "                # name standardization \n",
    "                temp_jk  = temp[j][k]\n",
    "                temp_jk1 = temp[j][k+1]\n",
    "                temp_jk  = temp_jk.lower().title().replace(' ', '')\n",
    "                temp_jk1 = temp_jk1.lower().title().replace(' ', '')\n",
    "                \n",
    "                # if this is a new key, add it and add the k+1 entry as it's child\n",
    "                if temp_jk  not in kegg_connections.keys():\n",
    "                    kegg_connections[temp_jk] = [temp_jk1]\n",
    "                else: \n",
    "                    # Check to see if there's a new child to add   \n",
    "                    if temp_jk1 not in kegg_connections[temp_jk]:\n",
    "                        # make sure that no key contains itself. This was a problem for 'Others' which is now disallowed.\n",
    "                        if (temp_jk != temp_jk1):\n",
    "                            # add it.\n",
    "                            kegg_connections[temp_jk].extend([temp_jk1])\n",
    "    return(kegg_connections)          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7ba4d769",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kegg_connections_clean(kegg_connections):\n",
    "    if 'Others' in kegg_connections.keys():\n",
    "        del kegg_connections['Others']\n",
    "        print('Removed node \"Others\"')\n",
    "\n",
    "    # remove 'Others' as a possible value\n",
    "    for key in kegg_connections.keys():\n",
    "        kegg_connections[key] = [e for e in kegg_connections[key] if e != 'Others']\n",
    "\n",
    "    # Make sure that no list contains it's own key\n",
    "    for key in kegg_connections.keys():\n",
    "        kegg_connections[key] = [e for e in kegg_connections[key] if e != key]\n",
    "\n",
    "    # there might be associations with no dependants and with no dependants except those that have no dependants.\n",
    "    # Build up a list with those keys that don't connect back to snps then I'll pass over the connection dict once to remove references to them.\n",
    "    rm_list = []\n",
    "    rm_list_i = len(rm_list)\n",
    "    rm_list_j = -1\n",
    "    for i in range(100):\n",
    "        if rm_list_i == rm_list_j:\n",
    "            break\n",
    "        else:\n",
    "            rm_list = [key for key in kegg_connections.keys() if [e for e in kegg_connections[key] if e not in rm_list] == []]\n",
    "            rm_list_j = rm_list_i \n",
    "            rm_list_i = len(rm_list)\n",
    "    # rm_list\n",
    "\n",
    "    for key in rm_list:\n",
    "        del kegg_connections[key]\n",
    "        \n",
    "    for key in kegg_connections.keys():\n",
    "        kegg_connections[key] = [e for e in kegg_connections[key] if e not in rm_list]\n",
    "    return kegg_connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "65a59244",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kegg_connections_append_y_hat(kegg_connections):\n",
    "    # add yhat node to the graph\n",
    "    temp_values = []\n",
    "    for key in kegg_connections.keys():\n",
    "        temp_values += kegg_connections[key]\n",
    "\n",
    "    kegg_connections['y_hat'] = [key for key in kegg_connections.keys() if key not in temp_values]\n",
    "    return kegg_connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "20f39cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kegg_connections_digraph(kegg_connections, option = ''):\n",
    "    dot = ''\n",
    "    if option == '':\n",
    "        dot = Digraph()\n",
    "        for key in tqdm(kegg_connections.keys()):\n",
    "            dot.node(key)\n",
    "            for value in kegg_connections[key]:\n",
    "                # edge takes a head/tail whereas edges takes name pairs concatednated (A, B -> AB)in a list\n",
    "                dot.edge(value, key)\n",
    "\n",
    "    if option == 'number':\n",
    "        name_to_num_dict = dict(zip(list(kegg_connections.keys()),\n",
    "                                    [str(i) for i in range(len(list(kegg_connections.keys())))]))\n",
    "\n",
    "        temp = {}\n",
    "        for key in kegg_connections.keys():\n",
    "            temp[name_to_num_dict[key]] = [name_to_num_dict[e] if e in name_to_num_dict.keys() else e for e in kegg_connections[key]]\n",
    "\n",
    "        dot = Digraph()\n",
    "        for key in tqdm(temp.keys()):\n",
    "            dot.node(key)\n",
    "            for value in temp[key]:\n",
    "                # edge takes a head/tail whereas edges takes name pairs concatednated (A, B -> AB)in a list\n",
    "                dot.edge(value, key)                 \n",
    "                 \n",
    "    return dot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1e6077",
   "metadata": {},
   "source": [
    "Version with the node names masked for size "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cb7324",
   "metadata": {},
   "source": [
    "### Setup to build the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "bbf22b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kegg_connections_find_in_out_nodes(kegg_connections):\n",
    "    # start by finding the top level -- all those keys which are theselves not values\n",
    "    res = find_uniq_keys_values(input_dict = kegg_connections)\n",
    "    all_keys = res['all_keys']\n",
    "    all_values = res['all_values']\n",
    "\n",
    "    # use the keys to find the input/outputs of the graph\n",
    "    output_nodes = [e for e in all_keys if e not in all_values]\n",
    "    input_nodes = [e for e in all_values if e not in all_keys]\n",
    "    return input_nodes, output_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6e3cd18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kegg_connections_find_dependancy_order(kegg_connections, input_nodes):\n",
    "    # find the dependancies for run order from many dependancies to none\n",
    "    temp = kegg_connections.copy()\n",
    "\n",
    "    no_dependants = find_input_nodes(all_key_value_dict = find_uniq_keys_values(input_dict = temp))\n",
    "    # first pass. Same as the output nodes identified above\n",
    "    dependancy_order = []\n",
    "    # Then iterate\n",
    "    for ith in range(100): #TODO <- this should be set as a input parameter\n",
    "        top_nodes = find_top_nodes(all_key_value_dict = find_uniq_keys_values(input_dict = temp))\n",
    "        if top_nodes == []:\n",
    "            break\n",
    "        else:\n",
    "            dependancy_order += top_nodes    \n",
    "            # remove nodes from the graph that are at the 'top' level and haven't already been removed\n",
    "            for key in [e for e in dependancy_order if e in temp.keys()]:\n",
    "                temp.pop(key)\n",
    "\n",
    "    # reverse to get the order that the nodes should be called\n",
    "    dependancy_order.reverse()\n",
    "    # dependancy_order\n",
    "\n",
    "    # Trying out new approach: add a node for the input data tha will only flatten the input.\n",
    "    dependancy_order = input_nodes+dependancy_order\n",
    "\n",
    "    for key in input_nodes:\n",
    "        kegg_connections[key] = [] #[key] # needs to contain itself so the model's `get_input_node()` function works \n",
    "                                # or that function needs to change.\n",
    "    \n",
    "    return kegg_connections, dependancy_order, no_dependants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "78b6c996",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kegg_gene_brite_build_node_lookup_dict(kegg_gene_brite):\n",
    "    # build a dict to go from the node names in `no_dependants` to the list index in `ACGT_gene_slice_list`\n",
    "    brite_node_to_list_idx_dict = {}\n",
    "    for i in tqdm(range(len(kegg_gene_brite))):\n",
    "        brite_node_to_list_idx_dict[str(kegg_gene_brite[i]['BRITE']['BRITE_PATHS'][0][-1])] = i\n",
    "    return brite_node_to_list_idx_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "005ba49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_input_tensor_dict(ACGT_gene_slice_list, no_dependants, brite_node_to_list_idx_dict):    \n",
    "    input_tensor_dict = {}\n",
    "    for e in no_dependants:\n",
    "        input_tensor_dict[e] = ACGT_gene_slice_list[brite_node_to_list_idx_dict[e]]\n",
    "    \n",
    "    return input_tensor_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d97df836",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_network_param_dicts(\n",
    "        kegg_connections,  # for input sizes\n",
    "        input_tensor_dict, # for input sizes\n",
    "        dependancy_order,\n",
    "        default_output_size,\n",
    "        default_dropout_pr,\n",
    "        default_block_reps,\n",
    "    ):\n",
    "    # Figure out expected input/output shapes\n",
    "    #==NOTE! This assumes only dense connections!==\n",
    "\n",
    "    # This could be replaced by a sort of \"distance from output\" measure\n",
    "    output_size_dict = dict(zip(dependancy_order, \n",
    "                            [default_output_size for i in range(len(dependancy_order))]))\n",
    "    output_size_dict['y_hat'] = 1 \n",
    "\n",
    "\n",
    "    # Setup dropout % dictionary\n",
    "    dropout_pr_dict = dict(zip(dependancy_order, \n",
    "                            [default_dropout_pr for i in range(len(dependancy_order))]))\n",
    "    dropout_pr_dict['y_hat'] = 0 # not required, output node is purely linear without dropout\n",
    "\n",
    "\n",
    "    # Setup replicates of layers dictionary\n",
    "    block_rep_dict = dict(zip(dependancy_order, \n",
    "                            [default_block_reps for i in range(len(dependancy_order))]))\n",
    "    block_rep_dict['y_hat'] = 1 # not required, output node is purely linear. Not a linear block\n",
    "\n",
    "    # output_size_dict\n",
    "    # dropout_pr_dict\n",
    "    # block_rep_dict\n",
    "\n",
    "    # CHANNEL AWARE VERSION -----------------------------------------------------------------------------------\n",
    "    input_size_dict = kegg_connections.copy()\n",
    "\n",
    "    # use the expected output sizes from `output_size_dict` to fill in the non-data sizes\n",
    "    tensor_ndim = len(input_tensor_dict[list(input_tensor_dict.keys())[0]].shape)\n",
    "    for e in tqdm(input_size_dict.keys()):\n",
    "        # overwrite named connections with the output size of those connections\n",
    "        # if the entry is in no_dependants it's data so it's size needs to be grabbed from the input_tensor_dict\n",
    "        \n",
    "        # is there no channel dim? (major/minor allele)\n",
    "        if 2 == tensor_ndim:\n",
    "            input_size_dict[e] = [\n",
    "                list(input_tensor_dict[ee].shape)[-1] # <- NOTE! THIS ASSUMES ONLY DENSE CONNECTIONS (i.e. only the 1st dim is needed)\n",
    "                if ee in no_dependants\n",
    "                else output_size_dict[ee] for ee in input_size_dict[e]]\n",
    "        elif 3 == tensor_ndim: # There is a channel dim\n",
    "            input_size_dict[e] = [\n",
    "                (list(input_tensor_dict[ee].shape)[1]*list(input_tensor_dict[ee].shape)[2]) # <- NOTE! THIS ASSUMES ONLY DENSE CONNECTIONS (i.e. only the 1st dim is needed)  \n",
    "                if ee in no_dependants\n",
    "                else output_size_dict[ee] for ee in input_size_dict[e]]\n",
    "\n",
    "    # Now walk over entries and overwrite with the sum of the inputs\n",
    "    for e in tqdm(input_size_dict.keys()):\n",
    "        input_size_dict[e] = np.sum(input_size_dict[e])\n",
    "\n",
    "    return input_size_dict, output_size_dict, dropout_pr_dict, block_rep_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "77d41d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_baseline:\n",
    "    default_output_size = 1\n",
    "    default_dropout_pr = 0.0\n",
    "    default_block_reps = 1\n",
    "\n",
    "\n",
    "    X = g2fc_datawrapper()\n",
    "    X.set_split()\n",
    "    X.load_all(name_list = ['obs_geno_lookup', 'YMat', 'KEGG_slices',], store=True) \n",
    "    X.calc_cs('YMat', version = 'np', filter = 'val:train')\n",
    "    ACGT_gene_slice_list =     X.get('KEGG_slices', ops_string='')\n",
    "    parsed_kegg_gene_entries = X.get('KEGG_entries')\n",
    "\n",
    "\n",
    "    # Restrict to only those with pathway\n",
    "    kegg_gene_brite = [e for e in parsed_kegg_gene_entries if 'BRITE' in e.keys()]\n",
    "\n",
    "    # also require to have a non-empty path\n",
    "    kegg_gene_brite = [e for e in kegg_gene_brite if not e['BRITE']['BRITE_PATHS'] == []]\n",
    "\n",
    "    print('Retaining '+ str(round(len(kegg_gene_brite)/len(parsed_kegg_gene_entries), 4)*100)+'%, '+str(len(kegg_gene_brite)\n",
    "        )+'/'+str(len(parsed_kegg_gene_entries)\n",
    "        )+' Entries'\n",
    "        )\n",
    "    # kegg_gene_brite[1]['BRITE']['BRITE_PATHS']\n",
    "\n",
    "\n",
    "    kegg_connections = kegg_connections_append_y_hat(\n",
    "        kegg_connections = kegg_connections_clean(\n",
    "            kegg_connections = kegg_connections_build(\n",
    "                n_genes = 6067, kegg_gene_brite = kegg_gene_brite) ) )\n",
    "\n",
    "    len(list(kegg_connections.keys()))\n",
    "\n",
    "\n",
    "    input_nodes, output_nodes                         = kegg_connections_find_in_out_nodes(kegg_connections= kegg_connections)\n",
    "    kegg_connections, dependancy_order, no_dependants = kegg_connections_find_dependancy_order(kegg_connections= kegg_connections, input_nodes= input_nodes)\n",
    "    brite_node_to_list_idx_dict                       = kegg_gene_brite_build_node_lookup_dict(kegg_gene_brite = kegg_gene_brite)\n",
    "    input_tensor_dict                                 = build_input_tensor_dict(ACGT_gene_slice_list=ACGT_gene_slice_list, no_dependants=no_dependants, brite_node_to_list_idx_dict = brite_node_to_list_idx_dict)\n",
    "\n",
    "    input_size_dict, output_size_dict, dropout_pr_dict, block_rep_dict = setup_network_param_dicts(\n",
    "            kegg_connections = kegg_connections,\n",
    "            input_tensor_dict = input_tensor_dict,\n",
    "            dependancy_order = dependancy_order,\n",
    "            default_output_size = default_output_size,\n",
    "            default_dropout_pr = default_dropout_pr,\n",
    "            default_block_reps = default_block_reps,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "3547d984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # make in out size graph\n",
    "# dot = ''\n",
    "# dot = Digraph()\n",
    "# for key in tqdm(kegg_connections.keys()):\n",
    "#     key_label = 'in: '+str(input_size_dict[key])+'\\nout: '+str(output_size_dict[key])\n",
    "#     dot.node(key, key_label)\n",
    "#     for value in kegg_connections[key\n",
    "# ]:\n",
    "#         # edge takes a head/tail whereas edges takes name pairs concatednated (A, B -> AB)in a list\n",
    "#         dot.edge(value, key)    \n",
    "\n",
    "# dot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c21850c",
   "metadata": {},
   "source": [
    "## Fit Using Original VNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6075d68",
   "metadata": {},
   "source": [
    "### Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "0cb35a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working version ====\n",
    "# Doesn't pass output node through relu\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, \n",
    "                 example_dict, # contains the node (excluding input tensors)\n",
    "                 example_dict_input_size, # contains the input sizes (including the tensors)\n",
    "                 example_dict_output_size,\n",
    "                 example_dict_dropout_pr,\n",
    "                 example_block_rep_dict,\n",
    "                 input_tensor_names,\n",
    "                 dependancy_order\n",
    "                ):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        def Linear_block(in_size, out_size, drop_pr, block_reps):\n",
    "            block_list = []\n",
    "            for i in range(block_reps):\n",
    "                if i == 0:\n",
    "                    block_list += [\n",
    "                        nn.Linear(in_size, out_size),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(drop_pr)]\n",
    "                else:\n",
    "                    block_list += [\n",
    "                        nn.Linear(out_size, out_size),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(drop_pr)]\n",
    "        \n",
    "            block = nn.ModuleList(block_list)\n",
    "            return(block)           \n",
    "        \n",
    "        # fill in the list in dependancy order. \n",
    "        layer_list = []\n",
    "        for key in dependancy_order:\n",
    "            if key in input_tensor_names:\n",
    "                layer_list += [\n",
    "                    nn.Flatten()\n",
    "                ]\n",
    "            elif key != 'y_hat':\n",
    "                layer_list += [\n",
    "                    Linear_block(in_size=example_dict_input_size[key], \n",
    "                                 out_size=example_dict_output_size[key], \n",
    "                                 drop_pr=example_dict_dropout_pr[key],\n",
    "                                 block_reps=example_block_rep_dict[key])\n",
    "                              ]\n",
    "            else:\n",
    "                layer_list += [\n",
    "                    nn.Linear(example_dict_input_size[key], \n",
    "                              example_dict_output_size[key])\n",
    "                              ]\n",
    "                \n",
    "\n",
    "        self.nn_layer_list = nn.ModuleList(layer_list)\n",
    "\n",
    "        # things for get_input_node in forward to work.\n",
    "        self.example_dict = example_dict\n",
    "        self.input_tensor_names = input_tensor_names\n",
    "        self.dependancy_order = dependancy_order\n",
    "        \n",
    "        self.input_tensor_lookup = dict(zip(input_tensor_names, \n",
    "                                            [i for i in range(len(input_tensor_names))]))\n",
    "        self.result_list = []\n",
    "        self.result_list_lookup = {}\n",
    "            \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Note: x will be a list. input_tensor_lookup will contain the name: list index pairs.\n",
    "        # I use a dict instead of a list comprehension here because there could be an arbitrarily\n",
    "        # large number of inputs in the list. \n",
    "        def get_input_node(self, input_node, get_x):  \n",
    "#             print(input_node, self.result_list_lookup)\n",
    "            return(self.result_list[self.result_list_lookup[input_node]])\n",
    "        \n",
    "        # trying reinstantiating to get around inplace replacement issue.\n",
    "        self.result_list = []\n",
    "        self.result_list_lookup = {}\n",
    "        for key in self.dependancy_order:\n",
    "            input_nodes = self.example_dict[key]\n",
    "            nn_layer_list_idx = [i for i in range(len(dependancy_order)) if dependancy_order[i]==key][0]\n",
    "            \n",
    "            self.result_list_lookup[key] = len(self.result_list_lookup)                \n",
    "            if key in self.input_tensor_names: # If the input node is an input (flatten) layer\n",
    "                self.result_list = self.result_list + [self.nn_layer_list[nn_layer_list_idx](\n",
    "                    x[self.input_tensor_lookup[key]]\n",
    "                ).clone()]\n",
    "\n",
    "            elif key != 'y_hat':\n",
    "                # refactored to handle module lists (even if module list contains only one entry)\n",
    "                out = torch.concat(\n",
    "                    [get_input_node(self, input_node = e, get_x = x) for e in input_nodes], \n",
    "                    -1)\n",
    "            \n",
    "                for module in self.nn_layer_list[nn_layer_list_idx]:\n",
    "                    out = module(out)\n",
    "        \n",
    "                self.result_list = self.result_list + [out] \n",
    "            \n",
    "            else:\n",
    "                self.result_list = self.result_list + [self.nn_layer_list[nn_layer_list_idx](torch.concat(\n",
    "                    [get_input_node(self, input_node = e, get_x = x) for e in input_nodes], \n",
    "                    -1)).clone()]            \n",
    "\n",
    "        return self.result_list[self.result_list_lookup['y_hat']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "55b56467",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ListDataset(Dataset): # for any G containing matix with many (phno) to one (geno)\n",
    "    def __init__(self, \n",
    "                 y, \n",
    "                 x_list,\n",
    "                 obs_idxs, # this is a list of the indexes used. It allows us to pass in smaller \n",
    "                           # tensors and then get the right genotype\n",
    "                 obs_geno_lookup,\n",
    "                 transform = None, target_transform = None,\n",
    "                 **kwargs \n",
    "                ):\n",
    "        self.device = device\n",
    "        self.y = y \n",
    "        self.x_list = x_list\n",
    "        self.obs_idxs = obs_idxs\n",
    "        self.obs_geno_lookup = obs_geno_lookup\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform    \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        y_idx =self.y[idx]\n",
    "        \n",
    "        new_idx = self.obs_idxs[idx]\n",
    "        idx_geno = self.obs_geno_lookup[new_idx, 1]\n",
    "        x_idx =[x[idx_geno, ] for x in self.x_list] \n",
    "        \n",
    "        if self.target_transform:\n",
    "            y_idx = self.transform(y_idx)\n",
    "            x_idx = [self.transform(x) for x in x_idx]\n",
    "            \n",
    "        return y_idx, x_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "306daab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class plVNN(pl.LightningModule):\n",
    "    def __init__(self, mod):\n",
    "        super().__init__()\n",
    "        self.mod = mod\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        y_i, xs_i = batch\n",
    "        pred = self.mod(xs_i)\n",
    "        loss = F.mse_loss(pred, y_i)\n",
    "        self.log(\"train_loss\", loss)   \n",
    "        return(loss)\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        y_i, xs_i = batch\n",
    "        pred = self.mod(xs_i)\n",
    "        loss = F.mse_loss(pred, y_i)\n",
    "        self.log('val_loss', loss)        \n",
    "     \n",
    "    def configure_optimizers(self, **kwargs):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), **kwargs)\n",
    "        return optimizer    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b6274e",
   "metadata": {},
   "source": [
    "### Trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "cb4ead9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_baseline:\n",
    "    x_list_temp = [torch.from_numpy(input_tensor_dict[key]).to(torch.float) for key in input_tensor_dict.keys()]\n",
    "\n",
    "\n",
    "    training_dataloader = DataLoader(ListDataset(\n",
    "            y =               X.get('YMat',ops_string='cs filter:val:train asarray from_numpy float cuda:0')[:, None],\n",
    "            x_list = [e.to('cuda') for e in x_list_temp],\n",
    "            obs_idxs =        X.get('val:train',       ops_string='   asarray from_numpy      '), \n",
    "            obs_geno_lookup = X.get('obs_geno_lookup', ops_string='   asarray from_numpy      ')\n",
    "        ),\n",
    "        batch_size = batch_size,\n",
    "        shuffle = True\n",
    "    )\n",
    "\n",
    "\n",
    "    validation_dataloader = DataLoader(ListDataset(\n",
    "            y =               X.get('YMat',ops_string='cs filter:val:test asarray from_numpy float cuda:0')[:, None],\n",
    "            x_list = [e.to('cuda') for e in x_list_temp],\n",
    "            obs_idxs =        X.get('val:test',       ops_string='   asarray from_numpy      '), \n",
    "            obs_geno_lookup = X.get('obs_geno_lookup', ops_string='   asarray from_numpy      ')\n",
    "        ),\n",
    "        batch_size = batch_size,\n",
    "        shuffle = True\n",
    "    )\n",
    "    # next(iter(training_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a872acf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_baseline:\n",
    "    model = NeuralNetwork(example_dict = kegg_connections, \n",
    "                        example_dict_input_size = input_size_dict,\n",
    "                        example_dict_output_size = output_size_dict,\n",
    "                        example_dict_dropout_pr= dropout_pr_dict,\n",
    "                        example_block_rep_dict = block_rep_dict,\n",
    "                        input_tensor_names = list(input_tensor_dict.keys()),\n",
    "                        dependancy_order = dependancy_order)\n",
    "\n",
    "\n",
    "    model.to('cuda')\n",
    "    # LSUV_(model, data = next(iter(training_dataloader))[1])\n",
    "    print(model(next(iter(training_dataloader))[1])[0:5])\n",
    "    # print(next(model.parameters()))\n",
    "    # print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "199df0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_baseline:\n",
    "    VNN = plVNN(model)\n",
    "    optimizer = VNN.configure_optimizers()\n",
    "\n",
    "    logger = TensorBoardLogger(\"tb_vnn_logs\", name=\"vnn-02.31-TESTING-REMOVE-ME\")\n",
    "    trainer = pl.Trainer(max_epochs=max_epoch, logger=logger, precision=16)\n",
    "\n",
    "    trainer.fit(model=VNN, train_dataloaders=training_dataloader, val_dataloaders=validation_dataloader)\n",
    "\n",
    "    # Baseline:\n",
    "    # 0.31 it/s\n",
    "    # using precision=16 in pl.Trainer()\n",
    "    # 0.30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0452d33b",
   "metadata": {},
   "source": [
    "## Fit Using VNNHelper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c22fe631",
   "metadata": {},
   "outputs": [],
   "source": [
    "            # training_dataloader = DataLoader(ListDataset(\n",
    "            #         y = X.get('YMat', ops_string='cs filter:val:train asarray from_numpy float cuda:0')[:, None],\n",
    "            #         # y = y_temp[train_idx][:, None].to('cuda'),\n",
    "            #         x_list = [e.to('cuda') for e in x_list_temp],\n",
    "\n",
    "            #         obs_idxs  = X.get('val:train',             ops_string='asarray from_numpy'),\n",
    "            #         obs_geno_lookup = X.get('obs_geno_lookup', ops_string='asarray from_numpy'),\n",
    "\n",
    "            #         # obs_idxs = train_idx, \n",
    "            #         # obs_geno_lookup = obs_geno_lookup\n",
    "            #     ),\n",
    "            #     batch_size = batch_size,\n",
    "            #     shuffle = True\n",
    "            # )\n",
    "\n",
    "            # validation_dataloader = DataLoader(ListDataset(\n",
    "            #         y = X.get('YMat', ops_string='cs filter:val:test asarray from_numpy float cuda:0')[:, None],\n",
    "            #         # y = y_temp[test_idx][:, None].to('cuda'),\n",
    "            #         x_list = [e.to('cuda') for e in x_list_temp],\n",
    "\n",
    "            #         obs_idxs  = X.get('val:test',              ops_string='asarray from_numpy'),\n",
    "            #         obs_geno_lookup = X.get('obs_geno_lookup', ops_string='asarray from_numpy'),\n",
    "\n",
    "            #         # obs_idxs = test_idx, \n",
    "            #         # obs_geno_lookup = obs_geno_lookup\n",
    "            #     ),\n",
    "            #     batch_size = batch_size,\n",
    "            #     shuffle = False\n",
    "            # )\n",
    "\n",
    "            # # next(iter(training_dataloader))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8708582",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "4b869a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # extracted from above\n",
    "\n",
    "                                # X = g2fc_datawrapper()\n",
    "                                # X.set_split()\n",
    "                                # X.load_all(name_list = ['obs_geno_lookup', 'YMat', 'KEGG_slices',], store=True) \n",
    "                                # X.calc_cs('YMat', version = 'np', filter = 'val:train')\n",
    "                                # ACGT_gene_slice_list =     X.get('KEGG_slices', ops_string='')\n",
    "                                # parsed_kegg_gene_entries = X.get('KEGG_entries')\n",
    "\n",
    "# # Restrict to only those with pathway\n",
    "# kegg_gene_brite = [e for e in parsed_kegg_gene_entries if 'BRITE' in e.keys()]\n",
    "\n",
    "# # also require to have a non-empty path\n",
    "# kegg_gene_brite = [e for e in kegg_gene_brite if not e['BRITE']['BRITE_PATHS'] == []]\n",
    "\n",
    "# print('Retaining '+ str(round(len(kegg_gene_brite)/len(parsed_kegg_gene_entries), 4)*100)+'%, '+str(len(kegg_gene_brite)\n",
    "#      )+'/'+str(len(parsed_kegg_gene_entries)\n",
    "#      )+' Entries'\n",
    "#      )\n",
    "\n",
    "# def kegg_gene_brite_build_node_lookup_dict(kegg_gene_brite):\n",
    "#     # build a dict to go from the node names in `no_dependants` to the list index in `ACGT_gene_slice_list`\n",
    "#     brite_node_to_list_idx_dict = {}\n",
    "#     for i in tqdm(range(len(kegg_gene_brite))):\n",
    "#         brite_node_to_list_idx_dict[str(kegg_gene_brite[i]['BRITE']['BRITE_PATHS'][0][-1])] = i\n",
    "#     return brite_node_to_list_idx_dict\n",
    "\n",
    "# brite_node_to_list_idx_dict = kegg_gene_brite_build_node_lookup_dict(kegg_gene_brite = kegg_gene_brite)\n",
    "\n",
    "# for e in list(brite_node_to_list_idx_dict.keys())[0:10]:\n",
    "#     print(f'{e} {brite_node_to_list_idx_dict[e]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "825798f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsed_kegg_gene_entries[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2a879f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(parsed_kegg_gene_entries), len(ACGT_gene_slice_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eede407e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ed8e82fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and storing default `phno`.\n",
      "Retaining 43.53%, 6067/13939 Entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 6067/6067 [00:00<00:00, 54709.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed node \"Others\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Same setup as above to create kegg_gene_brite\n",
    "X = g2fc_datawrapper()\n",
    "X.set_split()\n",
    "X.load_all(name_list = ['obs_geno_lookup', 'YMat', 'KEGG_slices',], store=True) \n",
    "X.calc_cs('YMat', version = 'np', filter = 'val:train')\n",
    "ACGT_gene_slice_list =     X.get('KEGG_slices', ops_string='')\n",
    "parsed_kegg_gene_entries = X.get('KEGG_entries')\n",
    "\n",
    "\n",
    "# Restrict to only those with pathway\n",
    "kegg_gene_brite = [e for e in parsed_kegg_gene_entries if 'BRITE' in e.keys()]\n",
    "\n",
    "# also require to have a non-empty path\n",
    "kegg_gene_brite = [e for e in kegg_gene_brite if not e['BRITE']['BRITE_PATHS'] == []]\n",
    "\n",
    "print('Retaining '+ str(round(len(kegg_gene_brite)/len(parsed_kegg_gene_entries), 4)*100)+'%, '+str(len(kegg_gene_brite)\n",
    "    )+'/'+str(len(parsed_kegg_gene_entries)\n",
    "    )+' Entries'\n",
    "    )\n",
    "# kegg_gene_brite[1]['BRITE']['BRITE_PATHS']\n",
    "\n",
    "\n",
    "kegg_connections = kegg_connections_append_y_hat(\n",
    "    kegg_connections = kegg_connections_clean(\n",
    "        kegg_connections = kegg_connections_build(\n",
    "            n_genes = 6067, kegg_gene_brite = kegg_gene_brite) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ef64b7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# have to clean the key values so that none have '.' because I'm looking up nodes by name and pytorch doesn't allow this.\n",
    "\n",
    "#NOTE this will potentially (but not here) create a bug if there are genes named with '.' \n",
    "\n",
    "replace_chars = {\n",
    "    '.':'_'\n",
    "}\n",
    "\n",
    "def replace_select_chars(in_str, replace_chars):\n",
    "    for key in replace_chars.keys():\n",
    "        in_str = in_str.replace(key, replace_chars[key])\n",
    "    return in_str\n",
    "\n",
    "new_kegg_connections = {}\n",
    "for key in kegg_connections.keys():\n",
    "    new_kegg_connections[replace_select_chars(in_str = key, replace_chars=replace_chars)] = [replace_select_chars(in_str = e, replace_chars=replace_chars) for e in kegg_connections[key]]\n",
    "\n",
    "kegg_connections = new_kegg_connections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "db7ac803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['103646080',\n",
       " '103632825',\n",
       " '542452',\n",
       " '100191949',\n",
       " '103639636',\n",
       " '100277723',\n",
       " '103627409',\n",
       " '100502380',\n",
       " '100191676',\n",
       " '103653470']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "myvnn = VNNHelper(edge_dict = kegg_connections)\n",
    "\n",
    "myvnn.nodes_inp[0:10]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7281b367",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "myvnn = VNNHelper(edge_dict = kegg_connections)\n",
    "\n",
    "myvnn.nodes_inp[0:10]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "6453392d",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_keys_vals = myvnn.node_keys_vals\n",
    "add_these_nodes = [e for e in node_keys_vals['all_values'] if e not in node_keys_vals['all_keys']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e4efd98b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'100193253' in myvnn.node_keys_vals['all_keys']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "0346cdc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'100193253' in myvnn.node_keys_vals['all_values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "63edeac2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myvnn = VNNHelper(edge_dict = kegg_connections)\n",
    "\n",
    "myvnn.nodes_inp[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a31c9db4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find_names = ['100383860', '100278565', '103630585', '100275685', '100383837', '100191673']\n",
    "find_names = myvnn.nodes_inp\n",
    "lookup_dict = {}\n",
    "\n",
    "for i in range(len(parsed_kegg_gene_entries)):\n",
    "    if 'BRITE' not in parsed_kegg_gene_entries[i].keys():\n",
    "        pass\n",
    "    elif parsed_kegg_gene_entries[i]['BRITE']['BRITE_PATHS'] == []:\n",
    "        pass\n",
    "    else:\n",
    "        name = parsed_kegg_gene_entries[i]['BRITE']['BRITE_PATHS'][0][-1]\n",
    "        if name in find_names:\n",
    "            lookup_dict[name] = i\n",
    "lookup_dict            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ee9ed8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACGT_gene_slice_list = X.get('KEGG_slices',     ops_string='')\n",
    "\n",
    "size_in_zip = zip(myvnn.nodes_inp, [np.prod(ACGT_gene_slice_list[lookup_dict[e]].shape[1:]) for e  in myvnn.nodes_inp])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ee40f313",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def build_input_tensor_dict(ACGT_gene_slice_list, no_dependants, brite_node_to_list_idx_dict):    \n",
    "#     input_tensor_dict = {}\n",
    "#     for e in no_dependants:\n",
    "#         input_tensor_dict[e] = ACGT_gene_slice_list[brite_node_to_list_idx_dict[e]]\n",
    "    \n",
    "#     return input_tensor_dict\n",
    "\n",
    "# input_tensor_dict = build_input_tensor_dict(ACGT_gene_slice_list=ACGT_gene_slice_list, \n",
    "#                                             no_dependants= myvnn.nodes_inp, \n",
    "#                                             brite_node_to_list_idx_dict = brite_node_to_list_idx_dict)\n",
    "\n",
    "\n",
    "# # use above as an intermediary. This is what I want:\n",
    "# inp_tensor_name_to_size = {}\n",
    "\n",
    "# for key in input_tensor_dict.keys():\n",
    "#     inp_tensor_name_to_size[key] = int(torch.prod(torch.tensor(list(input_tensor_dict[key].shape)[1:])))\n",
    "\n",
    "# inp_tensor_name_to_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817e6f2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "637ec7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO rename node_val_dict to node_val_zip to accurately reflect what this is and/or add explicit zip and dict modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d4d19eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "myvnn.set_node_props(key = 'inp', node_val_dict = size_in_zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305352c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init input node sizes\n",
    "# myvnn.set_node_props(key = 'inp', node_val_dict = zip(\n",
    "#     [e for e in inp_tensor_name_to_size.keys()],\n",
    "#     [inp_tensor_name_to_size[e] for e in inp_tensor_name_to_size.keys()])\n",
    "#     )\n",
    "\n",
    "# # init node output sizes\n",
    "myvnn.set_node_props(key = 'out', node_val_dict = zip(myvnn.nodes_inp, [5 for e in myvnn.nodes_inp]))\n",
    "myvnn.set_node_props(key = 'out', node_val_dict = zip(myvnn.nodes_edge,[5 for e in myvnn.nodes_edge]))\n",
    "myvnn.set_node_props(key = 'out', node_val_dict = zip(myvnn.nodes_out, [1 for e in myvnn.nodes_out]))\n",
    "\n",
    "\n",
    "# # options should be controlled by node_props\n",
    "myvnn.set_node_props(key = 'flatten', node_val_dict = zip(\n",
    "    myvnn.nodes_inp, \n",
    "    [True for e in myvnn.nodes_inp]))\n",
    "\n",
    "myvnn.set_node_props(key = 'reps', node_val_dict = zip(\n",
    "    myvnn.nodes_out+myvnn.nodes_inp+myvnn.nodes_edge, \n",
    "    [1 for e in myvnn.nodes_out+myvnn.nodes_inp+myvnn.nodes_edge]))\n",
    "\n",
    "# init dropout \n",
    "myvnn.set_node_props(key = 'drop', node_val_dict = zip(\n",
    "    myvnn.nodes_out+myvnn.nodes_inp+myvnn.nodes_edge, \n",
    "    [0.0 for e in myvnn.nodes_out+myvnn.nodes_inp+myvnn.nodes_edge]))\n",
    "\n",
    "# init edge node input size (propagate forward input/edge outpus)\n",
    "myvnn.calc_edge_inp()\n",
    "\n",
    "# myvnn.mk_digraph(include = ['node_name', 'inp_size', 'out_size'])\n",
    "# myvnn.mk_digraph(include = [''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68acf08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_list_temp = [torch.from_numpy(input_tensor_dict[key]).to(torch.float) for key in input_tensor_dict.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5a28b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modification of build_input_tensor_dict\n",
    "def build_node_to_inp_num_dict(ACGT_gene_slice_list, no_dependants, brite_node_to_list_idx_dict):    \n",
    "    input_tensor_dict = {}\n",
    "    for e in no_dependants:\n",
    "        input_tensor_dict[e] = brite_node_to_list_idx_dict[e]\n",
    "    \n",
    "    return input_tensor_dict\n",
    "\n",
    "temp_node_to_inp_num_dict = build_node_to_inp_num_dict(ACGT_gene_slice_list=ACGT_gene_slice_list, \n",
    "                                            no_dependants= myvnn.nodes_inp, \n",
    "                                            brite_node_to_list_idx_dict = brite_node_to_list_idx_dict)\n",
    "temp_node_to_inp_num_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c158d1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89f98c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACGT_gene_slice_list = X.get('KEGG_slices',     ops_string='')\n",
    "# x_list_temp = [ACGT_gene_slice_list[brite_node_to_list_idx_dict[e]] for e in myvnn.nodes_inp]\n",
    "\n",
    "# x_list_temp = [torch.tensor(e).to(torch.float)#.to('cuda') \n",
    "#                for e in x_list_temp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0578e525",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisableNeuralNetwork(nn.Module):\n",
    "    def __init__(self, \n",
    "                 node_props, \n",
    "                 Linear_block,\n",
    "                 dependancy_order,\n",
    "                 edge_dict,\n",
    "                 node_to_inp_num_dict\n",
    "                ):\n",
    "        super(VisableNeuralNetwork, self).__init__()\n",
    "        # Store nodes in dict\n",
    "        layer_dict = {}\n",
    "        for key in node_props.keys():\n",
    "            node_list = []\n",
    "            if 'flatten' in node_props[key]:\n",
    "                node_list += [nn.Flatten()]\n",
    "            node_list += [nn.Flatten()]\n",
    "            #TODO change linear block instead of conditioning on node name.\n",
    "            if key != 'y_hat':\n",
    "                node_list += [Linear_block(\n",
    "                    in_size=node_props[key]['inp'], \n",
    "                    out_size=node_props[key]['out'], \n",
    "                    drop_pr=node_props[key]['drop'],\n",
    "                    block_reps=node_props[key]['reps'])]\n",
    "            else:\n",
    "                node_list += [nn.Linear(node_props[key]['inp'], node_props[key]['out'])]\n",
    "\n",
    "            layer_dict[key] = nn.ModuleList(node_list)\n",
    "\n",
    "        self.layer_dict = nn.ModuleDict(layer_dict)\n",
    "        \n",
    "        self.dependancy_order = dependancy_order\n",
    "        self.edge_dict = edge_dict\n",
    "        self.node_to_inp_num_dict = node_to_inp_num_dict\n",
    "\n",
    "    def forward(self, x):\n",
    "        temp_res_dict = {}\n",
    "        for key in self.dependancy_order:\n",
    "            \n",
    "            # if the node depends on raw inputs, get them.\n",
    "            if key in self.node_to_inp_num_dict:\n",
    "                xin = [ x[self.node_to_inp_num_dict[key]] ]\n",
    "\n",
    "            # if the node depends on inputs that have been stored in the lookup dict\n",
    "            if self.edge_dict[key] != []:\n",
    "                if xin == None:\n",
    "                    xin = []\n",
    "                xin += [temp_res_dict[e] for e in self.edge_dict[key]]\n",
    "                \n",
    "            # join all input tensors.\n",
    "            xin = torch.concat(xin, axis = 1)\n",
    "\n",
    "            for l in self.layer_dict[key]:\n",
    "                if type(l) == torch.nn.modules.container.ModuleList:\n",
    "                    for ll in l:            \n",
    "                        xin = ll(xin)\n",
    "                else:\n",
    "                    xin = l(xin)\n",
    "                    \n",
    "            temp_res_dict[key] = xin\n",
    "            xin = None \n",
    "        return temp_res_dict[self.dependancy_order[-1]]\n",
    "\n",
    "\n",
    "model = VisableNeuralNetwork(\n",
    "    node_props = myvnn.node_props,\n",
    "    Linear_block = Linear_block,\n",
    "    edge_dict = myvnn.edge_dict,\n",
    "    dependancy_order = myvnn.dependancy_order,\n",
    "    # node_to_inp_num_dict = temp_node_to_inp_num_dict\n",
    "    node_to_inp_num_dict = lookup_dict\n",
    ")\n",
    "model = model.to('cuda')\n",
    "# model.cuda()\n",
    "next(model.parameters()).is_cuda\n",
    "\n",
    "\n",
    "\n",
    "model( [torch.tensor(e).to(torch.float).to('cuda') \n",
    "               for e in ACGT_gene_slice_list]\n",
    "               )\n",
    "\n",
    "# model(next(iter(training_dataloader))[1])[0:5]\n",
    "\n",
    "# model.node_to_inp_num_dict#['100383837']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78aa4bc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8c9cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataloader = DataLoader(ListDataset(\n",
    "        y =               X.get('YMat',ops_string='cs filter:val:train asarray from_numpy float cuda:0')[:, None],\n",
    "        x_list = [torch.tensor(e).to(torch.float).to('cuda') for e in ACGT_gene_slice_list],\n",
    "        obs_idxs =        X.get('val:train',       ops_string='   asarray from_numpy      '), \n",
    "        obs_geno_lookup = X.get('obs_geno_lookup', ops_string='   asarray from_numpy      ')\n",
    "    ),\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "\n",
    "validation_dataloader = DataLoader(ListDataset(\n",
    "        y =               X.get('YMat',ops_string='cs filter:val:test asarray from_numpy float cuda:0')[:, None],\n",
    "        x_list = [torch.tensor(e).to(torch.float).to('cuda') for e in ACGT_gene_slice_list],\n",
    "        obs_idxs =        X.get('val:test',        ops_string='   asarray from_numpy      '), \n",
    "        obs_geno_lookup = X.get('obs_geno_lookup', ops_string='   asarray from_numpy      ')\n",
    "    ),\n",
    "    batch_size = batch_size,\n",
    "    shuffle = False\n",
    ")\n",
    "# next(iter(training_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4757ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "LSUV_(model, data = next(iter(training_dataloader))[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f863df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "VNN = plVNN(model)\n",
    "optimizer = VNN.configure_optimizers()\n",
    "\n",
    "logger = TensorBoardLogger(\"tb_vnn_logs\", name=\"vnn-02.31-TESTING-REMOVE-ME\")\n",
    "trainer = pl.Trainer(max_epochs=max_epoch, logger=logger)\n",
    "\n",
    "trainer.fit(model=VNN, train_dataloaders=training_dataloader, val_dataloaders=validation_dataloader)\n",
    "\n",
    "\n",
    "# Baseline: (old version)\n",
    "# 0.31 it/s\n",
    "# This version:\n",
    "# 0.39 it/s -- slightly faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487e967a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d041c4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
