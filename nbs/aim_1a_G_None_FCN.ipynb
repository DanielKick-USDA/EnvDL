{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "source": [
    "# Fully Connected Genomic Model (No filtering) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy  as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from   torch import nn\n",
    "import torch.nn.functional as F\n",
    "from   torch.utils.data import Dataset\n",
    "from   torch.utils.data import DataLoader\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from   lightning.pytorch.loggers import CSVLogger\n",
    "\n",
    "from dataG2F.core import get_data\n",
    "from dataG2F.qol  import ensure_dir_path_exists\n",
    "\n",
    "from EnvDL.dlfn import BigDataset, plDNN_general\n",
    "from EnvDL.sets import mask_parents # for creating sets on the fly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from EnvDL.dlfn import Linear_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_path = '../nbs_artifacts/aim_1a_G_None_FCN/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run settings: \n",
    "params_run = {\n",
    "    'batch_size': 48,\n",
    "    'batch_size': 256,\n",
    "    'max_epoch' : 2,   \n",
    "}\n",
    "\n",
    "# data settings\n",
    "params_data = {\n",
    "    'y_var': 'Yield_Mg_ha',\n",
    "    'y_resid': 'None', # None, Env, Geno\n",
    "    'y_resid_strat': 'None', # None, naive_mean, filter_mean, ...\n",
    "    'holdout_parents': [\n",
    "        'PHZ51',\n",
    "    ],    \n",
    "}\n",
    "\n",
    "# in this file I define params later. I've included it here to gurantee that we can merge other params dicts into it.\n",
    "params = {\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = params_run['batch_size']\n",
    "max_epoch  = params_run['max_epoch']\n",
    "\n",
    "y_var = params_data['y_var']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "save_prefix = [e for e in cache_path.split('/') if e != ''][-1]\n",
    "\n",
    "if 'None' != params_data['y_resid_strat']:\n",
    "    save_prefix = save_prefix+'_'+params_data['y_resid_strat']\n",
    "\n",
    "ensure_dir_path_exists(dir_path = cache_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "use_gpu_num = 0\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if use_gpu_num in [0, 1]: \n",
    "    torch.cuda.set_device(use_gpu_num)\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_geno_lookup          = get_data('obs_geno_lookup')\n",
    "phno                     = get_data('phno')\n",
    "acgt                     = get_data('ACGT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten data\n",
    "acgt = acgt.reshape(4926, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make holdout sets\n",
    "holdout_parents = params_data['holdout_parents']\n",
    "\n",
    "# create a mask for parent genotype\n",
    "mask = mask_parents(df= phno, col_name= 'Hybrid', holdout_parents= holdout_parents)\n",
    "\n",
    "train_mask = mask.sum(axis=1) == 0\n",
    "test_mask  = mask.sum(axis=1) > 0\n",
    "\n",
    "train_idx = train_mask.loc[train_mask].index\n",
    "test_idx  = test_mask.loc[test_mask].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert y to residual if needed\n",
    "\n",
    "if params_data['y_resid'] == 'None':\n",
    "    pass\n",
    "else:\n",
    "    if params_data['y_resid_strat'] == 'naive_mean':\n",
    "        # use only data in the training set (especially since testers will be more likely to be found across envs)\n",
    "        # get enviromental means, subtract from observed value\n",
    "        tmp = phno.loc[train_idx, ]\n",
    "        env_mean = tmp.groupby(['Env_Idx']\n",
    "                     ).agg(Env_Mean = (y_var, 'mean')\n",
    "                     ).reset_index()\n",
    "        tmp = phno.merge(env_mean)\n",
    "        tmp.loc[:, y_var] = tmp.loc[:, y_var] - tmp.loc[:, 'Env_Mean']\n",
    "        phno = tmp.drop(columns='Env_Mean')\n",
    "\n",
    "    if params_data['y_resid_strat'] == 'filter_mean':\n",
    "        # for adjusting to environment we could use _all_ observations but ideally we will use the same set of genotypes across all observations\n",
    "        def minimum_hybrids_for_env(tmp = phno.loc[:, ['Env', 'Year', 'Hybrid']],\n",
    "                                    year = 2014):\n",
    "            # Within each year what hybrids are most common?\n",
    "            tmp = tmp.loc[(tmp.Year == year), ].groupby(['Env', 'Hybrid']).count().reset_index().sort_values('Year')\n",
    "\n",
    "            all_envs = set(tmp.Env)\n",
    "            # if we filter on the number of sites a hybrid is planted at, what is the largest number of sites we can ask for before we lose a location?\n",
    "            # site counts for sets which contain all envs\n",
    "            i = max([i for i in list(set(tmp.Year)) if len(set(tmp.loc[(tmp.Year >= i), 'Env'])) == len(all_envs)])\n",
    "\n",
    "            before = len(set(tmp.loc[:, 'Hybrid']))\n",
    "            after  = len(set(tmp.loc[(tmp.Year >= i), 'Hybrid']))\n",
    "            print(f'Reducing {year} hybrids from {before} to {after} ({round(100*after/before)}%).')\n",
    "            tmp = tmp.loc[(tmp.Year >= i), ['Env', 'Hybrid']].reset_index(drop=True)\n",
    "            return tmp\n",
    "\n",
    "\n",
    "        tmp = phno.loc[:, ['Env', 'Year', 'Hybrid']]\n",
    "        filter_hybrids = [minimum_hybrids_for_env(tmp = phno.loc[:, ['Env', 'Year', 'Hybrid']], year = i) \n",
    "                          for i in list(set(phno.Year)) ]\n",
    "        env_mean = pd.concat(filter_hybrids).merge(phno, how = 'left')\n",
    "\n",
    "        env_mean = env_mean.groupby(['Env_Idx']\n",
    "                          ).agg(Env_Mean = (y_var, 'mean')\n",
    "                          ).reset_index()\n",
    "\n",
    "        tmp = phno.merge(env_mean)\n",
    "        tmp.loc[:, y_var] = tmp.loc[:, y_var] - tmp.loc[:, 'Env_Mean']\n",
    "        phno = tmp.drop(columns='Env_Mean')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# center and y value data\n",
    "assert 0 == phno.loc[:, y_var].isna().sum()\n",
    "\n",
    "y = phno.loc[:, y_var]\n",
    "# use train index to prevent information leakage\n",
    "y_c = y[train_idx].mean()\n",
    "y_s = y[train_idx].std()\n",
    "\n",
    "y = (y - y_c)/y_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataloader = DataLoader(BigDataset(\n",
    "    lookups_are_filtered = False,\n",
    "    lookup_obs  = torch.from_numpy(np.array(train_idx)), \n",
    "    lookup_geno = torch.from_numpy(obs_geno_lookup),\n",
    "    y =           torch.from_numpy(y.to_numpy()).to(torch.float32)[:, None],\n",
    "    G =           torch.from_numpy(acgt).to(torch.float32),\n",
    "    G_type = 'raw',\n",
    "    send_batch_to_gpu = 'cuda:0'\n",
    "    ),\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "validation_dataloader = DataLoader(BigDataset(\n",
    "    lookups_are_filtered = False,\n",
    "    lookup_obs  = torch.from_numpy(np.array(test_idx)), \n",
    "    lookup_geno = torch.from_numpy(obs_geno_lookup),\n",
    "    y =           torch.from_numpy(y.to_numpy()).to(torch.float32)[:, None],\n",
    "    G =           torch.from_numpy(acgt).to(torch.float32),\n",
    "    G_type = 'raw',\n",
    "    send_batch_to_gpu = 'cuda:0'\n",
    "    ),\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Parameters of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convenince wrapper to fill in for R's seq or x:y notation\n",
    "def linrange(start, stop):\n",
    "    import numpy as np\n",
    "    diff = start - stop\n",
    "    res = np.linspace(start, stop, abs(diff)+1).astype(int)\n",
    "    return res "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2048, 1024, 512, 256, 128, 64, 32, 16]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[2**i for i in linrange(11, 4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This one is designed to go from (3x125891) -> ~1258.91  -> ~125.891 -> ~12.5891 -> 1\n",
    "layer_sizes = [1024, 128, 12]\n",
    "layer_drops = [0.1 for e in layer_sizes]\n",
    "\n",
    "num_layers = len(layer_sizes)\n",
    "\n",
    "params = {\n",
    "    'num_layers':num_layers,\n",
    "    f\"in_1_of_{num_layers}\": (4 * 125891)\n",
    "}\n",
    "\n",
    "for i in range(num_layers):\n",
    "    params[f\"out_{ i + 1}_of_{num_layers}\"] = layer_sizes[i]\n",
    "    params[f\"drop_{ i + 1}_of_{num_layers}\"] = layer_drops[i]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_layers': 3,\n",
       " 'in_1_of_3': 503564,\n",
       " 'out_1_of_3': 1024,\n",
       " 'drop_1_of_3': 0.1,\n",
       " 'out_2_of_3': 128,\n",
       " 'drop_2_of_3': 0.1,\n",
       " 'out_3_of_3': 12,\n",
       " 'drop_3_of_3': 0.1}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([256, 1]), torch.Size([256, 503564])]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[e.shape for e in next(iter(training_dataloader))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A quirk of this is that to get only a single layer the length of the input tensor must be passed in. for 2+ I'll figure it out.\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, parameterization):\n",
    "        super(NeuralNetwork, self).__init__()            \n",
    "        module_list = []\n",
    "\n",
    "        max_layer = parameterization['num_layers']\n",
    "        for i in range(max_layer):\n",
    "            if i  == 0:\n",
    "                name_in = f\"in_{i+1}_of_{max_layer}\"\n",
    "            else:\n",
    "                name_in = f\"out_{i}_of_{max_layer}\"\n",
    "            name_out = f\"out_{i+1}_of_{max_layer}\"\n",
    "            name_drop= f\"drop_{i+1}_of_{max_layer}\"\n",
    "\n",
    "            if i == 0:\n",
    "                module_list += [nn.Flatten()]\n",
    "            \n",
    "\n",
    "            module_list += [\n",
    "                Linear_block(\n",
    "                    in_size  = parameterization[name_in], \n",
    "                    out_size = parameterization[name_out], \n",
    "                    drop_pr  = parameterization[name_drop])]\n",
    "            \n",
    "            if (i+1) == max_layer:\n",
    "                module_list += [nn.Linear(parameterization[name_out], 1)]\n",
    "                \n",
    "        self.x_network = nn.ModuleList(module_list)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        for mod in self.x_network:\n",
    "            if mod == self.x_network[-1]:\n",
    "                out = x # get the penultimate layer's outputs for later\n",
    "            x = mod(x)\n",
    "        \n",
    "        pred = x\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(parameterization = params).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model(next(iter(training_dataloader))[1])[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kickd/miniconda3/envs/fastai/lib/python3.11/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/kickd/miniconda3/envs/fastai/lib/python3.11/si ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name | Type          | Params\n",
      "---------------------------------------\n",
      "0 | mod  | NeuralNetwork | 515 M \n",
      "---------------------------------------\n",
      "515 M     Trainable params\n",
      "0         Non-trainable params\n",
      "515 M     Total params\n",
      "2,063.133 Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ed760879f2b44cfb78162d2194ac90c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kickd/miniconda3/envs/fastai/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "/home/kickd/miniconda3/envs/fastai/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "/home/kickd/miniconda3/envs/fastai/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0281a589aa9249a7b7cc43416f1160b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "befe79ef4817414db5e50c89968e5413",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0c9dd096bfc4291bc34c132c3073bce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    }
   ],
   "source": [
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "DNN = plDNN_general(model)  \n",
    "\n",
    "optimizer = DNN.configure_optimizers()\n",
    "\n",
    "logger = CSVLogger(\"nifa_tb\", name=save_prefix)\n",
    "logger.log_hyperparams(params={\n",
    "    'params': params,\n",
    "    'params_data': params_data,\n",
    "    'params_run' : params_run\n",
    "})\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=max_epoch, logger=logger)\n",
    "\n",
    "trainer.fit(model=DNN, train_dataloaders=training_dataloader, val_dataloaders=validation_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model with versioning. \n",
    "# this is hacky but should work\n",
    "import os\n",
    "versions = os.listdir('nifa_tb/'+save_prefix)\n",
    "versions.sort()\n",
    "torch.save(DNN.mod, cache_path+versions[-1]+'.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
