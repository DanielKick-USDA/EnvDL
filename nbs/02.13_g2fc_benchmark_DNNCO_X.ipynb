{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aadccc0",
   "metadata": {},
   "source": [
    "# Benchmark DNNCO X\n",
    "\n",
    "> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc2f33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "pio.templates.default = \"plotly_white\"\n",
    "\n",
    "import hilbertcurve\n",
    "from hilbertcurve.hilbertcurve import HilbertCurve\n",
    "\n",
    "from EnvDL.core import * # includes remove_matching_files\n",
    "from EnvDL.dna  import *\n",
    "from EnvDL.dlfn import * # includes LSUV_\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7581e1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830cbde0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "use_gpu_num = 0\n",
    "\n",
    "# Imports --------------------------------------------------------------------\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import torch.nn.functional as F # F.mse_loss\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if use_gpu_num in [0, 1]: \n",
    "    torch.cuda.set_device(use_gpu_num)\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4600ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_path = '../nbs_artifacts/02.13_g2fc_benchmark_DNNCO_X/'\n",
    "ensure_dir_path_exists(dir_path = cache_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517af1f8",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d764092",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_from = '../nbs_artifacts/01.03_g2fc_prep_matrices/'\n",
    "phno_geno = pd.read_csv(load_from+'phno_geno.csv')\n",
    "phno = phno_geno\n",
    "\n",
    "obs_env_lookup  = np.load(load_from+'obs_env_lookup.npy')\n",
    "obs_geno_lookup = np.load(load_from+'obs_geno_lookup.npy') # Phno_Idx\tGeno_Idx\tIs_Phno_Idx\n",
    "YMat = np.load(load_from+'YMat.npy')\n",
    "SMat = np.load(load_from+'SMat.npy')\n",
    "WMat = np.load(load_from+'WMat.npy')\n",
    "# PlantHarvestNames = np.load(load_from+'PlantHarvestNames.npy') \n",
    "PlantHarvest = np.load(load_from+'PlantHarvest.npy')  # 'DOY_Planted', 'DOY_Harvested'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6df238",
   "metadata": {},
   "source": [
    "### Screeplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08980d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_from = '../data/zma/g2fc/genotypes_G3_filter/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4179e22e",
   "metadata": {},
   "source": [
    "### PCA Transformed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb289b06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4928, 4882)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GMat = pd.read_table(load_from+'PC_Imputed_5_Genotype_Data_All_Years_G3_Hz_0.0675.txt',\n",
    "                     sep = '\\t',\n",
    "                     skiprows=2,\n",
    "                    low_memory = False)\n",
    "GMat.shape # two more entries than the data I've been using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0b5ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to check taxa match\n",
    "assert 0 == len([e for e in list(set(phno_geno['Hybrid'])) if e not in list(GMat['Taxa'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a23e7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the desired genotypes in the desired order\n",
    "temp = pd.concat([phno_geno['Hybrid'], pd.DataFrame(obs_geno_lookup)], axis = 1)\n",
    "temp = temp.loc[:, ['Hybrid', 1]].drop_duplicates().sort_values(1).reset_index(drop = True)\n",
    "# temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38ae5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder PCA data, then convert to numpy\n",
    "GMat = temp.merge(GMat.rename(columns = {'Taxa':'Hybrid', 'index':'Gpca_Idx'})).drop(columns = 1)\n",
    "GMatHybrid = GMat['Hybrid']\n",
    "GMat = GMat.loc[:, [i for i in list(GMat) if i != 'Hybrid']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa2e84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "GMat = np.asarray(GMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10fd092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We should now be able to use obs_geno_lookup to lookup PCA values.\n",
    "assert GMat.shape[0] == len(set(obs_geno_lookup[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d132b79c",
   "metadata": {},
   "source": [
    "### Custom Dataloader for G S W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761d167d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([288, 288, 288, ..., 288, 288, 288])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_start = PlantHarvest[:, 0]-76 # 75 before planting date\n",
    "window_end   = PlantHarvest[:, 0]+212 # 212 after (288 total per W.npy in paper)\n",
    "window_end - window_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a49bed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-16, 18)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the increased range of planting dates causes the window to extend beyond the target year. \n",
    "(window_start.min(), \n",
    " window_end.max() - 364) #pd.to_datetime() is 0 indexed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1106628a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([254, 254, 254, ..., 254, 254, 254])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_start = PlantHarvest[:, 0]-60\n",
    "window_end   = PlantHarvest[:, 0]+194\n",
    "window_end - window_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cb3c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert window_start.min() >= 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32e8348",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert window_end.max() <= 364 #pd.to_datetime() is 0 indexed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da3e749",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef13898c",
   "metadata": {},
   "source": [
    "### Create train/test validate indicies from json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a02cab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_from = '../nbs_artifacts/01.06_g2fc_cluster_genotypes/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de62ab64",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_info = read_split_info(\n",
    "    load_from = '../nbs_artifacts/01.06_g2fc_cluster_genotypes/',\n",
    "    json_prefix = '2023:9:5:12:8:26')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c910dd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = phno.copy()\n",
    "temp[['Female', 'Male']] = temp['Hybrid'].str.split('/', expand = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead7a1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict = find_idxs_split_dict(\n",
    "    obs_df = temp, \n",
    "    split_dict = split_info['test'][0]\n",
    ")\n",
    "# test_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5288250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since this is applying predefined model structure no need for validation.\n",
    "# This is included for my future reference when validation is needed.\n",
    "temp = temp.loc[test_dict['train_idx'], ] # restrict before re-aplying\n",
    "\n",
    "val_dict = find_idxs_split_dict(\n",
    "    obs_df = temp, \n",
    "    split_dict = split_info['validate'][0]\n",
    ")\n",
    "# val_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e8a6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea0cacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation not strictly needed since there is no tuning to do. Included to help with comparison to other models\n",
    "# train_idx = test_dict['train_idx']\n",
    "# test_idx  = test_dict['test_idx']\n",
    "\n",
    "train_idx = val_dict['train_idx']\n",
    "test_idx  = val_dict['test_idx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbaedf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm all observation idxs are have genomic information\n",
    "assert [] == [e for e in list(train_idx)+list(test_idx) if e not in obs_geno_lookup[:, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02004189",
   "metadata": {},
   "outputs": [],
   "source": [
    "YMat_cs = calc_cs(YMat[train_idx])\n",
    "y_cs = apply_cs(YMat, YMat_cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fa747b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SMat_cs = calc_cs(SMat[list(set(obs_env_lookup[train_idx, 1])), ])\n",
    "s_cs = apply_cs(SMat, SMat_cs)\n",
    "\n",
    "WMat_cs = calc_cs(WMat[list(set(obs_env_lookup[train_idx, 1])), ])\n",
    "w_cs = apply_cs(WMat, WMat_cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a12589",
   "metadata": {},
   "outputs": [],
   "source": [
    "GMat_cs = calc_cs(GMat[list(set(obs_geno_lookup[train_idx, 1])), ])\n",
    "g_cs = apply_cs(GMat, GMat_cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f323b349",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c4d375",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d673ab54",
   "metadata": {},
   "source": [
    "## Use Pytorch Lightning to Train each Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dc7a9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ca0b84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a853ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2df1931",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678efb5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16a1664",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd006969",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51355d4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d73f9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638396f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [[e.shape for e in DNNCO_G().to(device)(g_idx, s_idx, w_idx)],\n",
    "#  [e.shape for e in DNNCO_S().to(device)(g_idx, s_idx, w_idx)],\n",
    "#  [e.shape for e in DNNCO_W().to(device)(g_idx, s_idx, w_idx)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3993a4d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cf7062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Module for training subnetworks.\n",
    "# class plDNNCO_subnet(pl.LightningModule):\n",
    "#     def __init__(self, mod):\n",
    "#         super().__init__()\n",
    "#         self.mod = mod\n",
    "        \n",
    "#     def training_step(self, batch, batch_idx):\n",
    "#         # train loop\n",
    "#         y_i, g_i, s_i, w_i = batch\n",
    "#         pred, out = self.mod(g_i, s_i, w_i)\n",
    "#         # print(y_i.shape, pred.shape)\n",
    "#         loss = F.mse_loss(pred, y_i)\n",
    "#         self.log(\"train_loss\", loss)\n",
    "#         # calculate layerwise stats \n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             weight_list=[(name, param) for name, param in model.named_parameters() if name.split('.')[-1] == 'weight']\n",
    "#             for l in weight_list:\n",
    "#                 self.log((\"train_mean\"+l[0]), l[1].mean())\n",
    "#                 self.log((\"train_std\"+l[0]), l[1].std())        \n",
    "        \n",
    "#         return(loss)\n",
    "        \n",
    "#     def validation_step(self, batch, batch_idx):\n",
    "#         y_i, g_i, s_i, w_i = batch\n",
    "#         pred, out = self.mod(g_i, s_i, w_i)\n",
    "#         # print(y_i.shape, pred.shape)\n",
    "#         loss = F.mse_loss(pred, y_i)\n",
    "#         self.log('val_loss', loss)        \n",
    "     \n",
    "#     def configure_optimizers(self, **kwargs):\n",
    "#         optimizer = torch.optim.Adam(self.parameters(), **kwargs)\n",
    "#         return optimizer    \n",
    "    \n",
    "    \n",
    "# # DNNG = plDNNCO_G(DNNCO_G().to(device))\n",
    "\n",
    "# # optimizer = DNNG.configure_optimizers()\n",
    "\n",
    "# # for batch_idx, batch in tqdm(enumerate(training_dataloader)):\n",
    "# #     loss = DNNG.training_step(batch, batch_idx)\n",
    "# #     loss.backward()\n",
    "# #     optimizer.step()\n",
    "# #     optimizer.zero_grad()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1aa4e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0f2a0f5",
   "metadata": {},
   "source": [
    "### Train with logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af875ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table3 in G3 paper https://academic.oup.com/g3journal/article/13/4/jkad006/6982634\n",
    "adam_params ={\n",
    "          #lr,     beta1,    beta2,    batch_size, max_epoch\n",
    "    'g':  [0.0001, 0.953368, 0.985947,  96,  12 ],\n",
    "    's':  [0.01,   0.928472, 0.997516, 176, 199],\n",
    "    'w':  [0.0001, 0.903649, 0.929582, 240, 629],\n",
    "    'co': [0.01,   0.98752,  0.972311, 112, 364],\n",
    "    'so': [0.001,  0.975893, 0.994607, 192, 711]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4c20de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSUV compatable version\n",
    "\n",
    "class LGSWDataset(Dataset): # for any G containing matix with many (phno) to one (geno)\n",
    "    def __init__(self, \n",
    "\n",
    "                 y, \n",
    "                 G, # not on gpu\n",
    "                 S,\n",
    "                 W,\n",
    "\n",
    "                 planting, # should be the vector so [:, 0]\n",
    "                 days_before_planting,\n",
    "                 days_after_planting,\n",
    "\n",
    "                 idx_original,\n",
    "                 idx_lookup_geno,\n",
    "                 idx_lookup_env,\n",
    "\n",
    "                 transform = None, target_transform = None,\n",
    "                 device = 'cuda',\n",
    "                 **kwargs \n",
    "                ):\n",
    "\n",
    "        self.device = device\n",
    "        self.y = y \n",
    "        self.G = G\n",
    "        self.S = S\n",
    "        self.W = W\n",
    "\n",
    "        self.window_start = planting - days_before_planting\n",
    "        self.window_end   = planting + days_after_planting\n",
    "\n",
    "        self.idx_original = idx_original\n",
    "        self.idx_lookup_geno = idx_lookup_geno\n",
    "        self.idx_lookup_env  = idx_lookup_env\n",
    "\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        y_idx =self.y[idx]\n",
    "\n",
    "        #                 |array containing correct index in deduplicated g \n",
    "        #                 |               index in phno    \n",
    "        uniq_g_idx = self.idx_lookup_geno[\n",
    "            self.idx_original[idx], 1]\n",
    "        g_idx = self.G[uniq_g_idx, :]\n",
    "\n",
    "\n",
    "        #                 |array containing correct index in deduplicated w \n",
    "        #                 |               index in phno    \n",
    "        uniq_env_idx = self.idx_lookup_env[self.idx_original[idx], 1]\n",
    "        s_idx = self.S[uniq_env_idx, :]\n",
    "        w_idx = self.W[uniq_env_idx, :, self.window_start[idx]:self.window_end[idx]]\n",
    "\n",
    "        # send all to gpu        \n",
    "        if (self.device != 'cpu'):\n",
    "            if y_idx.device.type == 'cpu':\n",
    "                y_idx = y_idx.to(self.device) \n",
    "\n",
    "            if g_idx.device.type == 'cpu':\n",
    "                g_idx = g_idx.to(self.device) \n",
    "\n",
    "            if s_idx.device.type == 'cpu':\n",
    "                s_idx = s_idx.to(self.device)                 \n",
    "\n",
    "            if w_idx.device.type == 'cpu':\n",
    "                w_idx = w_idx.to(self.device)         \n",
    "\n",
    "        if self.transform:\n",
    "            y_idx = self.transform(y_idx)\n",
    "            g_idx = self.transform(g_idx)\n",
    "            s_idx = self.transform(s_idx)\n",
    "            w_idx = self.transform(w_idx)\n",
    "\n",
    "#         res_list = [[y_idx, g_idx, s_idx, w_idx]]\n",
    "        res_list = [y_idx, g_idx, s_idx, w_idx]\n",
    "#         return list(res_list)\n",
    "        return res_list\n",
    "    \n",
    "training_dataloader = DataLoader(\n",
    "    LGSWDataset(\n",
    "        y = torch.from_numpy(y_cs[train_idx])[:, None].to(torch.float32), #torch.from_numpy(YMat[[0, 1], ]), \n",
    "        G = torch.from_numpy(g_cs).to(torch.float32), \n",
    "        S = torch.from_numpy(s_cs).to(torch.float32),\n",
    "        W = torch.from_numpy(w_cs).to(torch.float32),\n",
    "        planting = PlantHarvest[:, 0],\n",
    "        days_before_planting = 60,\n",
    "        days_after_planting = 194,\n",
    "        idx_original = torch.from_numpy(np.array(train_idx)),#[:, None],\n",
    "        idx_lookup_geno = obs_geno_lookup,\n",
    "        idx_lookup_env = obs_env_lookup,\n",
    "        device = 'cuda'),\n",
    "    batch_size = 256,\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "    LGSWDataset(\n",
    "        y = torch.from_numpy(y_cs[test_idx])[:, None].to(torch.float32), #torch.from_numpy(YMat[[0, 1], ]), \n",
    "        G = torch.from_numpy(g_cs).to(torch.float32), \n",
    "        S = torch.from_numpy(s_cs).to(torch.float32),\n",
    "        W = torch.from_numpy(w_cs).to(torch.float32),\n",
    "        planting = PlantHarvest[:, 0],\n",
    "        days_before_planting = 60,\n",
    "        days_after_planting = 194,\n",
    "        idx_original = torch.from_numpy(np.array(test_idx)),#[:, None],\n",
    "        idx_lookup_geno = obs_geno_lookup,\n",
    "        idx_lookup_env = obs_env_lookup,\n",
    "        device = 'cuda'),\n",
    "    batch_size = 256,\n",
    "    shuffle = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56bd96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module for training subnetworks.\n",
    "class plDNNCO_subnet(pl.LightningModule):\n",
    "    def __init__(self, mod):\n",
    "        super().__init__()\n",
    "        self.mod = mod\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # train loop\n",
    "#         y_i, g_i, s_i, w_i = batch\n",
    "#         pred, out = self.mod(g_i, s_i, w_i)\n",
    "        y_i, _, _, _ = batch\n",
    "        pred, out = self.mod(batch)\n",
    "        # print(y_i.shape, pred.shape)\n",
    "        loss = F.mse_loss(pred, y_i)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            weight_list=[(name, param) for name, param in model.named_parameters() if name.split('.')[-1] == 'weight']\n",
    "            for l in weight_list:\n",
    "                self.log((\"train_mean\"+l[0]), l[1].mean())\n",
    "                self.log((\"train_std\"+l[0]), l[1].std())        \n",
    "        return(loss)\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "#         y_i, g_i, s_i, w_i = batch\n",
    "#         pred, out = self.mod(g_i, s_i, w_i)\n",
    "        y_i, _, _, _ = batch\n",
    "        pred, out = self.mod(batch)\n",
    "        # print(y_i.shape, pred.shape)\n",
    "        loss = F.mse_loss(pred, y_i)\n",
    "        self.log('val_loss', loss)        \n",
    "     \n",
    "    def configure_optimizers(self, **kwargs):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), **kwargs)\n",
    "        return optimizer    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40957d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # [[ee.shape for ee in e] for e in next(iter(training_dataloader))]\n",
    "\n",
    "# class DNNCO_G(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(DNNCO_G, self).__init__()    \n",
    "        \n",
    "#         self.g_network =nn.Sequential(\n",
    "#             Linear_block(in_size= 4881, out_size= 83, drop_pr= 0.163923177),\n",
    "#             Linear_block(in_size= 83,   out_size= 133, drop_pr= 0.230663142)\n",
    "#         ) \n",
    "#         self.g_pred =nn.Sequential(\n",
    "#             nn.Linear(133, 1)\n",
    "#         ) \n",
    "                \n",
    "#     def forward(self, tensor_list):\n",
    "#         y, g, s, w = tensor_list\n",
    "#         g_out = self.g_network(g)\n",
    "#         g_pred = self.g_pred(g_out)\n",
    "#         return g_pred, g_out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8701a03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DNNCO_G().to(device)(next(iter(training_dataloader)) )[0].shape\n",
    "\n",
    "# model = DNNCO_G().to(device)\n",
    "# LSUV_(model, data = next(iter(training_dataloader)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c54a429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DNNG = plDNNCO_subnet(model)                          # 1. Update\n",
    "# optimizer = DNNG.configure_optimizers(lr = lr, betas=(beta1, beta2)) # 2. Update\n",
    "\n",
    "# logger = TensorBoardLogger(\"tb_logs\", name=\"dnnco-g-lsuv\")                # 3. Update\n",
    "# trainer = pl.Trainer(max_epochs=max_epoch, logger=logger)\n",
    "\n",
    "# trainer.fit(model=DNNG, train_dataloaders=training_dataloader, val_dataloaders=validation_dataloader)       # 4. Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27acaf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(DNNG.mod, cache_path+'dnnco-g-lsuv'+'.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c6f5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DNNCO_S(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(DNNCO_S, self).__init__()    \n",
    "        \n",
    "#         self.s_network =nn.Sequential(\n",
    "#             Linear_block(in_size= 23, out_size= 38, drop_pr= 0.148724301),\n",
    "#             Linear_block(in_size= 38, out_size= 13, drop_pr= 0.276340999),\n",
    "#             Linear_block(in_size= 13, out_size= 45, drop_pr= 0.005434164),\n",
    "#             Linear_block(in_size= 45, out_size= 29, drop_pr= 0.173380695),\n",
    "#             Linear_block(in_size= 29, out_size= 4,  drop_pr= 0.),\n",
    "#             Linear_block(in_size= 4,  out_size= 4,  drop_pr= 0.),\n",
    "#             Linear_block(in_size= 4,  out_size= 4,  drop_pr= 0.)\n",
    "#         )  \n",
    "#         self.s_pred =nn.Sequential(\n",
    "#             nn.Linear(4, 1)\n",
    "#         ) \n",
    "        \n",
    "#     def forward(self, tensor_list):\n",
    "#         y, g, s, w = tensor_list\n",
    "#         s_out = self.s_network(s)\n",
    "#         s_pred = self.s_pred(s_out)\n",
    "#         return s_pred, s_out\n",
    "    \n",
    "# # [e.shape for e in DNNCO_S().to(device)(g_idx, s_idx, w_idx)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c8042c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subnet = 's'                                                         # 0. Update\n",
    "# lr, beta1, beta2, batch_size, max_epoch = adam_params[subnet]\n",
    "\n",
    "# model = DNNCO_S().to(device)\n",
    "# LSUV_(model, data = next(iter(training_dataloader)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6cad41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DNNS = plDNNCO_subnet(model)                          # 1. Update\n",
    "# optimizer = DNNS.configure_optimizers(lr = lr, betas=(beta1, beta2)) # 2. Update\n",
    "\n",
    "# logger = TensorBoardLogger(\"tb_logs\", name=\"dnnco-s\")                # 3. Update\n",
    "# trainer = pl.Trainer(max_epochs=max_epoch, logger=logger)\n",
    "\n",
    "# trainer.fit(model=DNNS, train_dataloaders=training_dataloader)       # 4. Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7453de6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DNNCO_W(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(DNNCO_W, self).__init__()  \n",
    "        \n",
    "#         self.w_network =nn.Sequential(\n",
    "#             Conv1D_x2_Max_block(\n",
    "#                 in_channels = 16, \n",
    "#                 out_channels = 433, \n",
    "#                 kernel_size = 3,\n",
    "#                 maxpool_size = 2,\n",
    "#                 stride = 1),\n",
    "#             Conv1D_x2_Max_block(\n",
    "#                 in_channels = 433, \n",
    "#                 out_channels = 436, \n",
    "#                 kernel_size = 3,\n",
    "#                 maxpool_size = 2,\n",
    "#                 stride = 1),\n",
    "#             Conv1D_x2_Max_block(\n",
    "#                 in_channels = 436, \n",
    "#                 out_channels = 52, \n",
    "#                 kernel_size = 3,\n",
    "#                 maxpool_size = 2,\n",
    "#                 stride = 1),\n",
    "#             Conv1D_x2_Max_block(\n",
    "#                 in_channels = 52, \n",
    "#                 out_channels = 163, \n",
    "#                 kernel_size = 3,\n",
    "#                 maxpool_size = 2,\n",
    "#                 stride = 1),\n",
    "#             Conv1D_x2_Max_block(\n",
    "#                 in_channels = 163, \n",
    "#                 out_channels = 400, \n",
    "#                 kernel_size = 3,\n",
    "#                 maxpool_size = 2,\n",
    "#                 stride = 1),\n",
    "#             Conv1D_x2_Max_block(\n",
    "#                 in_channels = 400, \n",
    "#                 out_channels = 294, \n",
    "#                 kernel_size = 3,\n",
    "#                 maxpool_size = 2,\n",
    "#                 stride = 1)\n",
    "#         )\n",
    "        \n",
    "#         self.w_flatten =nn.Flatten()\n",
    "\n",
    "#         self.w_pred =nn.Sequential(\n",
    "#             nn.Linear(65856, 1)\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, tensor_list):\n",
    "#         y, g, s, w = tensor_list\n",
    "#         w_out = self.w_network(w)\n",
    "#         w_out = self.w_flatten(w_out)\n",
    "#         w_pred = self.w_pred(w_out)\n",
    "#         return w_pred, w_out\n",
    "    \n",
    "# # [e.shape for e in DNNCO_W().to(device)(g_idx, s_idx, w_idx)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039e83a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying orthogonal init (zero init if dim < 2) to params in 13 module(s).\n",
      "Applying LSUV to 13 module(s) (up to 10 iters per module):\n",
      "Module  0 after  2 itr(s) | Mean: -0.001 | Std: 1.000 | <class 'torch.nn.modules.conv.Conv1d'>\n",
      "Module  1 after  1 itr(s) | Mean: -0.001 | Std: 1.009 | <class 'torch.nn.modules.conv.Conv1d'>\n",
      "Module  2 after  1 itr(s) | Mean: -0.010 | Std: 1.007 | <class 'torch.nn.modules.conv.Conv1d'>\n",
      "Module  3 after  1 itr(s) | Mean: -0.016 | Std: 1.001 | <class 'torch.nn.modules.conv.Conv1d'>\n",
      "Module  4 after  1 itr(s) | Mean:  0.056 | Std: 1.085 | <class 'torch.nn.modules.conv.Conv1d'>\n",
      "Module  5 after  1 itr(s) | Mean:  0.017 | Std: 1.075 | <class 'torch.nn.modules.conv.Conv1d'>\n",
      "Module  6 after  1 itr(s) | Mean: -0.006 | Std: 0.978 | <class 'torch.nn.modules.conv.Conv1d'>\n",
      "Module  7 after  1 itr(s) | Mean:  0.005 | Std: 0.954 | <class 'torch.nn.modules.conv.Conv1d'>\n",
      "Module  8 after  1 itr(s) | Mean:  0.012 | Std: 0.999 | <class 'torch.nn.modules.conv.Conv1d'>\n",
      "Module  9 after  1 itr(s) | Mean: -0.011 | Std: 1.007 | <class 'torch.nn.modules.conv.Conv1d'>\n",
      "Module 10 after  1 itr(s) | Mean:  0.007 | Std: 0.995 | <class 'torch.nn.modules.conv.Conv1d'>\n",
      "Module 11 after  1 itr(s) | Mean: -0.007 | Std: 0.982 | <class 'torch.nn.modules.conv.Conv1d'>\n",
      "Module 12 after  1 itr(s) | Mean:  0.173 | Std: 0.962 | <class 'torch.nn.modules.linear.Linear'>\n"
     ]
    }
   ],
   "source": [
    "subnet = 'w'                                                         # 0. Update\n",
    "lr, beta1, beta2, batch_size, max_epoch = adam_params[subnet]\n",
    "\n",
    "model = DNNCO_W().to(device)\n",
    "LSUV_(model, data = next(iter(training_dataloader)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c32283",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kickd/miniconda3/envs/fastai/lib/python3.11/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/kickd/miniconda3/envs/fastai/lib/python3.11/si ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kickd/miniconda3/envs/fastai/lib/python3.11/site-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name | Type    | Params\n",
      "---------------------------------\n",
      "0 | mod  | DNNCO_W | 3.3 M \n",
      "---------------------------------\n",
      "3.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.3 M     Total params\n",
      "13.047    Total estimated model params size (MB)\n",
      "/home/kickd/miniconda3/envs/fastai/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52fca8b0c7154007a91120ed228d3967",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DNNW = plDNNCO_subnet(model)                          # 1. Update\n",
    "optimizer = DNNW.configure_optimizers(lr = lr, betas=(beta1, beta2)) # 2. Update\n",
    "\n",
    "logger = TensorBoardLogger(\"tb_logs\", name=\"dnnco-w\")                # 3. Update\n",
    "trainer = pl.Trainer(max_epochs=max_epoch, logger=logger)\n",
    "\n",
    "trainer.fit(model=DNNW, train_dataloaders=training_dataloader)       # 4. Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54518184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(DNNG.mod, cache_path+'dnnco-w'+'.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fef206",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b2c6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.load('../nbs_artifacts/02.10_g2fc_benchmark_DNNCO_G/dnnco-g-lsuv.pt')\n",
    "# torch.load('../nbs_artifacts/02.11')\n",
    "# torch.load('../nbs_artifacts/02.12')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08879dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNNCO_X(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DNNCO_X, self).__init__()  \n",
    "        \n",
    "        self.x_network =nn.Sequential(\n",
    "            Linear_block(in_size= 65993, out_size= 152, drop_pr= 0.18658661),\n",
    "            Linear_block(in_size= 152,   out_size= 207, drop_pr= 0.289893588),\n",
    "            Linear_block(in_size= 207,   out_size= 206, drop_pr= 0.004841293),\n",
    "            Linear_block(in_size= 206,   out_size= 188, drop_pr= 0.198121953),\n",
    "            Linear_block(in_size= 188,   out_size= 44,  drop_pr= 0.243027717),\n",
    "            Linear_block(in_size= 44,    out_size= 1,   drop_pr= 0.0)\n",
    "        )  \n",
    "        \n",
    "    def forward(self, g, s, w):        \n",
    "        x_pred = self.x_network(torch.concat([g, s, w], axis = 1))\n",
    "        return x_pred, None\n",
    "    \n",
    "# DNNCO_X().to(device)(\n",
    "#     DNNCO_G().to(device)(g_idx, s_idx, w_idx)[1],\n",
    "#     DNNCO_S().to(device)(g_idx, s_idx, w_idx)[1],\n",
    "#     DNNCO_W().to(device)(g_idx, s_idx, w_idx)[1])[0].shape   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1889d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module for training Interaction networks. \n",
    "# This version uses the subnetworks so it would be good for predicting new data but this could be\n",
    "# reformulated to instead use predictions from the subnetworks which would be more preformant.\n",
    "class plDNNCO_X(pl.LightningModule):\n",
    "    def __init__(self, mod, Gmod, Smod, Wmod):\n",
    "        super().__init__()\n",
    "        self.mod = mod\n",
    "        self.Gmod = Gmod\n",
    "        self.Smod = Smod\n",
    "        self.Wmod = Wmod\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # train loop\n",
    "        y_i, g_i, s_i, w_i = batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            _, g_out = self.Gmod(g_i, s_i, w_i)            \n",
    "            _, s_out = self.Smod(g_i, s_i, w_i)            \n",
    "            _, w_out = self.Wmod(g_i, s_i, w_i)            \n",
    "        \n",
    "        pred, out = self.mod(g_out, s_out, w_out)\n",
    "        # print(y_i.shape, pred.shape)\n",
    "        loss = F.mse_loss(pred, y_i)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return(loss)\n",
    "    \n",
    "    def configure_optimizers(self, **kwargs):\n",
    "        optimizer =  torch.optim.Adam(self.parameters(), **kwargs)\n",
    "        return optimizer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ea3f23",
   "metadata": {},
   "outputs": [],
   "source": [
    ", val_dataloaders=validation_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5715848",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2bc1d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f0125b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8eae441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now freeze layers to be extra sure they are not updated\n",
    "for param in DNNG.mod.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "for param in DNNS.mod.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "for param in DNNW.mod.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad85f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DNNS = plDNNCO_subnet(DNNCO_S().to(device))\n",
    "# optimizer = DNNS.configure_optimizers()\n",
    "# logger = TensorBoardLogger(\"tb_logs\", name=\"dnnco-sonly\")\n",
    "# trainer = pl.Trainer(#limit_train_batches=100, \n",
    "#                      max_epochs=10, logger=logger)\n",
    "# trainer.fit(model=DNNS, train_dataloaders=training_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25990261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DNNW = plDNNCO_subnet(DNNCO_W().to(device))\n",
    "# optimizer = DNNW.configure_optimizers()\n",
    "# logger = TensorBoardLogger(\"tb_logs\", name=\"dnnco-wonly\")\n",
    "# trainer = pl.Trainer(#limit_train_batches=100, \n",
    "#                      max_epochs=10, logger=logger)\n",
    "# trainer.fit(model=DNNW, train_dataloaders=training_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524eef4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "subnet = 'co'                                                        # 0. Update\n",
    "lr, beta1, beta2, batch_size, max_epoch = adam_params[subnet]\n",
    "max_epoch = 2 #FIXME\n",
    "training_dataloader = DataLoader(\n",
    "    GSWDataset(\n",
    "        y = torch.from_numpy(y_cs[train_idx])[:, None].to(torch.float32), #torch.from_numpy(YMat[[0, 1], ]), \n",
    "        G = torch.from_numpy(g_cs).to(torch.float32), \n",
    "        S = torch.from_numpy(s_cs).to(torch.float32),\n",
    "        W = torch.from_numpy(w_cs).to(torch.float32),\n",
    "        planting = PlantHarvest[:, 0],\n",
    "        days_before_planting = 60,\n",
    "        days_after_planting = 194,\n",
    "        idx_original = torch.from_numpy(np.array(train_idx)),#[:, None],\n",
    "        idx_lookup_geno = obs_geno_lookup,\n",
    "        idx_lookup_env = obs_env_lookup,\n",
    "        device = 'cuda'),\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "# DNNW = plDNNCO_subnet(DNNCO_W().to(device))                          # 1. Update\n",
    "# optimizer = DNNW.configure_optimizers(lr = lr, betas=(beta1, beta2)) # 2. Update\n",
    "\n",
    "# logger = TensorBoardLogger(\"tb_logs\", name=\"dnnco-w\")                # 3. Update\n",
    "# trainer = pl.Trainer(max_epochs=max_epoch, logger=logger)\n",
    "\n",
    "# trainer.fit(model=DNNW, train_dataloaders=training_dataloader)       # 4. Update\n",
    "\n",
    "\n",
    "DNNX = plDNNCO_X(DNNCO_X().to(device), Gmod = DNNG.mod, Smod = DNNS.mod, Wmod = DNNW.mod)\n",
    "optimizer = DNNX.configure_optimizers(lr = lr, betas=(beta1, beta2))\n",
    "\n",
    "logger = TensorBoardLogger(\"tb_logs\", name=\"dnnco-x\")\n",
    "trainer = pl.Trainer(max_epochs=max_epoch, logger=logger)\n",
    "\n",
    "trainer.fit(model=DNNX, train_dataloaders=training_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f9d02e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b7915a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DNNX = plDNNCO_X(DNNCO_X().to(device), Gmod = DNNG.mod, Smod = DNNS.mod, Wmod = DNNW.mod)\n",
    "optimizer = DNNS.configure_optimizers()\n",
    "logger = TensorBoardLogger(\"tb_logs\", name=\"dnnco-x2\")\n",
    "trainer = pl.Trainer(#limit_train_batches=100, \n",
    "                     max_epochs=10, logger=logger)\n",
    "trainer.fit(model=DNNX, train_dataloaders=training_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f08c943",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a2aaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lightning.pytorch import Trainer\n",
    "\n",
    "# logger = TensorBoardLogger(\"tb_logs\", name=\"dnnco-gonly\")\n",
    "# # trainer = Trainer(logger=logger)\n",
    "# # train the model (hint: here are some helpful Trainer arguments for rapid idea iteration)\n",
    "# trainer = pl.Trainer(#limit_train_batches=100, \n",
    "#                      max_epochs=10, logger=logger)\n",
    "# trainer.fit(model=DNNG, train_dataloaders=training_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
