{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a7219b3",
   "metadata": {},
   "source": [
    "# Deep Learning Convenience Functions\n",
    "\n",
    "> This notebook contains convenience functions to aid in modeling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32788a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp dlfn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053d375a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8510e582",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def calc_cs(x # numeric array\n",
    "           ): \n",
    "    \"Calculate nan mean and nan std of an array. Returned as list\"\n",
    "    import numpy as np\n",
    "    return [np.nanmean(x, axis = 0), np.nanstd(x, axis = 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b0a2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def apply_cs(xs, \n",
    "             cs_dict_entry # list of length 2 containing mean and s\n",
    "            ): return ((xs - cs_dict_entry[0]) / cs_dict_entry[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9321aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def reverse_cs(xs, cs_dict_entry): return (cs_dict_entry[1] * xs) + cs_dict_entry[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a47fbcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b1e463",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer, silent = False):\n",
    "#     import numpy as np\n",
    "#     import pandas as pd\n",
    "    import torch\n",
    "    from torch.utils.data import Dataset\n",
    "    from torch.utils.data import DataLoader\n",
    "#     from torch import nn\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (xs_i, y_i) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(xs_i)\n",
    "        loss = loss_fn(pred, y_i) # <----------------------------------------\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(y_i) # <----------------\n",
    "            if not silent:\n",
    "                print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22314751",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def train_error(dataloader, model, loss_fn, silent = False):\n",
    "#     import numpy as np\n",
    "#     import pandas as pd\n",
    "    import torch\n",
    "    from torch.utils.data import Dataset\n",
    "    from torch.utils.data import DataLoader\n",
    "#     from torch import nn\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    train_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xs_i, y_i in dataloader:\n",
    "            pred = model(xs_i)\n",
    "            train_loss += loss_fn(pred, y_i).item() # <----------------------\n",
    "            \n",
    "    train_loss /= num_batches\n",
    "    return(train_loss) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51151878",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn, silent = False):\n",
    "#     import numpy as np\n",
    "#     import pandas as pd\n",
    "    import torch\n",
    "    from torch.utils.data import Dataset\n",
    "    from torch.utils.data import DataLoader\n",
    "#     from torch import nn\n",
    "\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xs_i, y_i in dataloader:\n",
    "            pred = model(xs_i)\n",
    "            test_loss += loss_fn(pred, y_i).item() # <-----------------------\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    if not silent:\n",
    "        print(f\"Test Error: Avg loss: {test_loss:>8f}\")\n",
    "    return(test_loss) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543a7603",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def yhat_loop(dataloader, model):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import torch\n",
    "#     from torch.utils.data import Dataset\n",
    "#     from torch.utils.data import DataLoader\n",
    "#     from torch import nn\n",
    "\n",
    "    \n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    \n",
    "    y_true = np.array([])\n",
    "    y_pred = np.array([])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for xs_i, y_i in dataloader:\n",
    "            yhat_i = model(xs_i)\n",
    "            y_pred = np.append(y_pred, np.array(yhat_i.cpu()))\n",
    "            y_true = np.append(y_true, np.array(y_i.cpu()))\n",
    "    \n",
    "    out = np.concatenate([y_true[:, None], y_pred[:, None]], axis = 1) \n",
    "    out = pd.DataFrame(out, columns = ['y_true', 'y_pred'])\n",
    "    return(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c71c174",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def train_nn(\n",
    "    cache_path,\n",
    "    training_dataloader,\n",
    "    testing_dataloader,\n",
    "    model,\n",
    "    learning_rate = 1e-3,\n",
    "    batch_size = 64,\n",
    "    epochs = 500,\n",
    "    model_prefix = 'model'\n",
    "):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import torch\n",
    "#     from torch.utils.data import Dataset\n",
    "#     from torch.utils.data import DataLoader\n",
    "    from torch import nn\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    # Initialize the loss function\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    loss_df = pd.DataFrame([i for i in range(epochs)], columns = ['Epoch'])\n",
    "    loss_df['TrainMSE'] = np.nan\n",
    "    loss_df['TestMSE']  = np.nan\n",
    "\n",
    "    for t in tqdm(range(epochs)):        \n",
    "        # print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train_loop(training_dataloader, model, loss_fn, optimizer, silent = True)\n",
    "\n",
    "        loss_df.loc[loss_df.index == t, 'TrainMSE'\n",
    "                   ] = train_error(training_dataloader, model, loss_fn, silent = True)\n",
    "        \n",
    "        loss_df.loc[loss_df.index == t, 'TestMSE'\n",
    "                   ] = test_loop(testing_dataloader, model, loss_fn, silent = True)\n",
    "        \n",
    "        if (t+1)%5 == 0: # Cache in case training is interupted. \n",
    "            # print(loss_df.loc[loss_df.index == t, ['TrainMSE', 'TestMSE']])\n",
    "            torch.save(model.state_dict(), \n",
    "                       cache_path+'/'+model_prefix+'_'+str(t)+'_'+str(epochs)+'.pt') # convention is to use .pt or .pth\n",
    "        \n",
    "    return([model, loss_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c13dc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363c0a24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4b170a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def estimate_iterations(sec_per_it = 161):\n",
    "    import math\n",
    "    import pandas as pd\n",
    "    hours = [1, 2, 4, 8, 12, 24]\n",
    "    res = pd.DataFrame(zip(hours, \n",
    "    [math.floor(\n",
    "        ((i)*(60*60))/sec_per_it\n",
    "    ) for i in hours]), columns = ['Hours', 'Iterations'])\n",
    "    return(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707a9168",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e62927b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "class ACGTDataset(Dataset): # for any G containing matix with many (phno) to one (geno)\n",
    "    def __init__(self, \n",
    "                 y, \n",
    "                 G, # not on gpu\n",
    "                 idx_original,\n",
    "                 idx_lookup,\n",
    "                 transform = None, target_transform = None,\n",
    "                 use_gpu_num = 0,\n",
    "                 device = 'cuda',\n",
    "                 **kwargs \n",
    "                ):\n",
    "\n",
    "        self.device = device\n",
    "        self.y = y \n",
    "        self.G = G\n",
    "        self.idx_original = idx_original\n",
    "        self.idx_lookup = idx_lookup\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform    \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        y_idx =self.y[idx]\n",
    "            \n",
    "        #                 |array containing correct index in deduplicated g \n",
    "        #                 |               index in phno    \n",
    "        uniq_g_idx = self.idx_lookup[self.idx_original[idx], 1]\n",
    "        g_idx = self.G[uniq_g_idx, :, :]\n",
    "        \n",
    "        # send all to gpu        \n",
    "        if (self.device != 'cpu'):\n",
    "            if y_idx.device.type == 'cpu':\n",
    "                y_idx = y_idx.to(self.device) \n",
    "                \n",
    "            if g_idx.device.type == 'cpu':\n",
    "                g_idx = g_idx.to(self.device) \n",
    "        \n",
    "        \n",
    "        if self.transform:\n",
    "            g_idx = self.transform(g_idx)\n",
    "            \n",
    "        if self.target_transform:\n",
    "            y_idx = self.transform(y_idx)\n",
    "        return g_idx, y_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1711e096",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c81883",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eeb862b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| export\n",
    "\n",
    "# def train_loop3(dataloader, model, loss_fn, optimizer, silent = False):\n",
    "#     \"This is a version of train_loop which concatenates three ys.\"\n",
    "#     import torch\n",
    "#     from torch.utils.data import Dataset\n",
    "#     from torch.utils.data import DataLoader\n",
    "#     size = len(dataloader.dataset)\n",
    "#     for batch, (xs_i, y1_i, y2_i, y3_i) in enumerate(dataloader):\n",
    "#         # Compute prediction and loss\n",
    "#         pred = model(xs_i)\n",
    "#         loss = loss_fn(pred, torch.concat([y1_i, y2_i, y3_i], axis = 1)) # <----------------------------------------\n",
    "\n",
    "#         # Backpropagation\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         if batch % 100 == 0:\n",
    "#             loss, current = loss.item(), batch * len(y1_i) # <----------------\n",
    "#             if not silent:\n",
    "#                 print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef067bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0904feee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| export\n",
    "\n",
    "# def train_error3(dataloader, model, loss_fn, silent = False):\n",
    "#     import torch\n",
    "#     from torch.utils.data import Dataset\n",
    "#     from torch.utils.data import DataLoader\n",
    "#     size = len(dataloader.dataset)\n",
    "#     num_batches = len(dataloader)\n",
    "#     train_loss = 0\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for xs_i, y1_i, y2_i, y3_i in dataloader:\n",
    "#             pred = model(xs_i)\n",
    "#             train_loss += loss_fn(pred, torch.concat([y1_i, y2_i, y3_i], axis = 1)).item() # <----------------------\n",
    "            \n",
    "#     train_loss /= num_batches\n",
    "#     return(train_loss) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9f0fd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d9cf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| export\n",
    "\n",
    "# def test_loop3(dataloader, model, loss_fn, silent = False):\n",
    "#     import torch\n",
    "#     from torch.utils.data import Dataset\n",
    "#     from torch.utils.data import DataLoader\n",
    "\n",
    "#     size = len(dataloader.dataset)\n",
    "#     num_batches = len(dataloader)\n",
    "#     test_loss = 0\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for xs_i, y1_i, y2_i, y3_i in dataloader:\n",
    "#             pred = model(xs_i)\n",
    "#             test_loss += loss_fn(pred, torch.concat([y1_i, y2_i, y3_i], axis = 1)).item() # <-----------------------\n",
    "\n",
    "#     test_loss /= num_batches\n",
    "#     if not silent:\n",
    "#         print(f\"Test Error: Avg loss: {test_loss:>8f}\")\n",
    "#     return(test_loss) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c78817",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6745d1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962ace40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| export\n",
    "\n",
    "# def yhat_loop3(dataloader, model):\n",
    "#     \"Version of yhat_loop that returns 3 ys\"\n",
    "#     import numpy as np\n",
    "#     import pandas as pd\n",
    "#     import torch\n",
    "#     size = len(dataloader.dataset)\n",
    "#     num_batches = len(dataloader)\n",
    "    \n",
    "#     first_loop = True\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for xs_i, y1_i, y2_i, y3_i in dataloader:\n",
    "#             yhat_i = model(xs_i)\n",
    "#             y_i = torch.concat([y1_i, y2_i, y3_i], axis = 1) # <-----------------------\n",
    "\n",
    "#             if first_loop:\n",
    "#                 y_pred = np.array(yhat_i.cpu())\n",
    "#                 y_true = np.array(y_i.cpu())\n",
    "#                 first_loop = False\n",
    "#             else:            \n",
    "#                 y_pred = np.concatenate([y_pred, np.array(yhat_i.cpu())])\n",
    "#                 y_true = np.concatenate([y_true, np.array(y_i.cpu())])\n",
    "                \n",
    "#     out = np.concatenate([y_true[:, :], y_pred[:, :]], axis = 1) \n",
    "#     out = pd.DataFrame(out, columns = ['y1_true', 'y2_true', 'y3_true', 'y1_pred', 'y2_pred', 'y3_pred'])\n",
    "#     return(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56e9eaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8bce2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680915b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b659c93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9170be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
