{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a7219b3",
   "metadata": {},
   "source": [
    "# Deep Learning Convenience Functions\n",
    "\n",
    "> This notebook contains convenience functions to aid in modeling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32788a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp dlfn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053d375a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8510e582",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def calc_cs(x # numeric array\n",
    "           ): \n",
    "    \"Calculate nan mean and nan std of an array. Returned as list\"\n",
    "    import numpy as np\n",
    "    return [np.nanmean(x, axis = 0), np.nanstd(x, axis = 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b0a2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def apply_cs(xs, \n",
    "             cs_dict_entry # list of length 2 containing mean and s\n",
    "            ): return ((xs - cs_dict_entry[0]) / cs_dict_entry[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9321aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def reverse_cs(xs, cs_dict_entry): return (cs_dict_entry[1] * xs) + cs_dict_entry[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c133332b",
   "metadata": {},
   "source": [
    "## Train/Validate/Test Split info\n",
    "\n",
    "Stored as json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe75151f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def read_split_info(\n",
    "    load_from = '../nbs_artifacts/01.06_g2fc_cluster_genotypes/',\n",
    "    json_prefix = '2023:9:5:12:8:26'):\n",
    "    \"\"\n",
    "    import os, re\n",
    "    from EnvDL.core import read_json\n",
    "    \n",
    "    jsons = [e for e in os.listdir(load_from) if re.match('^'+json_prefix+'.+\\.json$', e)]\n",
    "    vals = [e for e in jsons if re.match('.+val\\d+\\.json$', e)]\n",
    "    vals.sort()\n",
    "    out = {}\n",
    "    out['test'] = [read_json(json_path = load_from+json_prefix+'-test.json')]\n",
    "    out['test_file'] = [json_prefix+'-test.json']\n",
    "    out['validate'] = [read_json(json_path = load_from+val) for val in vals]\n",
    "    out['validate_files'] = [val for val in vals]\n",
    "    return(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051b1a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def find_idxs_split_dict(\n",
    "    obs_df, # assumes presence of Year, Female, Male\n",
    "    split_dict # from read_split_info() output. Should be a test of validate dict.\n",
    "):\n",
    "    import pandas as pd\n",
    "    temp = obs_df\n",
    "    test_mask = ((temp.Year.isin(split_dict['test_years'])) & \n",
    "                 ((temp.Female.isin(split_dict['test_parents'])) |\n",
    "                  (temp.Male.isin(split_dict['test_parents']))))\n",
    "    temp['Split'] = ''\n",
    "    temp.loc[test_mask, 'Split'] = 'Test'\n",
    "\n",
    "    train_mask = (~(temp.Year.isin(split_dict['test_years'])) & \n",
    "                 (~((temp.Female.isin(split_dict['test_parents'])) |\n",
    "                  (temp.Male.isin(split_dict['test_parents'])))))\n",
    "    temp.loc[train_mask, 'Split'] = 'Train'\n",
    "\n",
    "    temp_test  = (temp.Split == 'Test') # should be the same as with the mask above\n",
    "    temp_train = (temp.Split == 'Train') # should be the same as with the mask above\n",
    "\n",
    "    # Confirm that there's no overlap in parents or years\n",
    "    temp_test_parents  = set(temp.loc[temp_test, 'Female']+temp.loc[temp_test, 'Male'])\n",
    "    temp_train_parents = set(temp.loc[temp_train, 'Female']+temp.loc[temp_train, 'Male'])\n",
    "\n",
    "    temp_test_years  = set(temp.loc[temp_test, 'Year'])\n",
    "    temp_train_years = set(temp.loc[temp_train, 'Year'])\n",
    "\n",
    "    assert [] == [e for e in temp_test_parents if e in temp_train_parents]\n",
    "    assert [] == [e for e in temp_train_parents if e in temp_test_parents]\n",
    "    assert [] == [e for e in temp_test_years if e in temp_train_years]\n",
    "    assert [] == [e for e in temp_train_years if e in temp_test_years]\n",
    "\n",
    "    return({\n",
    "        'test_idx': temp.loc[test_mask, ].index, \n",
    "        'train_idx': temp.loc[train_mask, ].index} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549c2b27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f5b3ef5",
   "metadata": {},
   "source": [
    "## Training (general)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b1e463",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer, silent = False):\n",
    "#     import numpy as np\n",
    "#     import pandas as pd\n",
    "    import torch\n",
    "    from torch.utils.data import Dataset\n",
    "    from torch.utils.data import DataLoader\n",
    "#     from torch import nn\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (xs_i, y_i) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(xs_i)\n",
    "        loss = loss_fn(pred, y_i) # <----------------------------------------\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(y_i) # <----------------\n",
    "            if not silent:\n",
    "                print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22314751",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def train_error(dataloader, model, loss_fn, silent = False):\n",
    "#     import numpy as np\n",
    "#     import pandas as pd\n",
    "    import torch\n",
    "    from torch.utils.data import Dataset\n",
    "    from torch.utils.data import DataLoader\n",
    "#     from torch import nn\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    train_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xs_i, y_i in dataloader:\n",
    "            pred = model(xs_i)\n",
    "            train_loss += loss_fn(pred, y_i).item() # <----------------------\n",
    "            \n",
    "    train_loss /= num_batches\n",
    "    return(train_loss) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51151878",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn, silent = False):\n",
    "#     import numpy as np\n",
    "#     import pandas as pd\n",
    "    import torch\n",
    "    from torch.utils.data import Dataset\n",
    "    from torch.utils.data import DataLoader\n",
    "#     from torch import nn\n",
    "\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xs_i, y_i in dataloader:\n",
    "            pred = model(xs_i)\n",
    "            test_loss += loss_fn(pred, y_i).item() # <-----------------------\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    if not silent:\n",
    "        print(f\"Test Error: Avg loss: {test_loss:>8f}\")\n",
    "    return(test_loss) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543a7603",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def yhat_loop(dataloader, model):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import torch\n",
    "#     from torch.utils.data import Dataset\n",
    "#     from torch.utils.data import DataLoader\n",
    "#     from torch import nn\n",
    "\n",
    "    \n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    \n",
    "    y_true = np.array([])\n",
    "    y_pred = np.array([])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for xs_i, y_i in dataloader:\n",
    "            yhat_i = model(xs_i)\n",
    "            y_pred = np.append(y_pred, np.array(yhat_i.cpu()))\n",
    "            y_true = np.append(y_true, np.array(y_i.cpu()))\n",
    "    \n",
    "    out = np.concatenate([y_true[:, None], y_pred[:, None]], axis = 1) \n",
    "    out = pd.DataFrame(out, columns = ['y_true', 'y_pred'])\n",
    "    return(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c71c174",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def train_nn(\n",
    "    cache_path,\n",
    "    training_dataloader,\n",
    "    testing_dataloader,\n",
    "    model,\n",
    "    learning_rate = 1e-3,\n",
    "    batch_size = 64,\n",
    "    epochs = 500,\n",
    "    model_prefix = 'model'\n",
    "):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import torch\n",
    "#     from torch.utils.data import Dataset\n",
    "#     from torch.utils.data import DataLoader\n",
    "    from torch import nn\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    # Initialize the loss function\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    loss_df = pd.DataFrame([i for i in range(epochs)], columns = ['Epoch'])\n",
    "    loss_df['TrainMSE'] = np.nan\n",
    "    loss_df['TestMSE']  = np.nan\n",
    "\n",
    "    for t in tqdm(range(epochs)):        \n",
    "        # print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train_loop(training_dataloader, model, loss_fn, optimizer, silent = True)\n",
    "\n",
    "        loss_df.loc[loss_df.index == t, 'TrainMSE'\n",
    "                   ] = train_error(training_dataloader, model, loss_fn, silent = True)\n",
    "        \n",
    "        loss_df.loc[loss_df.index == t, 'TestMSE'\n",
    "                   ] = test_loop(testing_dataloader, model, loss_fn, silent = True)\n",
    "        \n",
    "        if (t+1)%5 == 0: # Cache in case training is interupted. \n",
    "            # print(loss_df.loc[loss_df.index == t, ['TrainMSE', 'TestMSE']])\n",
    "            torch.save(model.state_dict(), \n",
    "                       cache_path+'/'+model_prefix+'_'+str(t)+'_'+str(epochs)+'.pt') # convention is to use .pt or .pth\n",
    "        \n",
    "    return([model, loss_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c13dc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363c0a24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4b170a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def estimate_iterations(sec_per_it = 161):\n",
    "    import math\n",
    "    import pandas as pd\n",
    "    hours = [1, 2, 4, 8, 12, 24]\n",
    "    res = pd.DataFrame(zip(hours, \n",
    "    [math.floor(\n",
    "        ((i)*(60*60))/sec_per_it\n",
    "    ) for i in hours]), columns = ['Hours', 'Iterations'])\n",
    "    return(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707a9168",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e62927b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "class ACGTDataset(Dataset): # for any G containing matix with many (phno) to one (geno)\n",
    "    def __init__(self, \n",
    "                 y, \n",
    "                 G, # not on gpu\n",
    "                 idx_original,\n",
    "                 idx_lookup,\n",
    "                 transform = None, target_transform = None,\n",
    "                 use_gpu_num = 0,\n",
    "                 device = 'cuda',\n",
    "                 **kwargs \n",
    "                ):\n",
    "\n",
    "        self.device = device\n",
    "        self.y = y \n",
    "        self.G = G\n",
    "        self.idx_original = idx_original\n",
    "        self.idx_lookup = idx_lookup\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform    \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        y_idx =self.y[idx]\n",
    "            \n",
    "        #                 |array containing correct index in deduplicated g \n",
    "        #                 |               index in phno    \n",
    "        uniq_g_idx = self.idx_lookup[self.idx_original[idx], 1]\n",
    "        g_idx = self.G[uniq_g_idx, :, :]\n",
    "        \n",
    "        # send all to gpu        \n",
    "        if (self.device != 'cpu'):\n",
    "            if y_idx.device.type == 'cpu':\n",
    "                y_idx = y_idx.to(self.device) \n",
    "                \n",
    "            if g_idx.device.type == 'cpu':\n",
    "                g_idx = g_idx.to(self.device) \n",
    "        \n",
    "        \n",
    "        if self.transform:\n",
    "            g_idx = self.transform(g_idx)\n",
    "            \n",
    "        if self.target_transform:\n",
    "            y_idx = self.transform(y_idx)\n",
    "        return g_idx, y_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbfd02f",
   "metadata": {},
   "source": [
    "## Functions for Visible Neural Nets (y first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404764b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def train_loop_yx(dataloader, model, loss_fn, optimizer, silent = False):\n",
    "    import torch\n",
    "    from torch.utils.data import Dataset\n",
    "    from torch.utils.data import DataLoader\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (y_i, xs_i) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(xs_i)\n",
    "        \n",
    "        # ensure both are on cuda\n",
    "        if pred.device.type == 'cpu':\n",
    "            pred = pred.to('cuda')\n",
    "        if y_i.device.type == 'cpu':\n",
    "            y_i = y_i.to('cuda')\n",
    "        \n",
    "        loss = loss_fn(pred, y_i)\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(y_i) \n",
    "            if not silent:\n",
    "                print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebfd2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def train_error_yx(dataloader, model, loss_fn, silent = False):\n",
    "    import torch\n",
    "    from torch.utils.data import Dataset\n",
    "    from torch.utils.data import DataLoader\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    train_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for y_i, xs_i in dataloader:\n",
    "            pred = model(xs_i)\n",
    "            \n",
    "            # ensure both are on cuda\n",
    "            if pred.device.type == 'cpu':\n",
    "                pred = pred.to('cuda')\n",
    "            if y_i.device.type == 'cpu':\n",
    "                y_i = y_i.to('cuda')\n",
    "            \n",
    "            train_loss += loss_fn(pred, y_i).item()\n",
    "            \n",
    "    train_loss /= num_batches\n",
    "    return(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857d493b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def test_loop_yx(dataloader, model, loss_fn, silent = False):\n",
    "    import torch\n",
    "    from torch.utils.data import Dataset\n",
    "    from torch.utils.data import DataLoader\n",
    "    \n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for y_i, xs_i in dataloader:\n",
    "            pred = model(xs_i)\n",
    "            \n",
    "            # ensure both are on cuda\n",
    "            if pred.device.type == 'cpu':\n",
    "                pred = pred.to('cuda')\n",
    "            if y_i.device.type == 'cpu':\n",
    "                y_i = y_i.to('cuda')\n",
    "                \n",
    "            test_loss += loss_fn(pred, y_i).item() \n",
    "\n",
    "    test_loss /= num_batches\n",
    "    if not silent:\n",
    "        print(f\"Test Error: Avg loss: {test_loss:>8f}\")\n",
    "    return(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79c78d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def train_nn_yx(\n",
    "    cache_path,\n",
    "    training_dataloader,\n",
    "    testing_dataloader,\n",
    "    model,\n",
    "    batch_size = 64,\n",
    "    epochs = 500,\n",
    "    model_prefix = 'model',\n",
    "    save_model = False,\n",
    "    **kwargs # can include 'silent' for train loop or 'save_on' for saving frequency\n",
    "):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import torch\n",
    "    from torch import nn\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    if 'optimizer' not in kwargs:\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=kwargs['learning_rate'])\n",
    "    else:\n",
    "        optimizer = kwargs['optimizer']\n",
    "        \n",
    "    if 'save_on' in kwargs:\n",
    "        save_on = kwargs['save_on']\n",
    "    else:\n",
    "        save_on = 5       \n",
    "    \n",
    "    # Initialize the loss function\n",
    "    loss_fn = nn.MSELoss()     \n",
    "\n",
    "    loss_df = pd.DataFrame([i for i in range(epochs)], columns = ['Epoch'])\n",
    "    loss_df['TrainMSE'] = np.nan\n",
    "    loss_df['TestMSE']  = np.nan\n",
    "\n",
    "    for t in tqdm(range(epochs)):        \n",
    "        if 'silent' in kwargs:\n",
    "            train_loop_yx(training_dataloader, model, loss_fn, optimizer, silent = kwargs['silent'])\n",
    "        else:\n",
    "            train_loop_yx(training_dataloader, model, loss_fn, optimizer, silent = True)\n",
    "\n",
    "        loss_df.loc[loss_df.index == t, 'TrainMSE'\n",
    "                   ] = train_error_yx(training_dataloader, model, loss_fn, silent = True)\n",
    "        \n",
    "        loss_df.loc[loss_df.index == t, 'TestMSE'\n",
    "                   ] = test_loop_yx(testing_dataloader, model, loss_fn, silent = True)\n",
    "        \n",
    "        if (t+1)%save_on == 0: # Cache in case training is interupted. \n",
    "            if save_model:\n",
    "                torch.save(model.state_dict(), \n",
    "                           cache_path+'/'+model_prefix+'_'+str(t)+'_'+str(epochs)+'.pt') # convention is to use .pt or .pth\n",
    "        \n",
    "    return([model, loss_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccbe2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def yhat_loop_yx(dataloader, model):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import torch\n",
    "\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    \n",
    "    y_true = np.array([])\n",
    "    y_pred = np.array([])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for y_i, xs_i in dataloader:\n",
    "            yhat_i = model(xs_i)\n",
    "            y_pred = np.append(y_pred, np.array(yhat_i.cpu()))\n",
    "            y_true = np.append(y_true, np.array(y_i.cpu()))\n",
    "    \n",
    "    out = np.concatenate([y_true[:, None], y_pred[:, None]], axis = 1) \n",
    "    out = pd.DataFrame(out, columns = ['y_true', 'y_pred'])\n",
    "    return(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b556ceeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1711e096",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00dceeb3",
   "metadata": {},
   "source": [
    "## Functions from multi-trait output tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eeb862b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| export\n",
    "\n",
    "# def train_loop3(dataloader, model, loss_fn, optimizer, silent = False):\n",
    "#     \"This is a version of train_loop which concatenates three ys.\"\n",
    "#     import torch\n",
    "#     from torch.utils.data import Dataset\n",
    "#     from torch.utils.data import DataLoader\n",
    "#     size = len(dataloader.dataset)\n",
    "#     for batch, (xs_i, y1_i, y2_i, y3_i) in enumerate(dataloader):\n",
    "#         # Compute prediction and loss\n",
    "#         pred = model(xs_i)\n",
    "#         loss = loss_fn(pred, torch.concat([y1_i, y2_i, y3_i], axis = 1)) # <----------------------------------------\n",
    "\n",
    "#         # Backpropagation\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         if batch % 100 == 0:\n",
    "#             loss, current = loss.item(), batch * len(y1_i) # <----------------\n",
    "#             if not silent:\n",
    "#                 print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef067bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0904feee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| export\n",
    "\n",
    "# def train_error3(dataloader, model, loss_fn, silent = False):\n",
    "#     import torch\n",
    "#     from torch.utils.data import Dataset\n",
    "#     from torch.utils.data import DataLoader\n",
    "#     size = len(dataloader.dataset)\n",
    "#     num_batches = len(dataloader)\n",
    "#     train_loss = 0\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for xs_i, y1_i, y2_i, y3_i in dataloader:\n",
    "#             pred = model(xs_i)\n",
    "#             train_loss += loss_fn(pred, torch.concat([y1_i, y2_i, y3_i], axis = 1)).item() # <----------------------\n",
    "            \n",
    "#     train_loss /= num_batches\n",
    "#     return(train_loss) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9f0fd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d9cf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| export\n",
    "\n",
    "# def test_loop3(dataloader, model, loss_fn, silent = False):\n",
    "#     import torch\n",
    "#     from torch.utils.data import Dataset\n",
    "#     from torch.utils.data import DataLoader\n",
    "\n",
    "#     size = len(dataloader.dataset)\n",
    "#     num_batches = len(dataloader)\n",
    "#     test_loss = 0\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for xs_i, y1_i, y2_i, y3_i in dataloader:\n",
    "#             pred = model(xs_i)\n",
    "#             test_loss += loss_fn(pred, torch.concat([y1_i, y2_i, y3_i], axis = 1)).item() # <-----------------------\n",
    "\n",
    "#     test_loss /= num_batches\n",
    "#     if not silent:\n",
    "#         print(f\"Test Error: Avg loss: {test_loss:>8f}\")\n",
    "#     return(test_loss) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c78817",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6745d1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962ace40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| export\n",
    "\n",
    "# def yhat_loop3(dataloader, model):\n",
    "#     \"Version of yhat_loop that returns 3 ys\"\n",
    "#     import numpy as np\n",
    "#     import pandas as pd\n",
    "#     import torch\n",
    "#     size = len(dataloader.dataset)\n",
    "#     num_batches = len(dataloader)\n",
    "    \n",
    "#     first_loop = True\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for xs_i, y1_i, y2_i, y3_i in dataloader:\n",
    "#             yhat_i = model(xs_i)\n",
    "#             y_i = torch.concat([y1_i, y2_i, y3_i], axis = 1) # <-----------------------\n",
    "\n",
    "#             if first_loop:\n",
    "#                 y_pred = np.array(yhat_i.cpu())\n",
    "#                 y_true = np.array(y_i.cpu())\n",
    "#                 first_loop = False\n",
    "#             else:            \n",
    "#                 y_pred = np.concatenate([y_pred, np.array(yhat_i.cpu())])\n",
    "#                 y_true = np.concatenate([y_true, np.array(y_i.cpu())])\n",
    "                \n",
    "#     out = np.concatenate([y_true[:, :], y_pred[:, :]], axis = 1) \n",
    "#     out = pd.DataFrame(out, columns = ['y1_true', 'y2_true', 'y3_true', 'y1_pred', 'y2_pred', 'y3_pred'])\n",
    "#     return(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b659c93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
