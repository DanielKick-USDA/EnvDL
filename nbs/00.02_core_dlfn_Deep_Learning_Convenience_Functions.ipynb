{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a7219b3",
   "metadata": {},
   "source": [
    "# Deep Learning Convenience Functions\n",
    "\n",
    "> This notebook contains convenience functions to aid in modeling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32788a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp dlfn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7060d245",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os, re\n",
    "import math\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from EnvDL.core import read_json\n",
    "from EnvDL.dna import np_3d_to_hilbert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b58d051",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053d375a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8510e582",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def calc_cs(x # numeric array\n",
    "           ): \n",
    "    \"Calculate nan mean and nan std of an array. Returned as list\"\n",
    "    return [np.nanmean(x, axis = 0), np.nanstd(x, axis = 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b0a2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def apply_cs(xs, \n",
    "             cs_dict_entry # list of length 2 containing mean and s\n",
    "            ): return ((xs - cs_dict_entry[0]) / cs_dict_entry[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9321aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def reverse_cs(xs, cs_dict_entry): return (cs_dict_entry[1] * xs) + cs_dict_entry[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c133332b",
   "metadata": {},
   "source": [
    "## Train/Validate/Test Split info\n",
    "\n",
    "Stored as json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe75151f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def read_split_info(\n",
    "    load_from = '../nbs_artifacts/01.06_g2fc_cluster_genotypes/',\n",
    "    json_prefix = '2023:9:5:12:8:26'):\n",
    "    \"\"\n",
    "    jsons = [e for e in os.listdir(load_from) if re.match('^'+json_prefix+'.+\\.json$', e)]\n",
    "    vals = [e for e in jsons if re.match('.+val\\d+\\.json$', e)]\n",
    "    vals.sort()\n",
    "    out = {}\n",
    "    out['test'] = [read_json(json_path = load_from+json_prefix+'-test.json')]\n",
    "    out['test_file'] = [json_prefix+'-test.json']\n",
    "    out['validate'] = [read_json(json_path = load_from+val) for val in vals]\n",
    "    out['validate_files'] = [val for val in vals]\n",
    "    return(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051b1a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def find_idxs_split_dict(\n",
    "    obs_df, # assumes presence of Year, Female, Male\n",
    "    split_dict # from read_split_info() output. Should be a test of validate dict.\n",
    "):\n",
    "    temp = obs_df\n",
    "    test_mask = ((temp.Year.isin(split_dict['test_years'])) & \n",
    "                 ((temp.Female.isin(split_dict['test_parents'])) |\n",
    "                  (temp.Male.isin(split_dict['test_parents']))))\n",
    "    temp['Split'] = ''\n",
    "    temp.loc[test_mask, 'Split'] = 'Test'\n",
    "\n",
    "    train_mask = (~(temp.Year.isin(split_dict['test_years'])) & \n",
    "                 (~((temp.Female.isin(split_dict['test_parents'])) |\n",
    "                  (temp.Male.isin(split_dict['test_parents'])))))\n",
    "    temp.loc[train_mask, 'Split'] = 'Train'\n",
    "\n",
    "    temp_test  = (temp.Split == 'Test') # should be the same as with the mask above\n",
    "    temp_train = (temp.Split == 'Train') # should be the same as with the mask above\n",
    "\n",
    "    # Confirm that there's no overlap in parents or years\n",
    "    temp_test_parents  = set(temp.loc[temp_test, 'Female']+temp.loc[temp_test, 'Male'])\n",
    "    temp_train_parents = set(temp.loc[temp_train, 'Female']+temp.loc[temp_train, 'Male'])\n",
    "\n",
    "    temp_test_years  = set(temp.loc[temp_test, 'Year'])\n",
    "    temp_train_years = set(temp.loc[temp_train, 'Year'])\n",
    "\n",
    "    assert [] == [e for e in temp_test_parents if e in temp_train_parents]\n",
    "    assert [] == [e for e in temp_train_parents if e in temp_test_parents]\n",
    "    assert [] == [e for e in temp_test_years if e in temp_train_years]\n",
    "    assert [] == [e for e in temp_train_years if e in temp_test_years]\n",
    "\n",
    "    return({\n",
    "        'test_idx': temp.loc[test_mask, ].index, \n",
    "        'train_idx': temp.loc[train_mask, ].index} )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be51f17",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549c2b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def LSUV_(model, data, apply_only_to=['Conv', 'Linear', 'Bilinear'],\n",
    "          std_tol=0.1, max_iters=10, do_ortho_init=True, logging_FN=print):\n",
    "    r\"\"\"\n",
    "    Refer to https://github.com/glassroom/torch_lsuv_init\n",
    "    Applies layer sequential unit variance (LSUV), as described in\n",
    "    `All you need is a good init` - Mishkin, D. et al (2015):\n",
    "    https://arxiv.org/abs/1511.06422\n",
    "\n",
    "    Args:\n",
    "        model: `torch.nn.Module` object on which to apply LSUV.\n",
    "        data: sample input data drawn from training dataset.\n",
    "        apply_only_to: list of strings indicating target children\n",
    "            modules. For example, ['Conv'] results in LSUV applied\n",
    "            to children of type containing the substring 'Conv'.\n",
    "        std_tol: positive number < 1.0, below which differences between\n",
    "            actual and unit standard deviation are acceptable.\n",
    "        max_iters: number of times to try scaling standard deviation\n",
    "            of each children module's output activations.\n",
    "        do_ortho_init: boolean indicating whether to apply orthogonal\n",
    "            init to parameters of dim >= 2 (zero init if dim < 2).\n",
    "        logging_FN: function for outputting progress information.\n",
    "\n",
    "    Example:\n",
    "        >>> model = nn.Sequential(nn.Linear(8, 2), nn.Softmax(dim=1))                                                                                                                                                                                                                                            \n",
    "        >>> data = torch.randn(100, 8)\n",
    "        >>> LSUV_(model, data)\n",
    "    \"\"\"\n",
    "\n",
    "    matched_modules = [m for m in model.modules() if any(substr in str(type(m)) for substr in apply_only_to)]\n",
    "\n",
    "    if do_ortho_init:\n",
    "        logging_FN(f\"Applying orthogonal init (zero init if dim < 2) to params in {len(matched_modules)} module(s).\")\n",
    "        for m in matched_modules:\n",
    "            for p in m.parameters():                \n",
    "                torch.nn.init.orthogonal_(p) if (p.dim() >= 2) else torch.nn.init.zeros_(p)\n",
    "\n",
    "    logging_FN(f\"Applying LSUV to {len(matched_modules)} module(s) (up to {max_iters} iters per module):\")\n",
    "\n",
    "    def _compute_and_store_LSUV_stats(m, inp, out):\n",
    "        m._LSUV_stats = { 'mean': out.detach().mean(), 'std': out.detach().std() }\n",
    "\n",
    "    was_training = model.training\n",
    "    model.train()  # sets all modules to training behavior\n",
    "    with torch.no_grad():\n",
    "        for i, m in enumerate(matched_modules):\n",
    "            with m.register_forward_hook(_compute_and_store_LSUV_stats):\n",
    "                for t in range(max_iters):\n",
    "                    _ = model(data)  # run data through model to get stats\n",
    "                    mean, std = m._LSUV_stats['mean'], m._LSUV_stats['std']\n",
    "                    if abs(std - 1.0) < std_tol:\n",
    "                        break\n",
    "                    m.weight.data /= (std + 1e-6)\n",
    "            logging_FN(f\"Module {i:2} after {(t+1):2} itr(s) | Mean:{mean:7.3f} | Std:{std:6.3f} | {type(m)}\")\n",
    "            delattr(m, '_LSUV_stats')\n",
    "\n",
    "    if not was_training: model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4957d8c",
   "metadata": {},
   "source": [
    "## Network Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fe1654",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def Linear_block(in_size, out_size, drop_pr):\n",
    "    block = nn.Sequential(\n",
    "        nn.Linear(in_size, out_size),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(drop_pr)\n",
    "    )\n",
    "    return(block) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6214c2b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbe6221",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def Conv1D_x2_Max_block(in_channels, out_channels, kernel_size, stride, maxpool_size):\n",
    "    block = nn.Sequential(\n",
    "        nn.Conv1d(\n",
    "            in_channels= in_channels, # second channel\n",
    "            out_channels= out_channels,\n",
    "            kernel_size= kernel_size,\n",
    "            stride= stride\n",
    "        ), \n",
    "        nn.Conv1d(\n",
    "            in_channels= out_channels, \n",
    "            out_channels= out_channels,\n",
    "            kernel_size= kernel_size,\n",
    "            stride= stride\n",
    "        ), \n",
    "        nn.BatchNorm1d(out_channels),\n",
    "        nn.MaxPool1d((maxpool_size), stride=stride)\n",
    "    )\n",
    "    return(block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bbdff1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "25d2f79c",
   "metadata": {},
   "source": [
    "### Resnet Blocks "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf58d524",
   "metadata": {},
   "source": [
    "#### Resnet For 4d (b, c, h, w) (2d conv)\n",
    "4d is shown first because this code was adapted from the pytorch implementation of ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02c96b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1):\n",
    "    # Using https://pytorch.org/vision/main/_modules/torchvision/models/resnet.html as a starting point\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "\n",
    "    return nn.Conv2d(\n",
    "        in_planes,\n",
    "        out_planes,\n",
    "        kernel_size=3,\n",
    "        stride=stride,\n",
    "        padding=dilation,\n",
    "        groups=groups,\n",
    "        bias=False,\n",
    "        dilation=dilation,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb3ebea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def conv1x1(in_planes: int, out_planes: int, stride: int = 1):\n",
    "\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c2282b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class BasicBlock2d(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self,\n",
    "        inplanes: int,\n",
    "        planes: int,\n",
    "        stride: int = 1,\n",
    "        downsample: nn.Module = None,\n",
    "        groups: int = 1,\n",
    "        base_width: int = 64,\n",
    "        dilation: int = 1,\n",
    "        norm_layer: nn.Module = None,\n",
    "        expansion: int = 1\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        # Set up defaults if none was passed in\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if groups != 1 or base_width !=  64: raise ValueError('Only groups = 1 and base_width = 64 supported')\n",
    "        if dilation > 1: raise NotImplementedError('Dilation not supported')\n",
    "\n",
    "        # self.expansion = expansion # in ResNet v1, this is 1, in ResNet v1.5 it is set to 4\n",
    "\n",
    "        # self.conv1 = conv3x3(inplanes, planes*self.expansion, stride)\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        # self.conv2 = conv3x3(planes, planes*self.expansion)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b8090d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class BottleneckBlock2d(nn.Module):\n",
    "    # This is the block used in ResNet v1.5. It is supposed to be more effective.\n",
    "    # Main changes are that \n",
    "    # - expansion is not set to 1 \n",
    "    # - now there is a third convolution that happens in the slow path\n",
    "    #\n",
    "    def __init__(\n",
    "        self,\n",
    "        inplanes: int,\n",
    "        planes: int,\n",
    "        stride: int = 1,\n",
    "        downsample: nn.Module = None,\n",
    "        groups: int = 1,\n",
    "        base_width: int = 64,\n",
    "        dilation: int = 1,\n",
    "        norm_layer: nn.Module = None,\n",
    "        expansion: int = 4\n",
    "    ) -> None:        \n",
    "        super().__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "            \n",
    "        self.expansion = expansion\n",
    "\n",
    "        width = int(planes * (base_width / 64.0)) * groups\n",
    "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv1x1(inplanes, width)\n",
    "        self.bn1 = norm_layer(width)\n",
    "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
    "        self.bn2 = norm_layer(width)\n",
    "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
    "        self.bn3 = norm_layer(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7801a723",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class ResNet2d(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        block, #: Type[Union[BasicBlock, Bottleneck]],\n",
    "        layers, #: List[int],\n",
    "        num_outputs: int = 1,\n",
    "        zero_init_residual: bool = False,\n",
    "        groups: int = 1,\n",
    "        width_per_group: int = 64,\n",
    "        replace_stride_with_dilation = None, #: Optional[List[bool]] = None,\n",
    "        norm_layer = None, #: Optional[Callable[..., nn.Module]] = None,\n",
    "        input_channels = 4\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        # _log_api_usage_once(self)\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        self.inplanes = 64\n",
    "        self.dilation = 1\n",
    "        if replace_stride_with_dilation is None:\n",
    "            # each element in the tuple indicates if we should replace\n",
    "            # the 2x2 stride with a dilated convolution instead\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\n",
    "                \"replace_stride_with_dilation should be None \"\n",
    "                f\"or a 3-element tuple, got {replace_stride_with_dilation}\"\n",
    "            )\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        self.conv1 = nn.Conv2d(input_channels, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False) # Note that this is 4 not 3\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, dilate=replace_stride_with_dilation[0])\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, dilate=replace_stride_with_dilation[1])\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, dilate=replace_stride_with_dilation[2])\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_outputs)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck) and m.bn3.weight is not None:\n",
    "                    nn.init.constant_(m.bn3.weight, 0)  # type: ignore[arg-type]\n",
    "                elif isinstance(m, BasicBlock) and m.bn2.weight is not None:\n",
    "                    nn.init.constant_(m.bn2.weight, 0)  # type: ignore[arg-type]\n",
    "\n",
    "    def _make_layer(\n",
    "        self,\n",
    "        block, #: Type[Union[BasicBlock, Bottleneck]],\n",
    "        planes: int,\n",
    "        blocks: int,\n",
    "        stride: int = 1,\n",
    "        dilate: bool = False,\n",
    "    ) -> nn.Sequential:\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(\n",
    "            block(\n",
    "                self.inplanes, planes, stride, downsample, self.groups, self.base_width, previous_dilation, norm_layer\n",
    "            )\n",
    "        )\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(\n",
    "                block(\n",
    "                    self.inplanes,\n",
    "                    planes,\n",
    "                    groups=self.groups,\n",
    "                    base_width=self.base_width,\n",
    "                    dilation=self.dilation,\n",
    "                    norm_layer=norm_layer,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _forward_impl(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # See note [TorchScript super()]\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self._forward_impl(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a16055",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d404cbb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "568521e6",
   "metadata": {},
   "source": [
    "#### Resnet For 3d (b, c, l) (1d conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72279530",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def conv0x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv1d(\n",
    "        in_planes,\n",
    "        out_planes,\n",
    "        kernel_size=3,\n",
    "        stride=stride,\n",
    "        padding=dilation,\n",
    "        groups=groups,\n",
    "        bias=False,\n",
    "        dilation=dilation,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dfbbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def conv0x1(in_planes: int, out_planes: int, stride: int = 1):\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv1d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f921612",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class BasicBlock1d(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self,\n",
    "        inplanes: int,\n",
    "        planes: int,\n",
    "        stride: int = 1,\n",
    "        downsample: nn.Module = None,\n",
    "        groups: int = 1,\n",
    "        base_width: int = 64,\n",
    "        dilation: int = 1,\n",
    "        norm_layer: nn.Module = None,\n",
    "        expansion: int = 1\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        # Set up defaults if none was passed in\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm1d\n",
    "        if groups != 1 or base_width !=  64: raise ValueError('Only groups = 1 and base_width = 64 supported')\n",
    "        if dilation > 1: raise NotImplementedError('Dilation not supported')\n",
    "\n",
    "        # self.expansion = expansion # in ResNet v1, this is 1, in ResNet v1.5 it is set to 4\n",
    "\n",
    "        # self.conv1 = conv3x3(inplanes, planes*self.expansion, stride)\n",
    "        self.conv1 = conv0x3(inplanes, planes, stride)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        # self.conv2 = conv3x3(planes, planes*self.expansion)\n",
    "        self.conv2 = conv0x3(planes, planes)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9195dbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "  \n",
    "class BottleneckBlock1d(nn.Module):\n",
    "    # This is the block used in ResNet v1.5. It is supposed to be more effective.\n",
    "    # Main changes are that \n",
    "    # - expansion is not set to 1 \n",
    "    # - now there is a third convolution that happens in the slow path\n",
    "    #\n",
    "    def __init__(\n",
    "        self,\n",
    "        inplanes: int,\n",
    "        planes: int,\n",
    "        stride: int = 1,\n",
    "        downsample: nn.Module = None,\n",
    "        groups: int = 1,\n",
    "        base_width: int = 64,\n",
    "        dilation: int = 1,\n",
    "        norm_layer: nn.Module = None,\n",
    "        expansion: int = 4\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm1d\n",
    "            \n",
    "        self.expansion = expansion\n",
    "\n",
    "        width = int(planes * (base_width / 64.0)) * groups\n",
    "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv0x1(inplanes, width)\n",
    "        self.bn1 = norm_layer(width)\n",
    "        self.conv2 = conv0x3(width, width, stride, groups, dilation)\n",
    "        self.bn2 = norm_layer(width)\n",
    "        self.conv3 = conv0x1(width, planes * self.expansion)\n",
    "        self.bn3 = norm_layer(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825ba1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ResNet1d(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        block, #: Type[Union[BasicBlock, Bottleneck]],\n",
    "        layers, #: List[int],\n",
    "        num_outputs: int = 1,\n",
    "        zero_init_residual: bool = False,\n",
    "        groups: int = 1,\n",
    "        width_per_group: int = 64,\n",
    "        replace_stride_with_dilation = None, #: Optional[List[bool]] = None,\n",
    "        norm_layer = None, #: Optional[Callable[..., nn.Module]] = None,\n",
    "        input_channels = 4\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        # _log_api_usage_once(self)\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm1d\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        self.inplanes = 64\n",
    "        self.dilation = 1\n",
    "        if replace_stride_with_dilation is None:\n",
    "            # each element in the tuple indicates if we should replace\n",
    "            # the 2x2 stride with a dilated convolution instead\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\n",
    "                \"replace_stride_with_dilation should be None \"\n",
    "                f\"or a 3-element tuple, got {replace_stride_with_dilation}\"\n",
    "            )\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        self.conv1 = nn.Conv1d(input_channels, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False) # Note that this is 4 not 3\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, dilate=replace_stride_with_dilation[0])\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, dilate=replace_stride_with_dilation[1])\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, dilate=replace_stride_with_dilation[2])\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d((1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_outputs)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            elif isinstance(m, (nn.BatchNorm1d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck) and m.bn3.weight is not None:\n",
    "                    nn.init.constant_(m.bn3.weight, 0)  # type: ignore[arg-type]\n",
    "                elif isinstance(m, BasicBlock) and m.bn2.weight is not None:\n",
    "                    nn.init.constant_(m.bn2.weight, 0)  # type: ignore[arg-type]\n",
    "\n",
    "    def _make_layer(\n",
    "        self,\n",
    "        block, #: Type[Union[BasicBlock, Bottleneck]],\n",
    "        planes: int,\n",
    "        blocks: int,\n",
    "        stride: int = 1,\n",
    "        dilate: bool = False,\n",
    "    ) -> nn.Sequential:\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv0x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(\n",
    "            block(\n",
    "                self.inplanes, planes, stride, downsample, self.groups, self.base_width, previous_dilation, norm_layer\n",
    "            )\n",
    "        )\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(\n",
    "                block(\n",
    "                    self.inplanes,\n",
    "                    planes,\n",
    "                    groups=self.groups,\n",
    "                    base_width=self.base_width,\n",
    "                    dilation=self.dilation,\n",
    "                    norm_layer=norm_layer,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _forward_impl(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # See note [TorchScript super()]\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self._forward_impl(x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e300a71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b81262d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d903df3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3efbd85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f5b3ef5",
   "metadata": {},
   "source": [
    "## Training (general)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b1e463",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def train_loop(dataloader, model, loss_fn, optimizer, silent = False):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (xs_i, y_i) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(xs_i)\n",
    "        loss = loss_fn(pred, y_i) # <----------------------------------------\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(y_i) # <----------------\n",
    "            if not silent:\n",
    "                print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22314751",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def train_error(dataloader, model, loss_fn, silent = False):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    train_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xs_i, y_i in dataloader:\n",
    "            pred = model(xs_i)\n",
    "            train_loss += loss_fn(pred, y_i).item() # <----------------------\n",
    "            \n",
    "    train_loss /= num_batches\n",
    "    return(train_loss) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51151878",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn, silent = False):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xs_i, y_i in dataloader:\n",
    "            pred = model(xs_i)\n",
    "            test_loss += loss_fn(pred, y_i).item() # <-----------------------\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    if not silent:\n",
    "        print(f\"Test Error: Avg loss: {test_loss:>8f}\")\n",
    "    return(test_loss) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543a7603",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def yhat_loop(dataloader, model):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    \n",
    "    y_true = np.array([])\n",
    "    y_pred = np.array([])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for xs_i, y_i in dataloader:\n",
    "            yhat_i = model(xs_i)\n",
    "            y_pred = np.append(y_pred, np.array(yhat_i.cpu()))\n",
    "            y_true = np.append(y_true, np.array(y_i.cpu()))\n",
    "    \n",
    "    out = np.concatenate([y_true[:, None], y_pred[:, None]], axis = 1) \n",
    "    out = pd.DataFrame(out, columns = ['y_true', 'y_pred'])\n",
    "    return(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c71c174",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def train_nn(\n",
    "    cache_path,\n",
    "    training_dataloader,\n",
    "    testing_dataloader,\n",
    "    model,\n",
    "    learning_rate = 1e-3,\n",
    "    batch_size = 64,\n",
    "    epochs = 500,\n",
    "    model_prefix = 'model'\n",
    "):\n",
    "    # Initialize the loss function\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    loss_df = pd.DataFrame([i for i in range(epochs)], columns = ['Epoch'])\n",
    "    loss_df['TrainMSE'] = np.nan\n",
    "    loss_df['TestMSE']  = np.nan\n",
    "\n",
    "    for t in tqdm(range(epochs)):        \n",
    "        # print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train_loop(training_dataloader, model, loss_fn, optimizer, silent = True)\n",
    "\n",
    "        loss_df.loc[loss_df.index == t, 'TrainMSE'\n",
    "                   ] = train_error(training_dataloader, model, loss_fn, silent = True)\n",
    "        \n",
    "        loss_df.loc[loss_df.index == t, 'TestMSE'\n",
    "                   ] = test_loop(testing_dataloader, model, loss_fn, silent = True)\n",
    "        \n",
    "        if (t+1)%5 == 0: # Cache in case training is interupted. \n",
    "            # print(loss_df.loc[loss_df.index == t, ['TrainMSE', 'TestMSE']])\n",
    "            torch.save(model.state_dict(), \n",
    "                       cache_path+'/'+model_prefix+'_'+str(t)+'_'+str(epochs)+'.pt') # convention is to use .pt or .pth\n",
    "        \n",
    "    return([model, loss_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c13dc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363c0a24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4b170a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def estimate_iterations(sec_per_it = 161):\n",
    "    hours = [1, 2, 4, 8, 12, 24]\n",
    "    res = pd.DataFrame(zip(hours, \n",
    "    [math.floor(\n",
    "        ((i)*(60*60))/sec_per_it\n",
    "    ) for i in hours]), columns = ['Hours', 'Iterations'])\n",
    "    return(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707a9168",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e62927b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ACGTDataset(Dataset): # for any G containing matix with many (phno) to one (geno)\n",
    "    def __init__(self, \n",
    "                 y, \n",
    "                 G, # not on gpu\n",
    "                 idx_original,\n",
    "                 idx_lookup,\n",
    "                 transform = None, target_transform = None,\n",
    "                 use_gpu_num = 0,\n",
    "                 device = 'cuda',\n",
    "                 **kwargs \n",
    "                ):\n",
    "\n",
    "        self.device = device\n",
    "        self.y = y \n",
    "        self.G = G\n",
    "        self.idx_original = idx_original\n",
    "        self.idx_lookup = idx_lookup\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform    \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        y_idx =self.y[idx]\n",
    "            \n",
    "        #                 |array containing correct index in deduplicated g \n",
    "        #                 |               index in phno    \n",
    "        uniq_g_idx = self.idx_lookup[self.idx_original[idx], 1]\n",
    "        g_idx = self.G[uniq_g_idx, :, :]\n",
    "        \n",
    "        # send all to gpu        \n",
    "        if (self.device != 'cpu'):\n",
    "            if y_idx.device.type == 'cpu':\n",
    "                y_idx = y_idx.to(self.device) \n",
    "                \n",
    "            if g_idx.device.type == 'cpu':\n",
    "                g_idx = g_idx.to(self.device) \n",
    "        \n",
    "        \n",
    "        if self.transform:\n",
    "            g_idx = self.transform(g_idx)\n",
    "            \n",
    "        if self.target_transform:\n",
    "            y_idx = self.transform(y_idx)\n",
    "        return g_idx, y_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Multiple Inputs (but not requiring multiple inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BigDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        lookup_obs,\n",
    "#         lookup_geno,\n",
    "#         lookup_env,\n",
    "#         y,\n",
    "#         G, \n",
    "#         G_type\n",
    "#         S,\n",
    "#         P,\n",
    "#         W,\n",
    "#         W_type,\n",
    "        transform = None, \n",
    "        target_transform = None,\n",
    "        **kwargs \n",
    "        ):\n",
    "        \"\"\"\n",
    "        This class produces a set with one or more input tensors. For flexibility the only _required_ input is `lookup_obs`, a tensor with the index of observations. \n",
    "        Everything else is provided as a kwarg. Output is a list of tensors1 ordered [y, G, S, W], any of these not initalized will be missing but not empty (e.g. [y, S, W] not [y, None, S, W]).       \n",
    "        Used inputs are:\n",
    "        lookup_obs: index for y, used by __getitem__ for obs_idx\n",
    "        lookup_geno: index for G, row obs_idx, column 1 is geno_idx (geno information is deduplicated, hence the need for a lookup)\n",
    "        lookup_env: index for S & W, , row obs_idx, column 1 is env_idx (env information is deduplicated, hence the need for a lookup)\n",
    "        y: yield\n",
    "        G: Genomic information \n",
    "        G_type: how the infomation should be returned, 'raw', 'hilbert', or 'list' (i.e. of tensors for snps in each gene)\n",
    "        S: Soil information\n",
    "        P: Planting/Harvest date contained in column 0, 1 respectively \n",
    "        W: Weather data\n",
    "        W_type: how the infomation should be returned, 'raw' or 'hilbert'\n",
    "\n",
    "        1 G may also be returned as a list of tensors\n",
    "        \"\"\"\n",
    "        # Lookup info (so that deduplication works)\n",
    "        self.lookup_obs = lookup_obs\n",
    "        # if 'lookup_obs'  in kwargs: self.lookup_obs  = kwargs['lookup_obs'];\n",
    "        if 'lookup_geno' in kwargs: self.lookup_geno = kwargs['lookup_geno'];\n",
    "        if 'lookup_env'  in kwargs: self.lookup_env  = kwargs['lookup_env'];\n",
    "        # Data\n",
    "        if 'y' in kwargs: self.y = kwargs['y'];\n",
    "        if 'G' in kwargs: self.G = kwargs['G'];\n",
    "        if 'S' in kwargs: self.S = kwargs['S'];\n",
    "        if 'P' in kwargs: self.P = kwargs['P']; # PlantHarvest so that planting can be added into W\n",
    "        if 'W' in kwargs: self.W = kwargs['W'];\n",
    "        # Data prep state information\n",
    "        if 'G_type' in kwargs: self.G_type = kwargs['G_type']; # raw, hilbert, list\n",
    "        if 'W_type' in kwargs: self.W_type = kwargs['W_type']; # raw, hilbert\n",
    "        # Data to be returned\n",
    "        self.out_names = [e for e in ['y', 'G', 'S', 'W'] if e in kwargs]\n",
    "        # Transformations\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.lookup_obs)\n",
    "    \n",
    "\n",
    "    # These used to be in __getitem__ but separating them like this allows for them to be overwritten more easily\n",
    "    def get_y(self, idx):\n",
    "        y_idx = self.y[idx]\n",
    "        if self.transform:\n",
    "            y_idx = self.transform(y_idx)\n",
    "        return(y_idx)\n",
    "        \n",
    "    def get_G(self, idx):\n",
    "        geno_idx = self.lookup_geno[idx, 1]\n",
    "        if self.G_type in ['raw', 'hilbert']:\n",
    "            G_idx = self.G[geno_idx]\n",
    "        if 'list' == self.G_type:\n",
    "            G_idx = [e[geno_idx] for e in self.G]\n",
    "        if self.transform:\n",
    "            G_idx = self.transform(G_idx)\n",
    "        return(G_idx)\n",
    "\n",
    "    def get_S(self, idx):\n",
    "        env_idx = self.lookup_env[idx, 1]\n",
    "        S_idx = self.S[env_idx]\n",
    "        if self.transform:\n",
    "            S_idx = self.transform(S_idx)\n",
    "        return(S_idx)\n",
    "\n",
    "    def get_W(self, idx):\n",
    "        W_device = torch.Tensor(self.W).get_device()\n",
    "\n",
    "\n",
    "        env_idx = self.lookup_env[idx, 1]\n",
    "        # get growing information\n",
    "        WPlant = np.zeros(365)\n",
    "        # WPlant[self.P[obs_idx, 0]:self.P[obs_idx, 1]] = 1\n",
    "        WPlant[self.P[idx, 0]:self.P[idx, 1]] = 1\n",
    "        if self.W_type == 'raw':\n",
    "            WPlant = torch.from_numpy(WPlant).to(torch.float)\n",
    "            # if needed send to gpu\n",
    "            if W_device != -1: WPlant = WPlant.to(W_device)            \n",
    "            W_idx = torch.concatenate([self.W[env_idx], WPlant[None, :]], axis = 0)\n",
    "        if self.W_type == 'hilbert':\n",
    "            # convert growing info to hilbert curve\n",
    "            WPlant_hilb = np_3d_to_hilbert(WPlant[None, :, None], silent = True)\n",
    "            WPlant_hilb = WPlant_hilb.squeeze(axis = 3)\n",
    "            WPlant_hilb[np.isnan(WPlant_hilb)] = 0\n",
    "            WPlant_hilb = torch.from_numpy(WPlant_hilb).to(torch.float)\n",
    "            # if needed send to gpu\n",
    "            if W_device != -1: WPlant_hilb = WPlant_hilb.to(W_device)\n",
    "            W_idx = torch.concatenate([self.W[env_idx], WPlant_hilb], axis = 0)\n",
    "        if self.transform:\n",
    "            W_idx = self.transform(W_idx)\n",
    "        return(W_idx)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        out = []\n",
    "        # obs_idx = self.lookup_obs[idx]\n",
    "        if 'y' in self.out_names: out += [self.get_y(idx)]\n",
    "        if 'G' in self.out_names: out += [self.get_G(idx)]\n",
    "        if 'S' in self.out_names: out += [self.get_S(idx)]\n",
    "        if 'W' in self.out_names: out += [self.get_W(idx)]\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# demo = BigDataset(\n",
    "#     lookup_obs = [i for i in range(len(YMat))], # Training/testing. To use a deduplicated dataset, \n",
    "#                                                 # Pass the training index through the desired lookup\n",
    "#                                                 # list(set(obs_env_lookup[:, 2])) -> phno idxs for each env (1 for each)\n",
    "#     y = YMat,\n",
    "#     lookup_geno = obs_geno_lookup,\n",
    "#     G = ACGT,\n",
    "#     G_type = 'raw',\n",
    "#     lookup_env = obs_env_lookup,\n",
    "#     S = SMat,\n",
    "#     P = PlantHarvest,\n",
    "#     W = WMat,\n",
    "#     W_type = 'raw',\n",
    "#           )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2668965",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Standard data prep\n",
    "\n",
    "# Wrapper function to hide the steps of loading data\n",
    "import numpy as np\n",
    "from EnvDL.core import get_cached_result\n",
    "from EnvDL.dlfn import read_split_info, find_idxs_split_dict\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "class g2fc_datawrapper():   \n",
    "    def __init__(self):\n",
    "        self.data_dict = {}\n",
    "        self.cs_dict = {}\n",
    "        print('Loading and storing default `phno`.')\n",
    "        self.load(name='phno', store = True)\n",
    "    \n",
    "\n",
    "    def set_split(self, load_from = '../nbs_artifacts/01.06_g2fc_cluster_genotypes/', json_prefix = '2023:9:5:12:8:26'):\n",
    "        if 'phno' not in self.data_dict.keys():\n",
    "            print('`phno` must be stored!\\nManually initialize with .load()')\n",
    "        else:\n",
    "            split_info = read_split_info(load_from = load_from, json_prefix = json_prefix)\n",
    "\n",
    "            temp = self.data_dict['phno'].copy()\n",
    "            temp[['Female', 'Male']] = temp['Hybrid'].str.split('/', expand = True)\n",
    "\n",
    "            self.test_dict = find_idxs_split_dict(\n",
    "                obs_df = temp, \n",
    "                split_dict = split_info['test'][0]\n",
    "            )\n",
    "\n",
    "            temp = temp.loc[self.test_dict['train_idx'], ] # restrict before re-aplying\n",
    "\n",
    "            self.val_dict = find_idxs_split_dict(\n",
    "                obs_df = temp, \n",
    "                split_dict = split_info['validate'][0]\n",
    "            )\n",
    "\n",
    "    def generic_load(self, load_from, file_name):        \n",
    "        if   file_name.split('.')[-1] == 'pkl': res = get_cached_result(load_from+file_name)\n",
    "        elif file_name.split('.')[-1] == 'npy': res = np.load(load_from+file_name)\n",
    "        elif file_name.split('.')[-1] == 'csv': res = pd.read_csv(load_from+file_name)\n",
    "        else: print(f'Unrecognized file encoding: {file_name.split(\".\")[-1]} \\nReturning None'); res = None\n",
    "        return res\n",
    "\n",
    "    def load(self, name='ACGT', store = False, **kwargs):\n",
    "        # defaults for quick access\n",
    "        defaults_dict = {\n",
    "            ## Genomic Data\n",
    "            'ACGT':         ['../nbs_artifacts/01.03_g2fc_prep_matrices/', 'ACGT.npy'],\n",
    "            'ACGT_hilb':    ['../nbs_artifacts/01.03_g2fc_prep_matrices/', 'ACGT_hilb.npy'],            \n",
    "            'KEGG_entries': ['../nbs_artifacts/01.05_g2fc_demo_model/', 'filtered_kegg_gene_entries.pkl'],\n",
    "            'KEGG_slices':  ['../nbs_artifacts/01.05_g2fc_demo_model/', 'ACGT_gene_slice_list.pkl'],\n",
    "\n",
    "            ## Soil and Management \n",
    "            'mgmtMatNames': ['../nbs_artifacts/01.03_g2fc_prep_matrices/', 'mgmtMatNames.npy'],\n",
    "            'mgmtMat':      ['../nbs_artifacts/01.03_g2fc_prep_matrices/', 'mgmtMat.npy'],\n",
    "            'SMatNames':    ['../nbs_artifacts/01.03_g2fc_prep_matrices/', 'SMatNames.npy'],\n",
    "            'SMat':         ['../nbs_artifacts/01.03_g2fc_prep_matrices/', 'SMat.npy'],\n",
    "\n",
    "            ## Weather\n",
    "            'PlantHarvestNames': ['../nbs_artifacts/01.03_g2fc_prep_matrices/', 'PlantHarvestNames.npy'],\n",
    "            'PlantHarvest':      ['../nbs_artifacts/01.03_g2fc_prep_matrices/', 'PlantHarvest.npy'],\n",
    "            'WMat':              ['../nbs_artifacts/01.03_g2fc_prep_matrices/', 'WMat.npy'],\n",
    "            'WMatNames':         ['../nbs_artifacts/01.03_g2fc_prep_matrices/', 'WMatNames.npy'],\n",
    "            'WMat_hilb':         ['../nbs_artifacts/01.03_g2fc_prep_matrices/', 'WMat_hilb.npy'],\n",
    "\n",
    "            # Response and lookup\n",
    "            'phno':            ['../nbs_artifacts/01.03_g2fc_prep_matrices/', 'phno_geno.csv'],\n",
    "            'obs_geno_lookup': ['../nbs_artifacts/01.03_g2fc_prep_matrices/', 'obs_geno_lookup.npy'], # Phno_Idx  Geno_Idx  Is_Phno_Idx\n",
    "            'obs_env_lookup':  ['../nbs_artifacts/01.03_g2fc_prep_matrices/', 'obs_env_lookup.npy'],  # Phno_Idx  Env_Idx   Is_Phno_Idx\n",
    "            'YMat':            ['../nbs_artifacts/01.03_g2fc_prep_matrices/', 'YMat.npy']\n",
    "        }\n",
    "\n",
    "        if name in defaults_dict.keys():\n",
    "            load_from, file_name = defaults_dict[name]\n",
    "        else: \n",
    "            print(f'`name` not recognized. Use `load_from` and `file_name` for greater control.\\,Allowed `names` are:\\n{list(defaults_dict.keys())}')\n",
    "        \n",
    "        # overwrite defaults if desired\n",
    "        if 'load_from' in kwargs.keys(): load_from = kwargs['load_from']\n",
    "        if 'file_name' in kwargs.keys(): file_name = kwargs['file_name']\n",
    "\n",
    "        res = self.generic_load(load_from=load_from, file_name= file_name)\n",
    "\n",
    "        if store:\n",
    "            self.data_dict[name] = res\n",
    "        else:\n",
    "            return res\n",
    "\n",
    "    def load_all(self, name_list = [], store = False):\n",
    "        if store:\n",
    "            for e in name_list:\n",
    "                self.load(name = e, store=store)\n",
    "        else:\n",
    "            res_list = []\n",
    "            for e in name_list:\n",
    "                res_list += [self.load(name = e, store=store)]\n",
    "            return res_list\n",
    "\n",
    "    def store_cs(self, name, cs_list):\n",
    "        self.cs_dict[name] = cs_list\n",
    "\n",
    "    def calc_cs(self, name, version = 'np', **kwargs):\n",
    "\n",
    "        res = self.data_dict[name]\n",
    "        if 'filter' in kwargs.keys():\n",
    "            which_dict, which_split = kwargs['filter'].split(':')\n",
    "\n",
    "            if which_split == 'train':  key = 'train_idx'\n",
    "            elif which_split == 'test': key = 'test_idx'\n",
    "            else: print('only `train` and `test` indexes are allowed.')\n",
    "\n",
    "            if which_dict == 'val':    mask = self.val_dict[key]\n",
    "            elif which_dict == 'test': mask = self.test_dict[key]\n",
    "            else: print('only `val` and `test` sets are allowed.')\n",
    "\n",
    "            if 'filter_lookup' in kwargs.keys():\n",
    "                # This block exists because some data is deduplicated. In the dataloader I use lookup tables to find the right values.\n",
    "                # That gets messy because the enviroment, genome, and yield all get different ones\n",
    "                # I could hardcode names to filters but that would make this code pretty inflexible (which I would like to avoid.)\n",
    "                # using the manual overwrite metod .store_cs() it's possible to get the desired behavior like this:\n",
    "                # X.store_cs('WMat', calc_cs(X.get('WMat')[np.array(list(set(X.get('obs_env_lookup', ops_string='filter:val:train')[:, 1]))),: ,:]))\n",
    "                # That is a lot messier looking than I would like. It's hard to see what's happening. \n",
    "                # To get around this I'm adding a 'filter_lookup' kwarg that does the same job as the lookup tables in the data loader.\n",
    "                lookup = self.data_dict[kwargs['filter_lookup']]\n",
    "                lookup = lookup[mask, 1]\n",
    "                # deduplicate; for cs we don't need the order of the obs.\n",
    "                mask = np.array(list(set(lookup)))\n",
    "            res = res[mask]\n",
    "\n",
    "        else:\n",
    "            print('''\n",
    "Scaling based on ALL data. To avoid this pass in a split to be used. \n",
    "If a lookup table should be used to select observations (e.g. obs_env_lookup ) its name should be passed in. \n",
    "E.g. filter = \\'val:train\\',  filter_lookup = \\'obs_env_lookup\\'\n",
    "                  ''')\n",
    "\n",
    "        if version == 'np':\n",
    "            self.cs_dict[name] = [np.asarray(np.mean(res, axis = 0)), np.asarray(np.std(res, axis = 0))]\n",
    "        elif version == 'torch':\n",
    "            self.cs_dict[name] = [torch.Tensor.mean(res, axis = 0), torch.Tensor.std(res, axis = 0)]\n",
    "\n",
    "    def calc_cs_all(self, name_list, version = 'np', **kwargs):\n",
    "        for name in name_list:\n",
    "            self.calc_cs(name=name, version = version, **kwargs)\n",
    "\n",
    "    def apply_cs(self, name, **kwargs):\n",
    "        if name not in self.cs_dict.keys():\n",
    "            self.calc_cs(name, kwargs)\n",
    "\n",
    "        vals = self.cs_dict[name]\n",
    "        res = self.data_dict[name]\n",
    "        \n",
    "        if type(res) == type(vals[0]):\n",
    "            pass\n",
    "        elif type(res) == torch.Tensor:\n",
    "            # convert to pytorch\n",
    "            vals = [torch.from_numpy(e) for e in vals]\n",
    "        elif type(res) == np.ndarray:\n",
    "            # convert to numpy\n",
    "            vals = [torch.Tensor.numpy(e) for e in vals]\n",
    "            \n",
    "        center, scale = vals\n",
    "        res = (res - center) / scale\n",
    "        return res\n",
    "\n",
    "    def get_cs(self, name):\n",
    "        if name not in self.cs_dict.keys():\n",
    "            res = None\n",
    "        else:\n",
    "            res = self.cs_dict[name]\n",
    "        return res\n",
    "\n",
    "    def reverse_cs(self, name, x):\n",
    "        vals = self.cs_dict[name]\n",
    "\n",
    "        if type(x) == type(vals[0]):\n",
    "            pass\n",
    "        elif type(x) == torch.Tensor:\n",
    "            # convert to pytorch\n",
    "            vals = [torch.from_numpy(e) for e in vals]\n",
    "        elif type(res) == np.ndarray:\n",
    "            # convert to numpy\n",
    "            vals = [torch.Tensor.numpy(e) for e in vals]\n",
    "\n",
    "        center, scale = vals\n",
    "        res = (res * scale) + center\n",
    "        return res\n",
    "\n",
    "    \n",
    "    def get(self, name, ops_string = ''):\n",
    "        # is split info being requested? (for lookup_obs most likely). Otherwise main data is being requested.\n",
    "        if name not in ['val:train', 'test:train', 'val:test', 'test:test']:\n",
    "            if name not in self.data_dict.keys():\n",
    "                self.load(name, store=True)\n",
    "            res = self.data_dict[name]\n",
    "        else: \n",
    "            which_dict, which_split = name.split(':')\n",
    "\n",
    "            if which_split == 'train':  key = 'train_idx'\n",
    "            elif which_split == 'test': key = 'test_idx'\n",
    "            else: print('only `train` and `test` indexes are allowed.')\n",
    "\n",
    "            if which_dict == 'val':    res = self.val_dict[key]\n",
    "            elif which_dict == 'test': res = self.test_dict[key]\n",
    "            else: print('only `val` and `test` sets are allowed.')\n",
    "            \n",
    "        # apply opperations\n",
    "        ops_string = [e for e in ops_string.split(' ') if e != '']\n",
    "        for ops in ops_string:\n",
    "            if ops[0:6] == 'filter':\n",
    "                _, which_dict, which_split = ops.split(':')\n",
    "\n",
    "                if which_split == 'train':  key = 'train_idx'\n",
    "                elif which_split == 'test': key = 'test_idx'\n",
    "                else: print('only `train` and `test` indexes are allowed.')\n",
    "\n",
    "                if which_dict == 'val':    res_idx = self.val_dict[key]\n",
    "                elif which_dict == 'test': res_idx = self.test_dict[key]\n",
    "                else: print('only `val` and `test` sets are allowed.')\n",
    "\n",
    "                res = res[res_idx]\n",
    "\n",
    "            if ops == 'cs':\n",
    "                res = self.apply_cs(name)\n",
    "            \n",
    "            if ops == 'asarray':\n",
    "                res = np.asarray(res)\n",
    "            if ops == 'from_numpy':\n",
    "                res = torch.from_numpy(res)\n",
    "            if ops == 'float':\n",
    "                res = res.to(torch.float)\n",
    "            if ops[0:4] == 'cuda':\n",
    "                # send to device by number. e.g. cuda:0 -> X.to(0)\n",
    "                res = res.to(int(ops.split(':')[-1]))\n",
    "\n",
    "        return res\n",
    "\n",
    "\n",
    "# some example usage \n",
    "# X = g2fc_datawrapper()\n",
    "# X.set_split()\n",
    "# X.load_all(name_list = ['obs_env_lookup', 'YMat', 'PlantHarvest', 'WMat',], store=True) \n",
    "# X.calc_cs('YMat', version = 'np', filter = 'val:train'); X.cs_dict['YMat']\n",
    "# X.calc_cs_all(['YMat'], version = 'np', filter = 'val:train'); X.cs_dict['YMat']\n",
    "# X.calc_cs('YMat', version = 'np'); X.cs_dict['YMat']\n",
    "\n",
    "# some demonstration of when to use kwargs for scaling \n",
    "# how do I manually do scaling for enviromental things?\n",
    "# X.store_cs('WMat', calc_cs(X.get('WMat')[np.array(list(set(X.get('obs_env_lookup', ops_string='filter:val:train')[:, 1]))),: ,:]))\n",
    "# [e[0:3, 0] for e in X.cs_dict['WMat']]\n",
    "# X.calc_cs('WMat', filter = 'val:train', filter_lookup= 'obs_env_lookup')\n",
    "# [e[0:3, 0] for e in X.cs_dict['WMat']]\n",
    "# X.calc_cs('WMat')\n",
    "# [e[0:3, 0] for e in X.cs_dict['WMat']]\n",
    "\n",
    "# X.get('WMat', ops_string='asarray')[0:3, 0:3, 0]\n",
    "# X.get('WMat', ops_string='cs asarray')[0:3, 0:3, 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b0972b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9adb6aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73648362",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2155ccdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a980bdc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdce66c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444bd667",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c83618e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562d15a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0f986d6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a5f1ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26867472",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3723f05e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c9b06b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08098c5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "abbfd02f",
   "metadata": {},
   "source": [
    "## Functions for Visible Neural Nets (y first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404764b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def train_loop_yx(dataloader, model, loss_fn, optimizer, silent = False):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (y_i, xs_i) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(xs_i)\n",
    "        \n",
    "        # ensure both are on cuda\n",
    "        if pred.device.type == 'cpu':\n",
    "            pred = pred.to('cuda')\n",
    "        if y_i.device.type == 'cpu':\n",
    "            y_i = y_i.to('cuda')\n",
    "        \n",
    "        loss = loss_fn(pred, y_i)\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(y_i) \n",
    "            if not silent:\n",
    "                print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebfd2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def train_error_yx(dataloader, model, loss_fn, silent = False):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    train_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for y_i, xs_i in dataloader:\n",
    "            pred = model(xs_i)\n",
    "            \n",
    "            # ensure both are on cuda\n",
    "            if pred.device.type == 'cpu':\n",
    "                pred = pred.to('cuda')\n",
    "            if y_i.device.type == 'cpu':\n",
    "                y_i = y_i.to('cuda')\n",
    "            \n",
    "            train_loss += loss_fn(pred, y_i).item()\n",
    "            \n",
    "    train_loss /= num_batches\n",
    "    return(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857d493b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def test_loop_yx(dataloader, model, loss_fn, silent = False):   \n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for y_i, xs_i in dataloader:\n",
    "            pred = model(xs_i)\n",
    "            \n",
    "            # ensure both are on cuda\n",
    "            if pred.device.type == 'cpu':\n",
    "                pred = pred.to('cuda')\n",
    "            if y_i.device.type == 'cpu':\n",
    "                y_i = y_i.to('cuda')\n",
    "                \n",
    "            test_loss += loss_fn(pred, y_i).item() \n",
    "\n",
    "    test_loss /= num_batches\n",
    "    if not silent:\n",
    "        print(f\"Test Error: Avg loss: {test_loss:>8f}\")\n",
    "    return(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79c78d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def train_nn_yx(\n",
    "    cache_path,\n",
    "    training_dataloader,\n",
    "    testing_dataloader,\n",
    "    model,\n",
    "    batch_size = 64,\n",
    "    epochs = 500,\n",
    "    model_prefix = 'model',\n",
    "    save_model = False,\n",
    "    **kwargs # can include 'silent' for train loop or 'save_on' for saving frequency\n",
    "):   \n",
    "    if 'optimizer' not in kwargs:\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=kwargs['learning_rate'])\n",
    "    else:\n",
    "        optimizer = kwargs['optimizer']\n",
    "        \n",
    "    if 'save_on' in kwargs:\n",
    "        save_on = kwargs['save_on']\n",
    "    else:\n",
    "        save_on = 5       \n",
    "    \n",
    "    # Initialize the loss function\n",
    "    loss_fn = nn.MSELoss()     \n",
    "\n",
    "    loss_df = pd.DataFrame([i for i in range(epochs)], columns = ['Epoch'])\n",
    "    loss_df['TrainMSE'] = np.nan\n",
    "    loss_df['TestMSE']  = np.nan\n",
    "\n",
    "    for t in tqdm(range(epochs)):        \n",
    "        if 'silent' in kwargs:\n",
    "            train_loop_yx(training_dataloader, model, loss_fn, optimizer, silent = kwargs['silent'])\n",
    "        else:\n",
    "            train_loop_yx(training_dataloader, model, loss_fn, optimizer, silent = True)\n",
    "\n",
    "        loss_df.loc[loss_df.index == t, 'TrainMSE'\n",
    "                   ] = train_error_yx(training_dataloader, model, loss_fn, silent = True)\n",
    "        \n",
    "        loss_df.loc[loss_df.index == t, 'TestMSE'\n",
    "                   ] = test_loop_yx(testing_dataloader, model, loss_fn, silent = True)\n",
    "        \n",
    "        if (t+1)%save_on == 0: # Cache in case training is interupted. \n",
    "            if save_model:\n",
    "                torch.save(model.state_dict(), \n",
    "                           cache_path+'/'+model_prefix+'_'+str(t)+'_'+str(epochs)+'.pt') # convention is to use .pt or .pth\n",
    "        \n",
    "    return([model, loss_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccbe2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def yhat_loop_yx(dataloader, model):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    \n",
    "    y_true = np.array([])\n",
    "    y_pred = np.array([])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for y_i, xs_i in dataloader:\n",
    "            yhat_i = model(xs_i)\n",
    "            y_pred = np.append(y_pred, np.array(yhat_i.cpu()))\n",
    "            y_true = np.append(y_true, np.array(y_i.cpu()))\n",
    "    \n",
    "    out = np.concatenate([y_true[:, None], y_pred[:, None]], axis = 1) \n",
    "    out = pd.DataFrame(out, columns = ['y_true', 'y_pred'])\n",
    "    return(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b556ceeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1711e096",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00dceeb3",
   "metadata": {},
   "source": [
    "## Functions from multi-trait output tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eeb862b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| export\n",
    "\n",
    "# def train_loop3(dataloader, model, loss_fn, optimizer, silent = False):\n",
    "#     \"This is a version of train_loop which concatenates three ys.\"\n",
    "#     import torch\n",
    "#     from torch.utils.data import Dataset\n",
    "#     from torch.utils.data import DataLoader\n",
    "#     size = len(dataloader.dataset)\n",
    "#     for batch, (xs_i, y1_i, y2_i, y3_i) in enumerate(dataloader):\n",
    "#         # Compute prediction and loss\n",
    "#         pred = model(xs_i)\n",
    "#         loss = loss_fn(pred, torch.concat([y1_i, y2_i, y3_i], axis = 1)) # <----------------------------------------\n",
    "\n",
    "#         # Backpropagation\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         if batch % 100 == 0:\n",
    "#             loss, current = loss.item(), batch * len(y1_i) # <----------------\n",
    "#             if not silent:\n",
    "#                 print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef067bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0904feee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| export\n",
    "\n",
    "# def train_error3(dataloader, model, loss_fn, silent = False):\n",
    "#     import torch\n",
    "#     from torch.utils.data import Dataset\n",
    "#     from torch.utils.data import DataLoader\n",
    "#     size = len(dataloader.dataset)\n",
    "#     num_batches = len(dataloader)\n",
    "#     train_loss = 0\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for xs_i, y1_i, y2_i, y3_i in dataloader:\n",
    "#             pred = model(xs_i)\n",
    "#             train_loss += loss_fn(pred, torch.concat([y1_i, y2_i, y3_i], axis = 1)).item() # <----------------------\n",
    "            \n",
    "#     train_loss /= num_batches\n",
    "#     return(train_loss) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9f0fd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d9cf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| export\n",
    "\n",
    "# def test_loop3(dataloader, model, loss_fn, silent = False):\n",
    "#     import torch\n",
    "#     from torch.utils.data import Dataset\n",
    "#     from torch.utils.data import DataLoader\n",
    "\n",
    "#     size = len(dataloader.dataset)\n",
    "#     num_batches = len(dataloader)\n",
    "#     test_loss = 0\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for xs_i, y1_i, y2_i, y3_i in dataloader:\n",
    "#             pred = model(xs_i)\n",
    "#             test_loss += loss_fn(pred, torch.concat([y1_i, y2_i, y3_i], axis = 1)).item() # <-----------------------\n",
    "\n",
    "#     test_loss /= num_batches\n",
    "#     if not silent:\n",
    "#         print(f\"Test Error: Avg loss: {test_loss:>8f}\")\n",
    "#     return(test_loss) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c78817",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6745d1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962ace40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| export\n",
    "\n",
    "# def yhat_loop3(dataloader, model):\n",
    "#     \"Version of yhat_loop that returns 3 ys\"\n",
    "#     import numpy as np\n",
    "#     import pandas as pd\n",
    "#     import torch\n",
    "#     size = len(dataloader.dataset)\n",
    "#     num_batches = len(dataloader)\n",
    "    \n",
    "#     first_loop = True\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for xs_i, y1_i, y2_i, y3_i in dataloader:\n",
    "#             yhat_i = model(xs_i)\n",
    "#             y_i = torch.concat([y1_i, y2_i, y3_i], axis = 1) # <-----------------------\n",
    "\n",
    "#             if first_loop:\n",
    "#                 y_pred = np.array(yhat_i.cpu())\n",
    "#                 y_true = np.array(y_i.cpu())\n",
    "#                 first_loop = False\n",
    "#             else:            \n",
    "#                 y_pred = np.concatenate([y_pred, np.array(yhat_i.cpu())])\n",
    "#                 y_true = np.concatenate([y_true, np.array(y_i.cpu())])\n",
    "                \n",
    "#     out = np.concatenate([y_true[:, :], y_pred[:, :]], axis = 1) \n",
    "#     out = pd.DataFrame(out, columns = ['y1_true', 'y2_true', 'y3_true', 'y1_pred', 'y2_pred', 'y3_pred'])\n",
    "#     return(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b659c93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
