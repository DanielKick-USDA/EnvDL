{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aadccc0",
   "metadata": {},
   "source": [
    "# G only KEGG based network architecture\n",
    "\n",
    "> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "392ee9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# from EnvDL.core import ensure_dir_path_exists \n",
    "from EnvDL.dlfn import g2fc_datawrapper#, BigDataset, plDNN_general\n",
    "# from EnvDL.dlfn import ResNet2d, BasicBlock2d\n",
    "# from EnvDL.dlfn import LSUV_\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F # F.mse_loss\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "\n",
    "# from EnvDL.dlfn import kegg_connections_build, kegg_connections_clean, kegg_connections_append_y_hat, kegg_connections_sanitize_names\n",
    "# from EnvDL.dlfn import VNNHelper, VisableNeuralNetwork, Linear_block_reps\n",
    "# from EnvDL.dlfn import plDNN_general, BigDataset\n",
    "# from EnvDL.dlfn import reverse_edge_dict, reverse_node_props\n",
    "# from EnvDL.dlfn import VNNVAEHelper, plVNNVAE\n",
    "# from EnvDL.dlfn import kegg_connections_build, kegg_connections_clean, kegg_connections_append_y_hat, kegg_connections_sanitize_names\n",
    "# from EnvDL.dlfn import VNNHelper, VisableNeuralNetwork, Linear_block_reps\n",
    "# from EnvDL.dlfn import ListDataset, plVNN\n",
    "# from EnvDL.dlfn import plDNN_general, BigDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f431ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b078cad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparsevnn.core import *\n",
    "from sparsevnn.qol  import ensure_dir_path_exists\n",
    "from sparsevnn.kegg import kegg_connections_build, kegg_connections_clean, kegg_connections_append_y_hat, kegg_connections_sanitize_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "302bf05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_path = '../nbs_artifacts/02.40_g2fc_G_ACGT_sparsevnn_baseline/'\n",
    "save_prefix = [e for e in cache_path.split('/') if e != ''][-1]\n",
    "\n",
    "# Run settings: \n",
    "max_epoch  = 200\n",
    "batch_size = 48\n",
    "batch_size = 256\n",
    "\n",
    "# VNN settings:\n",
    "default_out_nodes_inp   = 4 #  4\n",
    "default_out_nodes_edge  = 2 # 32\n",
    "default_out_nodes_out   = 1\n",
    "\n",
    "default_drop_nodes_inp  = 0.0\n",
    "default_drop_nodes_edge = 0.\n",
    "default_drop_nodes_out  = 0.0\n",
    "\n",
    "default_reps_nodes_inp  = 1\n",
    "default_reps_nodes_edge = 1\n",
    "default_reps_nodes_out  = 1\n",
    "\n",
    "max_epoch  = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce01a01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "use_gpu_num = 0\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if use_gpu_num in [0, 1]: \n",
    "    torch.cuda.set_device(use_gpu_num)\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6fba3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensure_dir_path_exists(dir_path = cache_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "199df0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from EnvDL.dlfn import kegg_connections_build, kegg_connections_clean, kegg_connections_append_y_hat, kegg_connections_sanitize_names\n",
    "# from EnvDL.dlfn import VNNHelper, VisableNeuralNetwork, Linear_block_reps\n",
    "# from EnvDL.dlfn import ListDataset, plVNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0452d33b",
   "metadata": {},
   "source": [
    "## Fit Using VNNHelper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed8e82fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and storing default `phno`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retaining 43.53%, 6067/13939 Entries\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Same setup as above to create kegg_gene_brite\n",
    "X = g2fc_datawrapper()\n",
    "X.set_split()\n",
    "X.load_all(name_list = ['obs_geno_lookup', 'YMat', 'KEGG_slices',], store=True) \n",
    "X.calc_cs('YMat', version = 'np', filter = 'val:train')\n",
    "ACGT_gene_slice_list =     X.get('KEGG_slices', ops_string='')\n",
    "parsed_kegg_gene_entries = X.get('KEGG_entries')\n",
    "\n",
    "\n",
    "# Restrict to only those with pathway\n",
    "kegg_gene_brite = [e for e in parsed_kegg_gene_entries if 'BRITE' in e.keys()]\n",
    "\n",
    "# also require to have a non-empty path\n",
    "kegg_gene_brite = [e for e in kegg_gene_brite if not e['BRITE']['BRITE_PATHS'] == []]\n",
    "\n",
    "print('Retaining '+ str(round(len(kegg_gene_brite)/len(parsed_kegg_gene_entries), 4)*100)+'%, '+str(len(kegg_gene_brite)\n",
    "    )+'/'+str(len(parsed_kegg_gene_entries)\n",
    "    )+' Entries'\n",
    "    )\n",
    "# kegg_gene_brite[1]['BRITE']['BRITE_PATHS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46dcf6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed node \"Others\"\n"
     ]
    }
   ],
   "source": [
    "kegg_connections = kegg_connections_build(kegg_gene_brite = kegg_gene_brite, \n",
    "                                          n_genes = 6067) \n",
    "kegg_connections = kegg_connections_clean(         kegg_connections = kegg_connections)\n",
    "kegg_connections = kegg_connections_append_y_hat(  kegg_connections = kegg_connections)\n",
    "kegg_connections = kegg_connections_sanitize_names(kegg_connections = kegg_connections, \n",
    "                                                   replace_chars = {'.':'_'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db7ac803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize helper for input nodes\n",
    "myvnn = VNNHelper(edge_dict = kegg_connections)\n",
    "\n",
    "myvnn.nodes_inp[0:10]\n",
    "\n",
    "# Get a mapping of brite names to tensor list index\n",
    "find_names = myvnn.nodes_inp # e.g. ['100383860', '100278565', ... ]\n",
    "lookup_dict = {}\n",
    "\n",
    "# the only difference lookup_dict and brite_node_to_list_idx_dict above is that this is made using the full set of genes in the list \n",
    "# whereas that is made using kegg_gene_brite which is a subset\n",
    "for i in range(len(parsed_kegg_gene_entries)):\n",
    "    if 'BRITE' not in parsed_kegg_gene_entries[i].keys():\n",
    "        pass\n",
    "    elif parsed_kegg_gene_entries[i]['BRITE']['BRITE_PATHS'] == []:\n",
    "        pass\n",
    "    else:\n",
    "        name = parsed_kegg_gene_entries[i]['BRITE']['BRITE_PATHS'][0][-1]\n",
    "        if name in find_names:\n",
    "            lookup_dict[name] = i\n",
    "# lookup_dict    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2afc0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if permuting gene identities\n",
    "# torch.manual_seed(5461)\n",
    "\n",
    "# keys = [e for e in lookup_dict.keys()]\n",
    "\n",
    "# # vals = [lookup_dict[e] for e in lookup_dict.keys()]\n",
    "# # dict(zip(keys, [int(i) for i in torch.randperm(len(keys))]))\n",
    "\n",
    "# idx = torch.tensor([lookup_dict[e] for e in myvnn.nodes_inp])\n",
    "# idx = idx[torch.randperm(idx.shape[0])]\n",
    "# idx = [int(i) for i in idx]\n",
    "# temp = dict(zip(myvnn.nodes_inp, idx))\n",
    "\n",
    "# randomized_lookup_dict = {}\n",
    "# for e in lookup_dict.keys():\n",
    "#     if e not in temp.keys():\n",
    "#         randomized_lookup_dict[e] = lookup_dict[e]\n",
    "#     else:\n",
    "#         randomized_lookup_dict[e] = temp[e]\n",
    "\n",
    "# lookup_dict = randomized_lookup_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4cd30b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "brite_node_to_list_idx_dict = {}\n",
    "for i in range(len(kegg_gene_brite)):\n",
    "    brite_node_to_list_idx_dict[str(kegg_gene_brite[i]['BRITE']['BRITE_PATHS'][0][-1])] = i        \n",
    "\n",
    "# Get the input sizes for the graph\n",
    "size_in_zip = zip(myvnn.nodes_inp, [np.prod(ACGT_gene_slice_list[lookup_dict[e]].shape[1:]) for e  in myvnn.nodes_inp])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fd0d299",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# init input node sizes\n",
    "myvnn.set_node_props(key = 'inp', node_val_zip = size_in_zip)\n",
    "\n",
    "# init node output sizes\n",
    "myvnn.set_node_props(key = 'out', node_val_zip = zip(myvnn.nodes_inp, [default_out_nodes_inp  for e in myvnn.nodes_inp]))\n",
    "myvnn.set_node_props(key = 'out', node_val_zip = zip(myvnn.nodes_edge,[default_out_nodes_edge for e in myvnn.nodes_edge]))\n",
    "myvnn.set_node_props(key = 'out', node_val_zip = zip(myvnn.nodes_out, [default_out_nodes_out  for e in myvnn.nodes_out]))\n",
    "\n",
    "\n",
    "# # options should be controlled by node_props\n",
    "myvnn.set_node_props(key = 'flatten', node_val_zip = zip(\n",
    "    myvnn.nodes_inp, \n",
    "    [True for e in myvnn.nodes_inp]))\n",
    "\n",
    "myvnn.set_node_props(key = 'reps', node_val_zip = zip(myvnn.nodes_inp, [default_reps_nodes_inp  for e in myvnn.nodes_inp]))\n",
    "myvnn.set_node_props(key = 'reps', node_val_zip = zip(myvnn.nodes_edge,[default_reps_nodes_edge for e in myvnn.nodes_edge]))\n",
    "myvnn.set_node_props(key = 'reps', node_val_zip = zip(myvnn.nodes_out, [default_reps_nodes_out  for e in myvnn.nodes_out]))\n",
    "\n",
    "myvnn.set_node_props(key = 'drop', node_val_zip = zip(myvnn.nodes_inp, [default_drop_nodes_inp  for e in myvnn.nodes_inp]))\n",
    "myvnn.set_node_props(key = 'drop', node_val_zip = zip(myvnn.nodes_edge,[default_drop_nodes_edge for e in myvnn.nodes_edge]))\n",
    "myvnn.set_node_props(key = 'drop', node_val_zip = zip(myvnn.nodes_out, [default_drop_nodes_out  for e in myvnn.nodes_out]))\n",
    "\n",
    "# init edge node input size (propagate forward input/edge outpus)\n",
    "myvnn.calc_edge_inp()\n",
    "\n",
    "# myvnn.mk_digraph(include = ['node_name', 'inp_size', 'out_size'])\n",
    "# myvnn.mk_digraph(include = [''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e23192c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from EnvDL.dlfn import plDNN_general, BigDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "742569ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = X.get('KEGG_slices', ops_string='asarray from_numpy float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a15d4301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restrict to the tensors that will be used\n",
    "vals = [vals[lookup_dict[i]] for i in myvnn.nodes_inp] #TODO CONFIRM.\n",
    "# send to gpu\n",
    "# vals = [val.to('cuda') for val in vals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1f2556d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace lookup so that it matches the lenght of the input tensors\n",
    "new_lookup_dict = {}\n",
    "for i in range(len(myvnn.nodes_inp)):\n",
    "    new_lookup_dict[myvnn.nodes_inp[i]] = i\n",
    "    # print((myvnn.nodes_inp[i], i))\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b75472d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## start insert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68c8d1c",
   "metadata": {},
   "source": [
    "### Calculate nodes membership in each matrix and positions within each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a2ea8763",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_props = myvnn.node_props\n",
    "# Linear_block = Linear_block_reps,\n",
    "edge_dict = myvnn.edge_dict\n",
    "dependancy_order = myvnn.dependancy_order\n",
    "node_to_inp_num_dict = new_lookup_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b506f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check dep order\n",
    "\n",
    "tally = []\n",
    "for d in dependancy_order:\n",
    "    if edge_dict[d] == []:\n",
    "        tally.append(d)\n",
    "    elif False not in [True if e in tally else False for e in edge_dict[d]]:\n",
    "        tally.append(d)\n",
    "    else:\n",
    "        print('error!')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b6f9a4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build output nodes \n",
    "d_out = {0:[]}\n",
    "for d in dependancy_order:\n",
    "    if edge_dict[d] == []:\n",
    "        d_out[min(d_out.keys())].append(d)\n",
    "    else:\n",
    "        # print((d, edge_dict[d]))\n",
    "\n",
    "        d_out_i = 1+max(sum([[key for key in d_out.keys() if e in d_out[key]]\n",
    "                   for e in edge_dict[d]], []))\n",
    "        \n",
    "        if d_out_i not in d_out.keys():\n",
    "            d_out[d_out_i] = []\n",
    "        d_out[d_out_i].append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ee02f8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[58, 55, 106, 89, 105, 43, 0]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build input nodes NOPE. THE PASSHTROUGHS! \n",
    "d_eye = {}\n",
    "tally = []\n",
    "for i in range(max(d_out.keys()), min(d_out.keys()), -1):\n",
    "    # print(i)\n",
    "    nodes_needed = sum([edge_dict[e] for e in d_out[i]], [])+tally\n",
    "    # check against what is there and then dedupe\n",
    "    nodes_needed = [e for e in nodes_needed if e not in d_out[i-1]]\n",
    "    nodes_needed = list(set(nodes_needed))\n",
    "    tally = nodes_needed\n",
    "    d_eye[i] = nodes_needed\n",
    "\n",
    "# d_inp[0]= d_out[0]\n",
    "    \n",
    "[len(d_eye[i]) for i in d_eye.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8b883050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 6067), (1, 2167), (2, 447), (3, 143), (4, 25), (5, 8), (6, 10), (7, 1)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(key, len(d_out[key])) for key in d_out.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "40581c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = {}\n",
    "for i in d_eye.keys():\n",
    "    dd[i] = {'out': d_out[i],\n",
    "             'inp': d_out[i-1],\n",
    "             'eye': d_eye[i]}\n",
    "# plus special 0 layer that handles the snps\n",
    "    \n",
    "dd[0] = {'out': d_out[0],\n",
    "         'inp': d_out[0],\n",
    "         'eye': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8f11f1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that the output nodes' inputs are satisfied by the same layer's inputs (inp and eye)\n",
    "\n",
    "for i in dd.keys():\n",
    "    # out node in each\n",
    "    for e in dd[i]['out']:\n",
    "        # node depends in inp/eye\n",
    "        node_pass_list = [True if ee in dd[i]['inp']+dd[i]['eye'] else False \n",
    "                          for ee in edge_dict[e]]\n",
    "        if False not in node_pass_list:\n",
    "            pass\n",
    "        else:\n",
    "            print('exit') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "227b2c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer\t#In\t#Out\n",
      "0:\t24268\t24268\n",
      "1:\t24268\t4506\n",
      "2:\t4506\t1104\n",
      "3:\t1104\t464\n",
      "4:\t464\t262\n",
      "5:\t262\t126\n",
      "6:\t126\t136\n",
      "7:\t136\t1\n"
     ]
    }
   ],
   "source": [
    "print(\"Layer\\t#In\\t#Out\")\n",
    "for i in range(min(dd.keys()), max(dd.keys())+1, 1):\n",
    "    node_in      = [node_props[e]['out'] for e in dd[i]['inp']+dd[i  ]['eye'] ]\n",
    "    if i == max(dd.keys()):\n",
    "        node_out = [node_props[e]['out'] for e in dd[i]['out'] ]\n",
    "    else:\n",
    "        node_out = [node_props[e]['out'] for e in dd[i]['out']+dd[i+1]['eye']]\n",
    "    print(f'{i}:\\t{sum(node_in)}\\t{sum(node_out)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c249b2f",
   "metadata": {},
   "source": [
    "### Creating Structured Matrices for Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "31998d87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([7, 6, 5, 4, 3, 2, 1, 0])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8b4a7464",
   "metadata": {},
   "outputs": [],
   "source": [
    "M_list = [structured_layer_info(i = ii, node_groups = dd, node_props= node_props, edge_dict = edge_dict, as_sparse=True) for ii in range(0, max(dd.keys())+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4d0879ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([24268, 144468]),\n",
       " torch.Size([4506, 24268]),\n",
       " torch.Size([1104, 4506]),\n",
       " torch.Size([464, 1104]),\n",
       " torch.Size([262, 464]),\n",
       " torch.Size([126, 262]),\n",
       " torch.Size([136, 126]),\n",
       " torch.Size([1, 136])]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[e.weight.shape for e in M_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99467119",
   "metadata": {},
   "source": [
    "### Setup Dataloader using `M_list`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c72d9d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = X.get('KEGG_slices', ops_string='asarray from_numpy float')\n",
    "# restrict to the tensors that will be used\n",
    "vals = torch.concat([vals[lookup_dict[i]].reshape(4926, -1) \n",
    "                     for i in M_list[0].row_inp\n",
    "                    #  for i in dd[0]['inp'] # matches\n",
    "                     ], axis = 1)\n",
    "vals.shape\n",
    "vals = vals.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "114a4feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataloader = DataLoader(BigDataset(\n",
    "    lookups_are_filtered = True,\n",
    "    lookup_obs =  X.get('val:train',       ops_string='                   asarray from_numpy'), \n",
    "    lookup_geno = X.get('obs_geno_lookup', ops_string='   filter:val:train asarray from_numpy'),\n",
    "    y =           X.get('YMat',            ops_string='cs filter:val:train asarray from_numpy float cuda:0')[:, None],\n",
    "    # y =           X.get('YMat',            ops_string='cs filter:val:train asarray from_numpy float')[:, None],\n",
    "    G =           vals,\n",
    "    G_type = 'raw',\n",
    "    # send_batch_to_gpu = 'cuda:0'\n",
    "    ),\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "validation_dataloader = DataLoader(BigDataset(\n",
    "    lookups_are_filtered = True,\n",
    "    lookup_obs =  X.get('val:test',        ops_string='                   asarray from_numpy'), \n",
    "    lookup_geno = X.get('obs_geno_lookup', ops_string='   filter:val:test asarray from_numpy'),\n",
    "    y =           X.get('YMat',            ops_string='cs filter:val:test asarray from_numpy float cuda:0')[:, None],\n",
    "    G =           vals,\n",
    "    G_type = 'raw',\n",
    "    # send_batch_to_gpu = 'cuda:0'\n",
    "    ),\n",
    "    batch_size = batch_size,\n",
    "    shuffle = False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "afdf63ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version to predict enviromental residuals?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2dcbd1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_from = '../nbs_artifacts/01.03_g2fc_prep_matrices/'\n",
    "# load_from = '../nbs_artifacts/01.03_g2fc_prep_matrices/'\n",
    "# phno_geno = pd.read_csv(load_from+'phno_geno.csv')\n",
    "# phno = phno_geno\n",
    "\n",
    "\n",
    "# obs_geno_lookup = np.load(load_from+'obs_geno_lookup.npy') # Phno_Idx  Geno_Idx  Is_Phno_Idx\n",
    "# obs_env_lookup = np.load(load_from+'obs_env_lookup.npy')   # Phno_Idx  Env_Idx   Is_Phno_Idx\n",
    "# YMat = np.load(load_from+'YMat.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8fcc3ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from EnvDL.dlfn import * \n",
    "# from EnvDL.dlfn import read_split_info, find_idxs_split_dict, calc_cs, apply_cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b0df08ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ## Create train/test validate indicies from json\n",
    "# load_from = '../nbs_artifacts/01.06_g2fc_cluster_genotypes/'\n",
    "\n",
    "# split_info = read_split_info(\n",
    "#     load_from = '../nbs_artifacts/01.06_g2fc_cluster_genotypes/',\n",
    "#     json_prefix = '2023:9:5:12:8:26')\n",
    "\n",
    "# temp = phno.copy()\n",
    "# temp[['Female', 'Male']] = temp['Hybrid'].str.split('/', expand = True)\n",
    "\n",
    "# test_dict = find_idxs_split_dict(\n",
    "#     obs_df = temp, \n",
    "#     split_dict = split_info['test'][0]\n",
    "# )\n",
    "# # test_dict\n",
    "\n",
    "# # since this is applying predefined model structure no need for validation.\n",
    "# # This is included for my future reference when validation is needed.\n",
    "# temp = temp.loc[test_dict['train_idx'], ] # restrict before re-aplying\n",
    "\n",
    "# val_dict = find_idxs_split_dict(\n",
    "#     obs_df = temp, \n",
    "#     split_dict = split_info['validate'][0]\n",
    "# )\n",
    "# # val_dict\n",
    "\n",
    "# # test_dict\n",
    "\n",
    "# train_idx = test_dict['train_idx']\n",
    "# test_idx  = test_dict['test_idx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a27290fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0452836b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Process data to get env means\n",
    "# # obs_env_lookup   # Phno_Idx  Env_Idx   Is_Phno_Idx\n",
    "\n",
    "# YMat_EnvMean = YMat.copy()\n",
    "\n",
    "# for i in tqdm(list(set(obs_env_lookup[:, 1]))):\n",
    "#     mask = (obs_env_lookup[:, 1] == i)\n",
    "#     YMat_EnvMean[mask] = YMat_EnvMean[mask].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bee3bd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # subtract to get residuals\n",
    "# YMat = YMat - YMat_EnvMean\n",
    "# # proceed as normal..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc249fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a84f6ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YMat_cs = calc_cs(YMat[train_idx])\n",
    "# y_cs = apply_cs(YMat, YMat_cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "394f7d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_temp = torch.from_numpy(y_cs).to(torch.float)#[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4d52ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6e9e7cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_dataloader = DataLoader(BigDataset(\n",
    "#     lookups_are_filtered = True,\n",
    "#     lookup_obs =  X.get('val:train',       ops_string='                   asarray from_numpy'), \n",
    "#     lookup_geno = X.get('obs_geno_lookup', ops_string='   filter:val:train asarray from_numpy'),\n",
    "#     y =           y_temp[train_idx][:, None].to('cuda'),\n",
    "#     G =           vals,\n",
    "#     G_type = 'raw',\n",
    "#     # send_batch_to_gpu = 'cuda:0'\n",
    "#     ),\n",
    "#     batch_size = batch_size,\n",
    "#     shuffle = True\n",
    "# )\n",
    "\n",
    "# validation_dataloader = DataLoader(BigDataset(\n",
    "#     lookups_are_filtered = True,\n",
    "#     lookup_obs =  X.get('val:test',        ops_string='                   asarray from_numpy'), \n",
    "#     lookup_geno = X.get('obs_geno_lookup', ops_string='   filter:val:test asarray from_numpy'),\n",
    "#     y =           y_temp[test_idx][:, None].to('cuda'),\n",
    "#     G =           vals,\n",
    "#     G_type = 'raw',\n",
    "#     # send_batch_to_gpu = 'cuda:0'\n",
    "#     ),\n",
    "#     batch_size = batch_size,\n",
    "#     shuffle = False\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e712e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "719f62b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_dataloader = DataLoader(BigDataset(\n",
    "#     lookups_are_filtered = True,\n",
    "#     lookup_obs =  X.get('val:train',       ops_string='                   asarray from_numpy'), \n",
    "#     lookup_geno = X.get('obs_geno_lookup', ops_string='   filter:test:train asarray from_numpy'),\n",
    "#     y =           X.get('YMat',            ops_string='cs filter:test:train asarray from_numpy float cuda:0')[:, None],\n",
    "#     # y =           X.get('YMat',            ops_string='cs filter:val:train asarray from_numpy float')[:, None],\n",
    "#     G =           vals,\n",
    "#     G_type = 'raw',\n",
    "#     # send_batch_to_gpu = 'cuda:0'\n",
    "#     ),\n",
    "#     batch_size = batch_size,\n",
    "#     shuffle = True\n",
    "# )\n",
    "\n",
    "# validation_dataloader = DataLoader(BigDataset(\n",
    "#     lookups_are_filtered = True,\n",
    "#     lookup_obs =  X.get('val:test',        ops_string='                   asarray from_numpy'), \n",
    "#     lookup_geno = X.get('obs_geno_lookup', ops_string='   filter:test:test asarray from_numpy'),\n",
    "#     y =           X.get('YMat',            ops_string='cs filter:test:test asarray from_numpy float cuda:0')[:, None],\n",
    "#     G =           vals,\n",
    "#     G_type = 'raw',\n",
    "#     # send_batch_to_gpu = 'cuda:0'\n",
    "#     ),\n",
    "#     batch_size = batch_size,\n",
    "#     shuffle = False\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66553003",
   "metadata": {},
   "source": [
    "## Structured Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "50541bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_list):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.layer_list = nn.ModuleList(layer_list)\n",
    " \n",
    "    def forward(self, x):\n",
    "        for l in self.layer_list:\n",
    "            x = l(x)\n",
    "        return x\n",
    "\n",
    "layer_list = []\n",
    "for i in range(len(M_list)):\n",
    "    \n",
    "    apply_relu = None\n",
    "    if i+1 != len(M_list): # apply relu to all but the last layer\n",
    "        apply_relu = F.relu\n",
    "    \n",
    "\n",
    "    l = SparseLinearCustom(\n",
    "        M_list[i].weight.shape[1], # have to transpose this?\n",
    "        M_list[i].weight.shape[0],\n",
    "        connectivity   = torch.LongTensor(M_list[i].weight.coalesce().indices()),\n",
    "        custom_weights = M_list[i].weight.coalesce().values(), \n",
    "        custom_bias    = M_list[i].bias.clone().detach(), \n",
    "        weight_grad_bool = M_list[i].weight_grad_bool, \n",
    "        bias_grad_bool   = M_list[i].bias_grad_bool, #.to_sparse()#.indices()\n",
    "        dropout_p        = M_list[i].dropout_p,\n",
    "        nonlinear_transform= apply_relu\n",
    "        )\n",
    "\n",
    "    layer_list += [l]\n",
    "\n",
    "model = NeuralNetwork(layer_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fac098f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Training\n",
    "\n",
    "# VNN = plDNN_general(model)  \n",
    "\n",
    "# optimizer = VNN.configure_optimizers()\n",
    "\n",
    "# logger = TensorBoardLogger(\"tb_vnnsparse_logs\", name='tb_sparsevnn_baseline')\n",
    "# trainer = pl.Trainer(max_epoch=max_epoch, logger=logger)\n",
    "\n",
    "# trainer.fit(model=VNN, train_dataloaders=training_dataloader, val_dataloaders=validation_dataloader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8761cfe",
   "metadata": {},
   "source": [
    "# Intepretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "12141d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up gradients for testing\n",
    "\n",
    "yi, xi = next(iter(training_dataloader))\n",
    "yi = yi.to('cpu')\n",
    "xi = xi.to('cpu')\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "loss = loss_fn(model(xi), yi)\n",
    "optimizer.zero_grad()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "60c47b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find indices in a structured matrix that correspond to the in/out of a given node. \n",
    "\n",
    "def find_node_in_layer_info(\n",
    "    layer_info_list:list, # List where all elements are `sparsevnn.core.structured_layer_info`. \n",
    "    edge_dict:dict, # dictionary of graph edges. Needed to find the correct input dim. indices of the `sparsevnn.core.SparseLinearCustom`. \n",
    "    query:str, # name of the node to be identified\n",
    "):\n",
    "    # find the right index in the list:\n",
    "    list_idx = [i for i in range(len(layer_info_list)) if query in layer_info_list[i].col_out]\n",
    "    assert len(list_idx) == 1\n",
    "    list_idx = list_idx[0]\n",
    "\n",
    "    # if an edge or output\n",
    "    if edge_dict[query] != []:\n",
    "        rows_info = {e : layer_info_list[list_idx].row_info[e] for  e in edge_dict[query]}\n",
    "    else: # if input\n",
    "        rows_info = {e : layer_info_list[list_idx].row_info[e] for  e in [query]}\n",
    "\n",
    "    return {\n",
    "        'list_idx':  list_idx,\n",
    "        'col_info':  layer_info_list[list_idx].col_info[query],\n",
    "        'rows_info': rows_info\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dd60d20c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'list_idx': 5,\n",
       " 'col_info': {'size': tensor(2, dtype=torch.int32),\n",
       "  'stop': tensor(10),\n",
       "  'start': tensor(8)},\n",
       " 'rows_info': {'ProteinSerine/ThreoninePhosphatases': {'size': tensor(2, dtype=torch.int32),\n",
       "   'stop': tensor(20),\n",
       "   'start': tensor(18)},\n",
       "  'ProteinTyrosinePhosphatases(Ptps)': {'size': tensor(2, dtype=torch.int32),\n",
       "   'stop': tensor(124),\n",
       "   'start': tensor(122)},\n",
       "  'HadPhosphatases': {'size': tensor(2, dtype=torch.int32),\n",
       "   'stop': tensor(216),\n",
       "   'start': tensor(214)}}}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_node_in_layer_info(\n",
    "    layer_info_list = M_list,\n",
    "    edge_dict=edge_dict,\n",
    "    query = 'ProteinPhosphatasesAndAssociatedProteins[Br-Zma01009]'        \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a114e9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sparsevnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6845d465",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_SparseLinearCustom(\n",
    "        slc:sparsevnn.core.SparseLinearCustom, # SparseLinearCustom layer\n",
    "        inp_min:int = None, # Start slice index for input. See the `structured_layer_info`'s attribute `.row_info['start']`\n",
    "        inp_max:int = None, # Stop slice index for input. See the `structured_layer_info`'s attribute `.row_info['stop']`\n",
    "        out_min:int = None, # Start slice index for output. See the `structured_layer_info`'s attribute `.col_info['start']`\n",
    "        out_max:int = None, # Stop slice index for output. See the `structured_layer_info`'s attribute `.col_info['stop']`\n",
    "        weight_or_bias:str = 'weight', # Specifies if weights or bias should be considered.\n",
    "        grad:bool = False, # Return parameters or gradients from `slc`\n",
    "        ):    \n",
    "    \"Return subset of a `SparseLinearCustom` layers weights or bias. Retrieve either the parameter values or the gradient.\"\n",
    "    connectivity = slc.connectivity\n",
    "\n",
    "    if weight_or_bias == 'bias':\n",
    "            if grad == False:\n",
    "                    return slc.bias.grad[out_min:out_max]\n",
    "            if grad: \n",
    "                    return slc.bias[out_min:out_max]\n",
    "\n",
    "    if weight_or_bias == 'weight':\n",
    "        # 1 is in features\n",
    "        inp_select_bool = (slc.connectivity[1] >= inp_min) & (slc.connectivity[1] <= inp_max)\n",
    "        out_select_bool = (slc.connectivity[0] >= out_min) & (slc.connectivity[0] <= out_max)\n",
    "\n",
    "        select_bool = inp_select_bool & out_select_bool\n",
    "\n",
    "        new_con0 = slc.connectivity[0][select_bool]\n",
    "        new_con1 = slc.connectivity[1][select_bool]\n",
    "        new_con0 -= new_con0.min()\n",
    "        new_con1 -= new_con1.min()\n",
    "\n",
    "        i = torch.concat([\n",
    "        new_con0[None, :],\n",
    "        new_con1[None, :]], axis = 0)\n",
    "\n",
    "\n",
    "        if grad == False:\n",
    "              v = slc.weights[select_bool]\n",
    "\n",
    "        if grad == True:\n",
    "              v = slc.weights.grad[select_bool]\n",
    "\n",
    "        out_shape = (\n",
    "              int(1+new_con0.max()),\n",
    "              int(1+new_con1.max())\n",
    "              )\n",
    "\n",
    "\n",
    "        return torch.sparse_coo_tensor(indices = i, \n",
    "                                       values = v, \n",
    "                                       size = out_shape\n",
    "                                       ).to_dense()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "11cb133c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'list_idx': 7,\n",
       " 'col_info': {'size': tensor(1, dtype=torch.int32),\n",
       "  'stop': tensor(1),\n",
       "  'start': tensor(0)},\n",
       " 'rows_info': {'KeggOrthology(Ko)[Br-Zma00001]': {'size': tensor(2, dtype=torch.int32),\n",
       "   'stop': tensor(34),\n",
       "   'start': tensor(32)},\n",
       "  'Enzymes[Br-Zma01000]': {'size': tensor(2, dtype=torch.int32),\n",
       "   'stop': tensor(128),\n",
       "   'start': tensor(126)},\n",
       "  'TransferRnaBiogenesis[Br-Zma03016]': {'size': tensor(2, dtype=torch.int32),\n",
       "   'stop': tensor(20),\n",
       "   'start': tensor(18)},\n",
       "  'ChromosomeAndAssociatedProteins[Br-Zma03036]': {'size': tensor(2, dtype=torch.int32),\n",
       "   'stop': tensor(18),\n",
       "   'start': tensor(16)},\n",
       "  'MessengerRnaBiogenesis[Br-Zma03019]': {'size': tensor(2, dtype=torch.int32),\n",
       "   'stop': tensor(16),\n",
       "   'start': tensor(14)},\n",
       "  'SecretionSystem[Br-Zma02044]': {'size': tensor(2, dtype=torch.int32),\n",
       "   'stop': tensor(94),\n",
       "   'start': tensor(92)},\n",
       "  'Transporters[Br-Zma02000]': {'size': tensor(2, dtype=torch.int32),\n",
       "   'stop': tensor(36),\n",
       "   'start': tensor(34)},\n",
       "  'UbiquitinSystem[Br-Zma04121]': {'size': tensor(2, dtype=torch.int32),\n",
       "   'stop': tensor(70),\n",
       "   'start': tensor(68)},\n",
       "  'MembraneTrafficking[Br-Zma04131]': {'size': tensor(2, dtype=torch.int32),\n",
       "   'stop': tensor(90),\n",
       "   'start': tensor(88)},\n",
       "  'Exosome[Br-Zma04147]': {'size': tensor(2, dtype=torch.int32),\n",
       "   'stop': tensor(108),\n",
       "   'start': tensor(106)},\n",
       "  'TranscriptionFactors[Br-Zma03000]': {'size': tensor(2, dtype=torch.int32),\n",
       "   'stop': tensor(14),\n",
       "   'start': tensor(12)},\n",
       "  'Spliceosome[Br-Zma03041]': {'size': tensor(2, dtype=torch.int32),\n",
       "   'stop': tensor(78),\n",
       "   'start': tensor(76)},\n",
       "  'CytoskeletonProteins[Br-Zma04812]': {'size': tensor(2, dtype=torch.int32),\n",
       "   'stop': tensor(12),\n",
       "   'start': tensor(10)},\n",
       "  'MitochondrialBiogenesis[Br-Zma03029]': {'size': tensor(2, dtype=torch.int32),\n",
       "   'stop': tensor(134),\n",
       "   'start': tensor(132)},\n",
       "  'ChaperonesAndFoldingCatalysts[Br-Zma03110]': {'size': tensor(2, dtype=torch.int32),\n",
       "   'stop': tensor(92),\n",
       "   'start': tensor(90)},\n",
       "  'CiliumAndAssociatedProteins[Br-Zma03037]': {'size': tensor(2, dtype=torch.int32),\n",
       "   'stop': tensor(48),\n",
       "   'start': tensor(46)},\n",
       "  'TranscriptionMachinery[Br-Zma03021]': {'size': tensor(2, dtype=torch.int32),\n",
       "   'stop': tensor(10),\n",
       "   'start': tensor(8)},\n",
       "  'Glycosyltransferases[Br-Zma01003]': {'size': tensor(2, dtype=torch.int32),\n",
       "   'stop': tensor(52),\n",
       "   'start': tensor(50)},\n",
       "  'PhotosynthesisProteins[Br-Zma00194]': {'size': tensor(2, dtype=torch.int32),\n",
       "   'stop': tensor(32),\n",
       "   'start': tensor(30)},\n",
       "  'Ribosome[Br-Zma03011]': {'size': tensor(2, dtype=torch.int32),\n",
       "   'stop': tensor(110),\n",
       "   'start': tensor(108)},\n",
       "  'DnaReplicationProteins[Br-Zma03032]': {'size': tensor(2, dtype=torch.int32),\n",
       "   'stop': tensor(8),\n",
       "   'start': tensor(6)},\n",
       "  'DnaRepairAndRecombinationProteins[Br-Zma03400]': {'size': tensor(2, dtype=torch.int32),\n",
       "   'stop': tensor(6),\n",
       "   'start': tensor(4)},\n",
       "  'ProteinKinases[Br-Zma01001]': {'size': tensor(2, dtype=torch.int32),\n",
       "   'stop': tensor(74),\n",
       "   'start': tensor(72)},\n",
       "  'PlantMloReceptors': {'size': tensor(2, dtype=torch.int32),\n",
       "   'stop': tensor(126),\n",
       "   'start': tensor(124)},\n",
       "  'ProteinPhosphatasesAndAssociatedProteins[Br-Zma01009]': {'size': tensor(2, dtype=torch.int32),\n",
       "   'stop': tensor(106),\n",
       "   'start': tensor(104)},\n",
       "  'Proteasome[Br-Zma03051]': {'size': tensor(2, dtype=torch.int32),\n",
       "   'stop': tensor(122),\n",
       "   'start': tensor(120)},\n",
       "  'AminoAcidRelatedEnzymes[Br-Zma01007]': {'size': tensor(2, dtype=torch.int32),\n",
       "   'stop': tensor(58),\n",
       "   'start': tensor(56)},\n",
       "  'ProkaryoticDefenseSystem[Br-Zma02048]': {'size': tensor(2, dtype=torch.int32),\n",
       "   'stop': tensor(136),\n",
       "   'start': tensor(134)},\n",
       "  'Domain-ContainingProteinsNotElsewhereClassified[Br-Zma04990]': {'size': tensor(2, dtype=torch.int32),\n",
       "   'stop': tensor(82),\n",
       "   'start': tensor(80)},\n",
       "  'PeptidasesAndInhibitors[Br-Zma01002]': {'size': tensor(2, dtype=torch.int32),\n",
       "   'stop': tensor(132),\n",
       "   'start': tensor(130)},\n",
       "  'Prenyltransferases[Br-Zma01006]': {'size': tensor(2, dtype=torch.int32),\n",
       "   'stop': tensor(56),\n",
       "   'start': tensor(54)},\n",
       "  'RibosomeBiogenesis[Br-Zma03009]': {'size': tensor(2, dtype=torch.int32),\n",
       "   'stop': tensor(4),\n",
       "   'start': tensor(2)},\n",
       "  'CdMolecules[Br-Zma04090]': {'size': tensor(2, dtype=torch.int32),\n",
       "   'stop': tensor(76),\n",
       "   'start': tensor(74)},\n",
       "  'CytochromeP450[Br-Zma00199]': {'size': tensor(2, dtype=torch.int32),\n",
       "   'stop': tensor(80),\n",
       "   'start': tensor(78)},\n",
       "  'Gtp-BindingProteins[Br-Zma04031]': {'size': tensor(2, dtype=torch.int32),\n",
       "   'stop': tensor(118),\n",
       "   'start': tensor(116)},\n",
       "  'IonChannels[Br-Zma04040]': {'size': tensor(2, dtype=torch.int32),\n",
       "   'stop': tensor(88),\n",
       "   'start': tensor(86)},\n",
       "  'GlycosaminoglycanBindingProteins[Br-Zma00536]': {'size': tensor(2, dtype=torch.int32),\n",
       "   'stop': tensor(28),\n",
       "   'start': tensor(26)},\n",
       "  'Glycosylphosphatidylinositol(Gpi)-AnchoredProteins[Br-Zma00537]': {'size': tensor(2, dtype=torch.int32),\n",
       "   'stop': tensor(116),\n",
       "   'start': tensor(114)},\n",
       "  'LipopolysaccharideBiosynthesisProteins[Br-Zma01005]': {'size': tensor(2, dtype=torch.int32),\n",
       "   'stop': tensor(44),\n",
       "   'start': tensor(42)},\n",
       "  'Trehalose': {'size': tensor(2, dtype=torch.int32),\n",
       "   'stop': tensor(60),\n",
       "   'start': tensor(58)},\n",
       "  'TranslationFactors[Br-Zma03012]': {'size': tensor(2, dtype=torch.int32),\n",
       "   'stop': tensor(2),\n",
       "   'start': tensor(0)},\n",
       "  'YipfProteins': {'size': tensor(2, dtype=torch.int32),\n",
       "   'stop': tensor(22),\n",
       "   'start': tensor(20)},\n",
       "  'LipidBiosynthesisProteins[Br-Zma01004]': {'size': tensor(2, dtype=torch.int32),\n",
       "   'stop': tensor(42),\n",
       "   'start': tensor(40)},\n",
       "  'Efr3-Pi4KiiiAlphaComplexAndAssociatedProteins': {'size': tensor(2, dtype=torch.int32),\n",
       "   'stop': tensor(46),\n",
       "   'start': tensor(44)},\n",
       "  'RhoGtpaseAssociatedProteins': {'size': tensor(2, dtype=torch.int32),\n",
       "   'stop': tensor(72),\n",
       "   'start': tensor(70)},\n",
       "  'Ap-1Complex': {'size': tensor(2, dtype=torch.int32),\n",
       "   'stop': tensor(112),\n",
       "   'start': tensor(110)},\n",
       "  'WashComplex': {'size': tensor(2, dtype=torch.int32),\n",
       "   'stop': tensor(114),\n",
       "   'start': tensor(112)},\n",
       "  'EarpComplex': {'size': tensor(2, dtype=torch.int32),\n",
       "   'stop': tensor(50),\n",
       "   'start': tensor(48)},\n",
       "  'ErMembraneProteinComplex': {'size': tensor(2, dtype=torch.int32),\n",
       "   'stop': tensor(30),\n",
       "   'start': tensor(28)},\n",
       "  'RabGtpasesOfSpecificOrganelles': {'size': tensor(2, dtype=torch.int32),\n",
       "   'stop': tensor(120),\n",
       "   'start': tensor(118)},\n",
       "  'BacterialToxins[Br-Zma02042]': {'size': tensor(2, dtype=torch.int32),\n",
       "   'stop': tensor(68),\n",
       "   'start': tensor(66)},\n",
       "  'Ap-4Complex': {'size': tensor(2, dtype=torch.int32),\n",
       "   'stop': tensor(26),\n",
       "   'start': tensor(24)},\n",
       "  'Post-GpiAttachmentToProteinsFactors(Pgap)': {'size': tensor(2, dtype=torch.int32),\n",
       "   'stop': tensor(98),\n",
       "   'start': tensor(96)},\n",
       "  'Lectins[Br-Zma04091]': {'size': tensor(2, dtype=torch.int32),\n",
       "   'stop': tensor(130),\n",
       "   'start': tensor(128)},\n",
       "  'PeptidoglycanBiosynthesisAndDegradationProteins[Br-Zma01011]': {'size': tensor(2, dtype=torch.int32),\n",
       "   'stop': tensor(64),\n",
       "   'start': tensor(62)},\n",
       "  'HydrophobicMolecule': {'size': tensor(2, dtype=torch.int32),\n",
       "   'stop': tensor(96),\n",
       "   'start': tensor(94)},\n",
       "  'Ap-3Complex': {'size': tensor(2, dtype=torch.int32),\n",
       "   'stop': tensor(86),\n",
       "   'start': tensor(84)},\n",
       "  'Neuronostatin': {'size': tensor(2, dtype=torch.int32),\n",
       "   'stop': tensor(38),\n",
       "   'start': tensor(36)},\n",
       "  'Oxysterol-BindingProteins': {'size': tensor(2, dtype=torch.int32),\n",
       "   'stop': tensor(84),\n",
       "   'start': tensor(82)},\n",
       "  'OtherGlycosyltransferases': {'size': tensor(2, dtype=torch.int32),\n",
       "   'stop': tensor(124),\n",
       "   'start': tensor(122)},\n",
       "  'NadphOxidases(Nox)AndAssociatedProteins': {'size': tensor(2, dtype=torch.int32),\n",
       "   'stop': tensor(40),\n",
       "   'start': tensor(38)},\n",
       "  'RetrieverComplex': {'size': tensor(2, dtype=torch.int32),\n",
       "   'stop': tensor(102),\n",
       "   'start': tensor(100)},\n",
       "  'Non-CodingRnas[Br-Zma03100]': {'size': tensor(2, dtype=torch.int32),\n",
       "   'stop': tensor(24),\n",
       "   'start': tensor(22)},\n",
       "  'PolyketideBiosynthesisProteins[Br-Zma01008]': {'size': tensor(2, dtype=torch.int32),\n",
       "   'stop': tensor(62),\n",
       "   'start': tensor(60)},\n",
       "  'QualityControl': {'size': tensor(2, dtype=torch.int32),\n",
       "   'stop': tensor(100),\n",
       "   'start': tensor(98)},\n",
       "  'Bloc-1Complex': {'size': tensor(2, dtype=torch.int32),\n",
       "   'stop': tensor(66),\n",
       "   'start': tensor(64)},\n",
       "  'Proteoglycans[Br-Zma00535]': {'size': tensor(2, dtype=torch.int32),\n",
       "   'stop': tensor(104),\n",
       "   'start': tensor(102)},\n",
       "  'PatternRecognitionReceptors[Br-Zma04054]': {'size': tensor(2, dtype=torch.int32),\n",
       "   'stop': tensor(54),\n",
       "   'start': tensor(52)}}}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# input\n",
    "node_info = find_node_in_layer_info(\n",
    "    layer_info_list = M_list,\n",
    "    edge_dict=edge_dict,\n",
    "    query = '103646242'\n",
    ")\n",
    "\n",
    "\n",
    "# edge \n",
    "# node_info = find_node_in_layer_info(\n",
    "#     layer_info_list = M_list,\n",
    "#     edge_dict=edge_dict,\n",
    "#     query = 'ProteinPhosphatasesAndAssociatedProteins[Br-Zma01009]'        \n",
    "# )\n",
    "\n",
    "node_info = find_node_in_layer_info(\n",
    "    layer_info_list = M_list,\n",
    "    edge_dict=edge_dict,\n",
    "    query = 'y_hat'        \n",
    ")\n",
    "\n",
    "node_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680281f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353261cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1be5c161",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_slice_SparseLinearCustom(\n",
    "    # for `find_node_in_layer_info`\n",
    "    layer_info_list:list, # List where all elements are `sparsevnn.core.structured_layer_info`. \n",
    "    edge_dict:dict, # dictionary of graph edges. Needed to find the correct input dim. indices of the `sparsevnn.core.SparseLinearCustom`. \n",
    "    query:str, # name of the node to be identified\n",
    "    # additional parameter for `slice_SparseLinearCustom`\n",
    "    neural_network_layer_list, # Neural networks's ModuleList which contains `SparseLinearCustom` layers\n",
    "    weight_or_bias:str = 'weight', # Specifies if weights or bias should be considered.\n",
    "    grad:bool = False, # Return parameters or gradients from `slc`\n",
    "    ):\n",
    "    \"Wraps `find_node_in_layer_info` and `slice_SparseLinearCustom` to get all the requested values for each edge\"\n",
    "    node_info = find_node_in_layer_info(\n",
    "        layer_info_list = layer_info_list,\n",
    "        edge_dict=edge_dict,\n",
    "        query = query\n",
    "        )\n",
    "    \n",
    "    out_list = []\n",
    "    out_list_nodes = []\n",
    "    in_nodes = node_info['rows_info'].keys()\n",
    "    for in_node in in_nodes:\n",
    "        e = node_info['rows_info'][in_node]\n",
    "        e_out = slice_SparseLinearCustom(\n",
    "                slc = neural_network_layer_list[node_info['list_idx']],\n",
    "                inp_min = e['start'],\n",
    "                inp_max = e['stop'],\n",
    "                out_min = node_info['col_info']['start'],\n",
    "                out_max = node_info['col_info']['stop'],\n",
    "                weight_or_bias = weight_or_bias, \n",
    "                grad = grad\n",
    "                )\n",
    "        \n",
    "        out_list += [e_out]\n",
    "        out_list_nodes += [in_node]\n",
    "\n",
    "    return {\n",
    "        'node_names': out_list_nodes, \n",
    "        'node_vals': out_list\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0339c897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'node_names': ['KeggOrthology(Ko)[Br-Zma00001]',\n",
       "  'Enzymes[Br-Zma01000]',\n",
       "  'TransferRnaBiogenesis[Br-Zma03016]',\n",
       "  'ChromosomeAndAssociatedProteins[Br-Zma03036]',\n",
       "  'MessengerRnaBiogenesis[Br-Zma03019]',\n",
       "  'SecretionSystem[Br-Zma02044]',\n",
       "  'Transporters[Br-Zma02000]',\n",
       "  'UbiquitinSystem[Br-Zma04121]',\n",
       "  'MembraneTrafficking[Br-Zma04131]',\n",
       "  'Exosome[Br-Zma04147]',\n",
       "  'TranscriptionFactors[Br-Zma03000]',\n",
       "  'Spliceosome[Br-Zma03041]',\n",
       "  'CytoskeletonProteins[Br-Zma04812]',\n",
       "  'MitochondrialBiogenesis[Br-Zma03029]',\n",
       "  'ChaperonesAndFoldingCatalysts[Br-Zma03110]',\n",
       "  'CiliumAndAssociatedProteins[Br-Zma03037]',\n",
       "  'TranscriptionMachinery[Br-Zma03021]',\n",
       "  'Glycosyltransferases[Br-Zma01003]',\n",
       "  'PhotosynthesisProteins[Br-Zma00194]',\n",
       "  'Ribosome[Br-Zma03011]',\n",
       "  'DnaReplicationProteins[Br-Zma03032]',\n",
       "  'DnaRepairAndRecombinationProteins[Br-Zma03400]',\n",
       "  'ProteinKinases[Br-Zma01001]',\n",
       "  'PlantMloReceptors',\n",
       "  'ProteinPhosphatasesAndAssociatedProteins[Br-Zma01009]',\n",
       "  'Proteasome[Br-Zma03051]',\n",
       "  'AminoAcidRelatedEnzymes[Br-Zma01007]',\n",
       "  'ProkaryoticDefenseSystem[Br-Zma02048]',\n",
       "  'Domain-ContainingProteinsNotElsewhereClassified[Br-Zma04990]',\n",
       "  'PeptidasesAndInhibitors[Br-Zma01002]',\n",
       "  'Prenyltransferases[Br-Zma01006]',\n",
       "  'RibosomeBiogenesis[Br-Zma03009]',\n",
       "  'CdMolecules[Br-Zma04090]',\n",
       "  'CytochromeP450[Br-Zma00199]',\n",
       "  'Gtp-BindingProteins[Br-Zma04031]',\n",
       "  'IonChannels[Br-Zma04040]',\n",
       "  'GlycosaminoglycanBindingProteins[Br-Zma00536]',\n",
       "  'Glycosylphosphatidylinositol(Gpi)-AnchoredProteins[Br-Zma00537]',\n",
       "  'LipopolysaccharideBiosynthesisProteins[Br-Zma01005]',\n",
       "  'Trehalose',\n",
       "  'TranslationFactors[Br-Zma03012]',\n",
       "  'YipfProteins',\n",
       "  'LipidBiosynthesisProteins[Br-Zma01004]',\n",
       "  'Efr3-Pi4KiiiAlphaComplexAndAssociatedProteins',\n",
       "  'RhoGtpaseAssociatedProteins',\n",
       "  'Ap-1Complex',\n",
       "  'WashComplex',\n",
       "  'EarpComplex',\n",
       "  'ErMembraneProteinComplex',\n",
       "  'RabGtpasesOfSpecificOrganelles',\n",
       "  'BacterialToxins[Br-Zma02042]',\n",
       "  'Ap-4Complex',\n",
       "  'Post-GpiAttachmentToProteinsFactors(Pgap)',\n",
       "  'Lectins[Br-Zma04091]',\n",
       "  'PeptidoglycanBiosynthesisAndDegradationProteins[Br-Zma01011]',\n",
       "  'HydrophobicMolecule',\n",
       "  'Ap-3Complex',\n",
       "  'Neuronostatin',\n",
       "  'Oxysterol-BindingProteins',\n",
       "  'OtherGlycosyltransferases',\n",
       "  'NadphOxidases(Nox)AndAssociatedProteins',\n",
       "  'RetrieverComplex',\n",
       "  'Non-CodingRnas[Br-Zma03100]',\n",
       "  'PolyketideBiosynthesisProteins[Br-Zma01008]',\n",
       "  'QualityControl',\n",
       "  'Bloc-1Complex',\n",
       "  'Proteoglycans[Br-Zma00535]',\n",
       "  'PatternRecognitionReceptors[Br-Zma04054]'],\n",
       " 'node_vals': [tensor([[ 0.5344, -0.3428, -0.1381]], grad_fn=<ToDenseBackward0>),\n",
       "  tensor([[0.5276, 0.6426, 0.1846]], grad_fn=<ToDenseBackward0>),\n",
       "  tensor([[-0.3978, -0.6157, -0.2862]], grad_fn=<ToDenseBackward0>),\n",
       "  tensor([[-0.2798,  0.1655, -0.3978]], grad_fn=<ToDenseBackward0>),\n",
       "  tensor([[-0.6318,  0.0683, -0.2798]], grad_fn=<ToDenseBackward0>),\n",
       "  tensor([[ 0.6131,  0.3268, -0.1795]], grad_fn=<ToDenseBackward0>),\n",
       "  tensor([[-0.1381,  0.2253,  0.0833]], grad_fn=<ToDenseBackward0>),\n",
       "  tensor([[-0.4595, -0.2747,  0.0246]], grad_fn=<ToDenseBackward0>),\n",
       "  tensor([[-0.4190, -0.5046, -0.2211]], grad_fn=<ToDenseBackward0>),\n",
       "  tensor([[-0.0269,  0.5965,  0.6212]], grad_fn=<ToDenseBackward0>),\n",
       "  tensor([[-0.0460,  0.6180, -0.6318]], grad_fn=<ToDenseBackward0>),\n",
       "  tensor([[-0.1013, -0.4199,  0.4618]], grad_fn=<ToDenseBackward0>),\n",
       "  tensor([[-0.4948,  0.5285, -0.0460]], grad_fn=<ToDenseBackward0>),\n",
       "  tensor([[-0.5839,  0.5693, -0.6686]], grad_fn=<ToDenseBackward0>),\n",
       "  tensor([[-0.2211, -0.3345,  0.6131]], grad_fn=<ToDenseBackward0>),\n",
       "  tensor([[ 0.3784,  0.1006, -0.1209]], grad_fn=<ToDenseBackward0>),\n",
       "  tensor([[ 0.4770,  0.3479, -0.4948]], grad_fn=<ToDenseBackward0>),\n",
       "  tensor([[0.1619, 0.5606, 0.3341]], grad_fn=<ToDenseBackward0>),\n",
       "  tensor([[ 0.3820, -0.2975,  0.5344]], grad_fn=<ToDenseBackward0>),\n",
       "  tensor([[ 0.6212,  0.3564, -0.5427]], grad_fn=<ToDenseBackward0>),\n",
       "  tensor([[-0.1803,  0.0493,  0.4770]], grad_fn=<ToDenseBackward0>),\n",
       "  tensor([[-0.3423, -0.0647, -0.1803]], grad_fn=<ToDenseBackward0>),\n",
       "  tensor([[ 0.1858,  0.6897, -0.4676]], grad_fn=<ToDenseBackward0>),\n",
       "  tensor([[-0.1875, -0.5500,  0.5276]], grad_fn=<ToDenseBackward0>),\n",
       "  tensor([[-0.7039, -0.5524, -0.0269]], grad_fn=<ToDenseBackward0>),\n",
       "  tensor([[-0.0799, -0.4076,  0.0600]], grad_fn=<ToDenseBackward0>),\n",
       "  tensor([[-0.0771, -0.6856,  0.1532]], grad_fn=<ToDenseBackward0>),\n",
       "  tensor([[-0.6686,  0.6947]], grad_fn=<ToDenseBackward0>),\n",
       "  tensor([[-0.2352, -0.2893,  0.2607]], grad_fn=<ToDenseBackward0>),\n",
       "  tensor([[ 0.6879, -0.5230, -0.5839]], grad_fn=<ToDenseBackward0>),\n",
       "  tensor([[ 0.1661,  0.0481, -0.0771]], grad_fn=<ToDenseBackward0>),\n",
       "  tensor([[-0.4621,  0.6755, -0.3423]], grad_fn=<ToDenseBackward0>),\n",
       "  tensor([[-0.4676,  0.5615, -0.1013]], grad_fn=<ToDenseBackward0>),\n",
       "  tensor([[ 0.4618, -0.5812, -0.2352]], grad_fn=<ToDenseBackward0>),\n",
       "  tensor([[-0.6086,  0.6492,  0.1230]], grad_fn=<ToDenseBackward0>),\n",
       "  tensor([[ 0.0875,  0.4355, -0.4190]], grad_fn=<ToDenseBackward0>),\n",
       "  tensor([[-0.3407, -0.5500, -0.3184]], grad_fn=<ToDenseBackward0>),\n",
       "  tensor([[-0.4298,  0.3362, -0.6086]], grad_fn=<ToDenseBackward0>),\n",
       "  tensor([[-0.5592, -0.6247, -0.2761]], grad_fn=<ToDenseBackward0>),\n",
       "  tensor([[ 0.1532, -0.6831,  0.2704]], grad_fn=<ToDenseBackward0>),\n",
       "  tensor([[-0.6282, -0.5515, -0.4621]], grad_fn=<ToDenseBackward0>),\n",
       "  tensor([[-0.2862, -0.4376, -0.6018]], grad_fn=<ToDenseBackward0>),\n",
       "  tensor([[ 0.5153,  0.0527, -0.5592]], grad_fn=<ToDenseBackward0>),\n",
       "  tensor([[-0.2761,  0.6035,  0.3784]], grad_fn=<ToDenseBackward0>),\n",
       "  tensor([[ 0.0246, -0.4369,  0.1858]], grad_fn=<ToDenseBackward0>),\n",
       "  tensor([[-0.5427,  0.2480,  0.3924]], grad_fn=<ToDenseBackward0>),\n",
       "  tensor([[ 0.3924,  0.1194, -0.4298]], grad_fn=<ToDenseBackward0>),\n",
       "  tensor([[-0.1209, -0.6319,  0.1619]], grad_fn=<ToDenseBackward0>),\n",
       "  tensor([[-0.3184,  0.1455,  0.3820]], grad_fn=<ToDenseBackward0>),\n",
       "  tensor([[ 0.1230, -0.4818, -0.0799]], grad_fn=<ToDenseBackward0>),\n",
       "  tensor([[-0.5370, -0.4259, -0.4595]], grad_fn=<ToDenseBackward0>),\n",
       "  tensor([[ 0.4406, -0.3713, -0.3407]], grad_fn=<ToDenseBackward0>),\n",
       "  tensor([[-0.5766,  0.5364,  0.5104]], grad_fn=<ToDenseBackward0>),\n",
       "  tensor([[ 0.1846, -0.6173,  0.6879]], grad_fn=<ToDenseBackward0>),\n",
       "  tensor([[-0.6385, -0.0118, -0.3814]], grad_fn=<ToDenseBackward0>),\n",
       "  tensor([[-0.1795, -0.6041, -0.5766]], grad_fn=<ToDenseBackward0>),\n",
       "  tensor([[-0.2545, -0.0189,  0.0875]], grad_fn=<ToDenseBackward0>),\n",
       "  tensor([[ 0.0833, -0.2945,  0.4671]], grad_fn=<ToDenseBackward0>),\n",
       "  tensor([[ 0.2607,  0.4440, -0.2545]], grad_fn=<ToDenseBackward0>),\n",
       "  tensor([[ 0.0600, -0.6180, -0.1875]], grad_fn=<ToDenseBackward0>),\n",
       "  tensor([[ 0.4671, -0.6398,  0.5153]], grad_fn=<ToDenseBackward0>),\n",
       "  tensor([[ 0.0236, -0.6906, -0.1701]], grad_fn=<ToDenseBackward0>),\n",
       "  tensor([[-0.6018,  0.0493,  0.4406]], grad_fn=<ToDenseBackward0>),\n",
       "  tensor([[ 0.2704, -0.5277, -0.6385]], grad_fn=<ToDenseBackward0>),\n",
       "  tensor([[ 0.5104, -0.4390,  0.0236]], grad_fn=<ToDenseBackward0>),\n",
       "  tensor([[-0.3814,  0.2945, -0.5370]], grad_fn=<ToDenseBackward0>),\n",
       "  tensor([[-0.1701,  0.1120, -0.7039]], grad_fn=<ToDenseBackward0>),\n",
       "  tensor([[ 0.3341, -0.2834,  0.1661]], grad_fn=<ToDenseBackward0>)]}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_node_slice_SparseLinearCustom(\n",
    "    # for `find_node_in_layer_info`\n",
    "    layer_info_list = M_list,\n",
    "    edge_dict = edge_dict,\n",
    "    query = 'y_hat',\n",
    "    # additional parameter for `slice_SparseLinearCustom`\n",
    "    neural_network_layer_list = model.layer_list,\n",
    "    weight_or_bias = 'weight', \n",
    "    grad = False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e5f642ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stophere' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m stophere\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stophere' is not defined"
     ]
    }
   ],
   "source": [
    "stophere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d1a68d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee550918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_i, xs_i = next(iter(training_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d1eb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_fn = nn.MSELoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c61393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37e64fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ii in tqdm(range(2)):\n",
    "#     pred = model(xs_i)\n",
    "#     loss = loss_fn(pred, y_i)\n",
    "#     if ii % 100 == 0:\n",
    "#         print(f'{loss}')\n",
    "#     optimizer.zero_grad()\n",
    "#     loss.backward()\n",
    "#     optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5aa1a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # After training are unity weights still unity?\n",
    "# i = 2\n",
    "# (\n",
    "# M_list[i].bias_eye_bool, \n",
    "# M_list[i].weight_eye_bool,\n",
    "# model.layer_list[((2*(i)))].weight,\n",
    "# model.layer_list[((2*(i)))].weight.grad\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c206af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a0be4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# M.row_inp\n",
    "# M.row_eye\n",
    "# M.col_out\n",
    "# M.col_eye\n",
    "\n",
    "# M.row_info\n",
    "# # M.col_info\n",
    "\n",
    "# # M.weight\n",
    "# # M.weight_bool\n",
    "# # M.weight_eye_bool\n",
    "# # M.bias\n",
    "# # M.bias_eye_bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c7ae39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = list(M_list[i].col_info.keys())[0]\n",
    "# slice_accumulator = []\n",
    "# # M_list[i].row_info[query]\n",
    "# c1 = M_list[i].col_info[query]['start'] \n",
    "# c2 = M_list[i].col_info[query]['stop']\n",
    "\n",
    "# # could get full slice then drop zero values too\n",
    "# for e in edge_dict[query]:\n",
    "#     r1 = M_list[i].row_info[e]['start'] \n",
    "#     r2 = M_list[i].row_info[e]['stop']\n",
    "#     slice_accumulator += [model.layer_list[((2*(i)))].weight.swapaxes(0,1)[r1:r2, c1:c2].clone().detach().requires_grad_(False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a16b1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.concat(slice_accumulator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352d196c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.layer_list[((2*(i)))].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89783b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8725cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0e59e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc35b8d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b76717e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# node_to_weights = {}\n",
    "# for i in range(0, len(M_list)):\n",
    "#     for query in M_list[i].col_out:\n",
    "#         slice_accumulator = []\n",
    "#         # M_list[i].row_info[query]\n",
    "#         c1 = M_list[i].col_info[query]['start'] \n",
    "#         c2 = M_list[i].col_info[query]['stop']\n",
    "\n",
    "#         # could get full slice then drop zero values too\n",
    "#         if i == 0:\n",
    "#             r1 = M_list[i].row_info[query]['start'] \n",
    "#             r2 = M_list[i].row_info[query]['stop']\n",
    "#             slice_accumulator += [model.layer_list[((2*(i)))].weight.swapaxes(0,1)[r1:r2, c1:c2].clone().detach().requires_grad_(False)]\n",
    "\n",
    "#         else: \n",
    "#             for e in edge_dict[query]:\n",
    "#                 r1 = M_list[i].row_info[e]['start'] \n",
    "#                 r2 = M_list[i].row_info[e]['stop']\n",
    "#                 slice_accumulator += [model.layer_list[((2*(i)))].weight.swapaxes(0,1)[r1:r2, c1:c2].clone().detach().requires_grad_(False)]\n",
    "\n",
    "#         slice_accumulator = torch.concat(slice_accumulator)\n",
    "#         node_to_weights[query] = slice_accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b078d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574b35dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0305f4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xx = [np.round(float(node_to_weights[key].abs().mean()), 3) for key in node_to_weights.keys()]\n",
    "# color_vals = ['#ffffff', '#fff7ec', '#fee8c8', '#fdd49e', '#fdbb84', '#fc8d59', '#ef6548', '#d7301f', '#b30000'#, '#7f0000'\n",
    "#               ]\n",
    "# color_cutoffs = [i*max(xx)/len(color_vals) for i in range(len(color_vals))]\n",
    "\n",
    "\n",
    "# dot = Digraph()\n",
    "# for key in node_to_weights.keys():\n",
    "\n",
    "\n",
    "#     key_mean_w = np.round(float(node_to_weights[key].abs().mean()), 3)\n",
    "#     color_val = color_vals[[i for i in range(len(color_cutoffs)) if key_mean_w >= color_cutoffs[i]][-1]]\n",
    "\n",
    "#     # key_label = name_cleanup(input = key, newline_char_threshold = 20)+'\\nMean: '+str(key_mean_w)\n",
    "#     # dot.node(key, key_label, style='filled', fillcolor=color_val)  \n",
    "    \n",
    "    \n",
    "#     key_label = name_cleanup(input = key, newline_char_threshold = 20)+'\\n           '  \n",
    "#     dot.node(key, key_label)\n",
    "\n",
    "#     if key in kegg_connections.keys():\n",
    "#         for value in kegg_connections[key]:\n",
    "#             # edge takes a head/tail whereas edges takes name pairs concatednated (A, B -> AB)in a list\n",
    "#             dot.edge(value, key)    \n",
    "\n",
    "# dot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
