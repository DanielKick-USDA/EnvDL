{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aadccc0",
   "metadata": {},
   "source": [
    "# Test out Hilbert Curve 1d\n",
    "\n",
    "> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# import plotly.express as px\n",
    "# import plotly.io as pio\n",
    "# pio.templates.default = \"plotly_white\"\n",
    "# from sklearn.manifold import TSNE # for visualizing embeddings\n",
    "\n",
    "# import hilbertcurve\n",
    "# from hilbertcurve.hilbertcurve import HilbertCurve\n",
    "\n",
    "from EnvDL.core import * # includes remove_matching_files\n",
    "from EnvDL.dna  import *\n",
    "from EnvDL.dlfn import * # includes LSUV_\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "use_gpu_num = 0\n",
    "\n",
    "# Imports --------------------------------------------------------------------\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import torch.nn.functional as F # F.mse_loss\n",
    "\n",
    "import einops # for einops.rearrange\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if use_gpu_num in [0, 1]: \n",
    "    torch.cuda.set_device(use_gpu_num)\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4600ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_path = '../nbs_artifacts/03.22_g2fc_W_timeseries_conv1d_resnet.ipynb/'\n",
    "ensure_dir_path_exists(dir_path = cache_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517af1f8",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a724cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Standard data prep\n",
    "\n",
    "# # Wrapper function to hide the steps of loading data\n",
    "# import numpy as np\n",
    "# from EnvDL.core import get_cached_result\n",
    "# from EnvDL.dlfn import read_split_info, find_idxs_split_dict\n",
    "# import pandas as pd\n",
    "# import torch\n",
    "\n",
    "# class g2fc_datawrapper():   \n",
    "#     def __init__(self):\n",
    "#         self.data_dict = {}\n",
    "#         self.cs_dict = {}\n",
    "#         print('Loading and storing default `phno`.')\n",
    "#         self.load(name='phno', store = True)\n",
    "    \n",
    "\n",
    "#     def set_split(self, load_from = '../nbs_artifacts/01.06_g2fc_cluster_genotypes/', json_prefix = '2023:9:5:12:8:26'):\n",
    "#         if 'phno' not in self.data_dict.keys():\n",
    "#             print('`phno` must be stored!\\nManually initialize with .load()')\n",
    "#         else:\n",
    "#             split_info = read_split_info(load_from = load_from, json_prefix = json_prefix)\n",
    "\n",
    "#             temp = self.data_dict['phno'].copy()\n",
    "#             temp[['Female', 'Male']] = temp['Hybrid'].str.split('/', expand = True)\n",
    "\n",
    "#             self.test_dict = find_idxs_split_dict(\n",
    "#                 obs_df = temp, \n",
    "#                 split_dict = split_info['test'][0]\n",
    "#             )\n",
    "\n",
    "#             temp = temp.loc[self.test_dict['train_idx'], ] # restrict before re-aplying\n",
    "\n",
    "#             self.val_dict = find_idxs_split_dict(\n",
    "#                 obs_df = temp, \n",
    "#                 split_dict = split_info['validate'][0]\n",
    "#             )\n",
    "\n",
    "#     def generic_load(self, load_from, file_name):        \n",
    "#         if   file_name.split('.')[-1] == 'pkl': res = get_cached_result(load_from+file_name)\n",
    "#         elif file_name.split('.')[-1] == 'npy': res = np.load(load_from+file_name)\n",
    "#         elif file_name.split('.')[-1] == 'csv': res = pd.read_csv(load_from+file_name)\n",
    "#         else: print(f'Unrecognized file encoding: {file_name.split(\".\")[-1]} \\nReturning None'); res = None\n",
    "#         return res\n",
    "\n",
    "#     def load(self, name='ACGT', store = False, **kwargs):\n",
    "#         # defaults for quick access\n",
    "#         defaults_dict = {\n",
    "#             ## Genomic Data\n",
    "#             'ACGT':         ['../nbs_artifacts/01.03_g2fc_prep_matrices/', 'ACGT.npy'],\n",
    "#             'ACGT_hilb':    ['../nbs_artifacts/01.03_g2fc_prep_matrices/', 'ACGT_hilb.npy'],            \n",
    "#             'KEGG_entries': ['../nbs_artifacts/01.05_g2fc_demo_model/', 'filtered_kegg_gene_entries.pkl'],\n",
    "#             'KEGG_slices':  ['../nbs_artifacts/01.05_g2fc_demo_model/', 'ACGT_gene_slice_list.pkl'],\n",
    "\n",
    "#             ## Soil and Management \n",
    "#             'mgmtMatNames': ['../nbs_artifacts/01.03_g2fc_prep_matrices/', 'mgmtMatNames.npy'],\n",
    "#             'mgmtMat':      ['../nbs_artifacts/01.03_g2fc_prep_matrices/', 'mgmtMat.npy'],\n",
    "#             'SMatNames':    ['../nbs_artifacts/01.03_g2fc_prep_matrices/', 'SMatNames.npy'],\n",
    "#             'SMat':         ['../nbs_artifacts/01.03_g2fc_prep_matrices/', 'SMat.npy'],\n",
    "\n",
    "#             ## Weather\n",
    "#             'PlantHarvestNames': ['../nbs_artifacts/01.03_g2fc_prep_matrices/', 'PlantHarvestNames.npy'],\n",
    "#             'PlantHarvest':      ['../nbs_artifacts/01.03_g2fc_prep_matrices/', 'PlantHarvest.npy'],\n",
    "#             'WMat':              ['../nbs_artifacts/01.03_g2fc_prep_matrices/', 'WMat.npy'],\n",
    "#             'WMatNames':         ['../nbs_artifacts/01.03_g2fc_prep_matrices/', 'WMatNames.npy'],\n",
    "#             'WMat_hilb':         ['../nbs_artifacts/01.03_g2fc_prep_matrices/', 'WMat_hilb.npy'],\n",
    "\n",
    "#             # Response and lookup\n",
    "#             'phno':            ['../nbs_artifacts/01.03_g2fc_prep_matrices/', 'phno_geno.csv'],\n",
    "#             'obs_geno_lookup': ['../nbs_artifacts/01.03_g2fc_prep_matrices/', 'obs_geno_lookup.npy'], # Phno_Idx  Geno_Idx  Is_Phno_Idx\n",
    "#             'obs_env_lookup':  ['../nbs_artifacts/01.03_g2fc_prep_matrices/', 'obs_env_lookup.npy'],  # Phno_Idx  Env_Idx   Is_Phno_Idx\n",
    "#             'YMat':            ['../nbs_artifacts/01.03_g2fc_prep_matrices/', 'YMat.npy']\n",
    "#         }\n",
    "\n",
    "#         if name in defaults_dict.keys():\n",
    "#             load_from, file_name = defaults_dict[name]\n",
    "#         else: \n",
    "#             print(f'`name` not recognized. Use `load_from` and `file_name` for greater control.\\,Allowed `names` are:\\n{list(defaults_dict.keys())}')\n",
    "        \n",
    "#         # overwrite defaults if desired\n",
    "#         if 'load_from' in kwargs.keys(): load_from = kwargs['load_from']\n",
    "#         if 'file_name' in kwargs.keys(): file_name = kwargs['file_name']\n",
    "\n",
    "#         res = self.generic_load(load_from=load_from, file_name= file_name)\n",
    "\n",
    "#         if store:\n",
    "#             self.data_dict[name] = res\n",
    "#         else:\n",
    "#             return res\n",
    "\n",
    "#     def load_all(self, name_list = [], store = False):\n",
    "#         if store:\n",
    "#             for e in name_list:\n",
    "#                 self.load(name = e, store=store)\n",
    "#         else:\n",
    "#             res_list = []\n",
    "#             for e in name_list:\n",
    "#                 res_list += [self.load(name = e, store=store)]\n",
    "#             return res_list\n",
    "\n",
    "#     def store_cs(self, name, cs_list):\n",
    "#         self.cs_dict[name] = cs_list\n",
    "\n",
    "#     def calc_cs(self, name, version = 'np', **kwargs):\n",
    "\n",
    "#         res = self.data_dict[name]\n",
    "#         if 'filter' in kwargs.keys():\n",
    "#             which_dict, which_split = kwargs['filter'].split(':')\n",
    "\n",
    "#             if which_split == 'train':  key = 'train_idx'\n",
    "#             elif which_split == 'test': key = 'test_idx'\n",
    "#             else: print('only `train` and `test` indexes are allowed.')\n",
    "\n",
    "#             if which_dict == 'val':    mask = self.val_dict[key]\n",
    "#             elif which_dict == 'test': mask = self.test_dict[key]\n",
    "#             else: print('only `val` and `test` sets are allowed.')\n",
    "\n",
    "#             if 'filter_lookup' in kwargs.keys():\n",
    "#                 # This block exists because some data is deduplicated. In the dataloader I use lookup tables to find the right values.\n",
    "#                 # That gets messy because the enviroment, genome, and yield all get different ones\n",
    "#                 # I could hardcode names to filters but that would make this code pretty inflexible (which I would like to avoid.)\n",
    "#                 # using the manual overwrite metod .store_cs() it's possible to get the desired behavior like this:\n",
    "#                 # X.store_cs('WMat', calc_cs(X.get('WMat')[np.array(list(set(X.get('obs_env_lookup', ops_string='filter:val:train')[:, 1]))),: ,:]))\n",
    "#                 # That is a lot messier looking than I would like. It's hard to see what's happening. \n",
    "#                 # To get around this I'm adding a 'filter_lookup' kwarg that does the same job as the lookup tables in the data loader.\n",
    "#                 lookup = self.data_dict[kwargs['filter_lookup']]\n",
    "#                 lookup = lookup[mask, 1]\n",
    "#                 # deduplicate; for cs we don't need the order of the obs.\n",
    "#                 mask = np.array(list(set(lookup)))\n",
    "#             res = res[mask]\n",
    "\n",
    "#         else:\n",
    "#             print('''\n",
    "# Scaling based on ALL data. To avoid this pass in a split to be used. \n",
    "# If a lookup table should be used to select observations (e.g. obs_env_lookup ) its name should be passed in. \n",
    "# E.g. filter = \\'val:train\\',  filter_lookup = \\'obs_env_lookup\\'\n",
    "#                   ''')\n",
    "\n",
    "#         if version == 'np':\n",
    "#             self.cs_dict[name] = [np.asarray(np.mean(res, axis = 0)), np.asarray(np.std(res, axis = 0))]\n",
    "#         elif version == 'torch':\n",
    "#             self.cs_dict[name] = [torch.Tensor.mean(res, axis = 0), torch.Tensor.std(res, axis = 0)]\n",
    "\n",
    "#     def calc_cs_all(self, name_list, version = 'np', **kwargs):\n",
    "#         for name in name_list:\n",
    "#             self.calc_cs(name=name, version = version, **kwargs)\n",
    "\n",
    "#     def apply_cs(self, name, **kwargs):\n",
    "#         if name not in self.cs_dict.keys():\n",
    "#             self.calc_cs(name, kwargs)\n",
    "\n",
    "#         vals = self.cs_dict[name]\n",
    "#         res = self.data_dict[name]\n",
    "        \n",
    "#         if type(res) == type(vals[0]):\n",
    "#             pass\n",
    "#         elif type(res) == torch.Tensor:\n",
    "#             # convert to pytorch\n",
    "#             vals = [torch.from_numpy(e) for e in vals]\n",
    "#         elif type(res) == np.ndarray:\n",
    "#             # convert to numpy\n",
    "#             vals = [torch.Tensor.numpy(e) for e in vals]\n",
    "            \n",
    "#         center, scale = vals\n",
    "#         res = (res - center) / scale\n",
    "#         return res\n",
    "\n",
    "#     def get_cs(self, name):\n",
    "#         if name not in self.cs_dict.keys():\n",
    "#             res = None\n",
    "#         else:\n",
    "#             res = self.cs_dict[name]\n",
    "#         return res\n",
    "\n",
    "#     def reverse_cs(self, name, x):\n",
    "#         vals = self.cs_dict[name]\n",
    "\n",
    "#         if type(x) == type(vals[0]):\n",
    "#             pass\n",
    "#         elif type(x) == torch.Tensor:\n",
    "#             # convert to pytorch\n",
    "#             vals = [torch.from_numpy(e) for e in vals]\n",
    "#         elif type(res) == np.ndarray:\n",
    "#             # convert to numpy\n",
    "#             vals = [torch.Tensor.numpy(e) for e in vals]\n",
    "\n",
    "#         center, scale = vals\n",
    "#         res = (res * scale) + center\n",
    "#         return res\n",
    "\n",
    "    \n",
    "#     def get(self, name, ops_string = ''):\n",
    "#         # is split info being requested? (for lookup_obs most likely). Otherwise main data is being requested.\n",
    "#         if name not in ['val:train', 'test:train', 'val:test', 'test:test']:\n",
    "#             if name not in self.data_dict.keys():\n",
    "#                 self.load(name, store=True)\n",
    "#             res = self.data_dict[name]\n",
    "#         else: \n",
    "#             which_dict, which_split = name.split(':')\n",
    "\n",
    "#             if which_split == 'train':  key = 'train_idx'\n",
    "#             elif which_split == 'test': key = 'test_idx'\n",
    "#             else: print('only `train` and `test` indexes are allowed.')\n",
    "\n",
    "#             if which_dict == 'val':    res = self.val_dict[key]\n",
    "#             elif which_dict == 'test': res = self.test_dict[key]\n",
    "#             else: print('only `val` and `test` sets are allowed.')\n",
    "            \n",
    "#         # apply opperations\n",
    "#         ops_string = [e for e in ops_string.split(' ') if e != '']\n",
    "#         for ops in ops_string:\n",
    "#             if ops[0:6] == 'filter':\n",
    "#                 _, which_dict, which_split = ops.split(':')\n",
    "\n",
    "#                 if which_split == 'train':  key = 'train_idx'\n",
    "#                 elif which_split == 'test': key = 'test_idx'\n",
    "#                 else: print('only `train` and `test` indexes are allowed.')\n",
    "\n",
    "#                 if which_dict == 'val':    res_idx = self.val_dict[key]\n",
    "#                 elif which_dict == 'test': res_idx = self.test_dict[key]\n",
    "#                 else: print('only `val` and `test` sets are allowed.')\n",
    "\n",
    "#                 res = res[res_idx]\n",
    "\n",
    "#             if ops == 'cs':\n",
    "#                 res = self.apply_cs(name)\n",
    "            \n",
    "#             if ops == 'asarray':\n",
    "#                 res = np.asarray(res)\n",
    "#             if ops == 'from_numpy':\n",
    "#                 res = torch.from_numpy(res)\n",
    "#             if ops == 'float':\n",
    "#                 res = res.to(torch.float)\n",
    "#             if ops[0:4] == 'cuda':\n",
    "#                 # send to device by number. e.g. cuda:0 -> X.to(0)\n",
    "#                 res = res.to(int(ops.split(':')[-1]))\n",
    "\n",
    "#         return res\n",
    "\n",
    "\n",
    "# # some example usage \n",
    "# # X = g2fc_datawrapper()\n",
    "# # X.set_split()\n",
    "# # X.load_all(name_list = ['obs_env_lookup', 'YMat', 'PlantHarvest', 'WMat',], store=True) \n",
    "# # X.calc_cs('YMat', version = 'np', filter = 'val:train'); X.cs_dict['YMat']\n",
    "# # X.calc_cs_all(['YMat'], version = 'np', filter = 'val:train'); X.cs_dict['YMat']\n",
    "# # X.calc_cs('YMat', version = 'np'); X.cs_dict['YMat']\n",
    "\n",
    "# # some demonstration of when to use kwargs for scaling \n",
    "# # how do I manually do scaling for enviromental things?\n",
    "# # X.store_cs('WMat', calc_cs(X.get('WMat')[np.array(list(set(X.get('obs_env_lookup', ops_string='filter:val:train')[:, 1]))),: ,:]))\n",
    "# # [e[0:3, 0] for e in X.cs_dict['WMat']]\n",
    "# # X.calc_cs('WMat', filter = 'val:train', filter_lookup= 'obs_env_lookup')\n",
    "# # [e[0:3, 0] for e in X.cs_dict['WMat']]\n",
    "# # X.calc_cs('WMat')\n",
    "# # [e[0:3, 0] for e in X.cs_dict['WMat']]\n",
    "\n",
    "# # X.get('WMat', ops_string='asarray')[0:3, 0:3, 0]\n",
    "# # X.get('WMat', ops_string='cs asarray')[0:3, 0:3, 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372ccc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from EnvDL.dlfn import g2fc_datawrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8452a9f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and storing default `phno`.\n"
     ]
    }
   ],
   "source": [
    "X = g2fc_datawrapper()\n",
    "X.set_split()\n",
    "X.load_all(name_list = ['obs_env_lookup', 'YMat', 'PlantHarvest', 'WMat',], store=True) \n",
    "\n",
    "X.calc_cs('YMat', version = 'np', filter = 'val:train')\n",
    "X.calc_cs('WMat', filter = 'val:train', filter_lookup= 'obs_env_lookup')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e415b8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_dataloader = DataLoader(BigDataset(\n",
    "    y =          X.get('YMat',           ops_string='filter:val:train cs asarray from_numpy float cuda:0'),\n",
    "    lookup_obs = X.get('val:train',      ops_string='                    asarray from_numpy             '),\n",
    "    lookup_env = X.get('obs_env_lookup', ops_string='filter:val:train    asarray from_numpy             '),\n",
    "    W =          X.get('WMat',           ops_string='                 cs         from_numpy float cuda:0'),\n",
    "    P =          X.get('PlantHarvest',   ops_string='                            from_numpy             '),\n",
    "    W_type = 'raw'\n",
    "    ),\n",
    "    batch_size = 50,\n",
    "    shuffle = True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c5bb53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bc9dde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850c02fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc30ab87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63769228",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214e35f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829fd680",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30df8616",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bbb51a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e0d2b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f5c6cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca3d36a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6d20ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([11.3382, 10.9088, 11.7227,  ..., 11.8326,  8.6582, 12.3963],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.get('YMat',           ops_string='filter:val:train asarray from_numpy float cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c286774d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X.get('YMat', ops_string='asarray from_numpy float cuda:0').shape, \n",
    "X.get('YMat', ops_string='filter:val:train asarray from_numpy float cuda:0').shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0250f482",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# TODO move this to module and fix old versions. \u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# This one has lookups deduplicated data in a way that's more in line with my expectations (no more filtering lookup tables for the dataloader!)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBigDataset\u001b[39;00m(Dataset):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m      6\u001b[0m         lookup_obs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs \n\u001b[1;32m     19\u001b[0m         ):\n\u001b[1;32m     20\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03m        This class produces a set with one or more input tensors. For flexibility the only _required_ input is `lookup_obs`, a tensor with the index of observations. \u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;03m        Everything else is provided as a kwarg. Output is a list of tensors1 ordered [y, G, S, W], any of these not initalized will be missing but not empty (e.g. [y, S, W] not [y, None, S, W]).       \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03m        1 G may also be returned as a list of tensors\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03m        \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# TODO move this to module and fix old versions. \n",
    "# This one has lookups deduplicated data in a way that's more in line with my expectations (no more filtering lookup tables for the dataloader!)\n",
    "class BigDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        lookup_obs,\n",
    "#         lookup_geno,\n",
    "#         lookup_env,\n",
    "#         y,\n",
    "#         G, \n",
    "#         G_type\n",
    "#         S,\n",
    "#         P,\n",
    "#         W,\n",
    "#         W_type,\n",
    "        transform = None, \n",
    "        target_transform = None,\n",
    "        **kwargs \n",
    "        ):\n",
    "        \"\"\"\n",
    "        This class produces a set with one or more input tensors. For flexibility the only _required_ input is `lookup_obs`, a tensor with the index of observations. \n",
    "        Everything else is provided as a kwarg. Output is a list of tensors1 ordered [y, G, S, W], any of these not initalized will be missing but not empty (e.g. [y, S, W] not [y, None, S, W]).       \n",
    "        Used inputs are:\n",
    "        lookup_obs: index for y, used by __getitem__ for obs_idx\n",
    "        lookup_geno: index for G, row obs_idx, column 1 is geno_idx (geno information is deduplicated, hence the need for a lookup)\n",
    "        lookup_env: index for S & W, , row obs_idx, column 1 is env_idx (env information is deduplicated, hence the need for a lookup)\n",
    "        y: yield\n",
    "        G: Genomic information \n",
    "        G_type: how the infomation should be returned, 'raw', 'hilbert', or 'list' (i.e. of tensors for snps in each gene)\n",
    "        S: Soil information\n",
    "        P: Planting/Harvest date contained in column 0, 1 respectively \n",
    "        W: Weather data\n",
    "        W_type: how the infomation should be returned, 'raw' or 'hilbert'\n",
    "\n",
    "        1 G may also be returned as a list of tensors\n",
    "        \"\"\"\n",
    "        # Lookup info (so that deduplication works)\n",
    "        self.lookup_obs = lookup_obs\n",
    "        # if 'lookup_obs'  in kwargs: self.lookup_obs  = kwargs['lookup_obs'];\n",
    "        if 'lookup_geno' in kwargs: self.lookup_geno = kwargs['lookup_geno'];\n",
    "        if 'lookup_env'  in kwargs: self.lookup_env  = kwargs['lookup_env'];\n",
    "        # Data\n",
    "        if 'y' in kwargs: self.y = kwargs['y'];\n",
    "        if 'G' in kwargs: self.G = kwargs['G'];\n",
    "        if 'S' in kwargs: self.S = kwargs['S'];\n",
    "        if 'P' in kwargs: self.P = kwargs['P']; # PlantHarvest so that planting can be added into W\n",
    "        if 'W' in kwargs: self.W = kwargs['W'];\n",
    "        # Data prep state information\n",
    "        if 'G_type' in kwargs: self.G_type = kwargs['G_type']; # raw, hilbert, list\n",
    "        if 'W_type' in kwargs: self.W_type = kwargs['W_type']; # raw, hilbert\n",
    "        # Data to be returned\n",
    "        self.out_names = [e for e in ['y', 'G', 'S', 'W'] if e in kwargs]\n",
    "        # Transformations\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.lookup_obs)\n",
    "    \n",
    "\n",
    "    # These used to be in __getitem__ but separating them like this allows for them to be overwritten more easily\n",
    "    def get_y(self, idx):\n",
    "        y_idx = self.y[idx]\n",
    "        if self.transform:\n",
    "            y_idx = self.transform(y_idx)\n",
    "        return(y_idx)\n",
    "        \n",
    "    def get_G(self, idx):\n",
    "        geno_idx = self.lookup_geno[idx, 1]\n",
    "        if self.G_type in ['raw', 'hilbert']:\n",
    "            G_idx = self.G[geno_idx]\n",
    "        if 'list' == self.G_type:\n",
    "            G_idx = [e[geno_idx] for e in self.G]\n",
    "        if self.transform:\n",
    "            G_idx = self.transform(G_idx)\n",
    "        return(G_idx)\n",
    "\n",
    "    def get_S(self, idx):\n",
    "        env_idx = self.lookup_env[idx, 1]\n",
    "        S_idx = self.S[env_idx]\n",
    "        if self.transform:\n",
    "            S_idx = self.transform(S_idx)\n",
    "        return(S_idx)\n",
    "\n",
    "    def get_W(self, idx):\n",
    "        W_device = torch.Tensor(self.W).get_device()\n",
    "\n",
    "\n",
    "        env_idx = self.lookup_env[idx, 1]\n",
    "        # get growing information\n",
    "        WPlant = np.zeros(365)\n",
    "        # WPlant[self.P[obs_idx, 0]:self.P[obs_idx, 1]] = 1\n",
    "        WPlant[self.P[idx, 0]:self.P[idx, 1]] = 1\n",
    "        if self.W_type == 'raw':\n",
    "            WPlant = torch.from_numpy(WPlant).to(torch.float)\n",
    "            # if needed send to gpu\n",
    "            if W_device != -1: WPlant = WPlant.to(W_device)            \n",
    "            W_idx = torch.concatenate([self.W[env_idx], WPlant[None, :]], axis = 0)\n",
    "        if self.W_type == 'hilbert':\n",
    "            # convert growing info to hilbert curve\n",
    "            WPlant_hilb = np_3d_to_hilbert(WPlant[None, :, None], silent = True)\n",
    "            WPlant_hilb = WPlant_hilb.squeeze(axis = 3)\n",
    "            WPlant_hilb[np.isnan(WPlant_hilb)] = 0\n",
    "            WPlant_hilb = torch.from_numpy(WPlant_hilb).to(torch.float)\n",
    "            # if needed send to gpu\n",
    "            if W_device != -1: WPlant_hilb = WPlant_hilb.to(W_device)\n",
    "            W_idx = torch.concatenate([self.W[env_idx], WPlant_hilb], axis = 0)\n",
    "        if self.transform:\n",
    "            W_idx = self.transform(W_idx)\n",
    "        return(W_idx)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        out = []\n",
    "        obs_idx = self.lookup_obs[idx]\n",
    "        if 'y' in self.out_names: out += [self.get_y(idx)]\n",
    "        if 'G' in self.out_names: out += [self.get_G(obs_idx)]\n",
    "        if 'S' in self.out_names: out += [self.get_S(obs_idx)]\n",
    "        if 'W' in self.out_names: out += [self.get_W(obs_idx)]\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d32602",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataloader = DataLoader(BigDataset(\n",
    "    y =          torch.from_numpy(                        y_cs[train_idx])[:, None].to(torch.float),\n",
    "    lookup_obs = torch.from_numpy(np.asarray([i for i in range(train_idx.size)])),\n",
    "    lookup_env = torch.from_numpy(np.asarray(obs_env_lookup[   train_idx])),\n",
    "    W =          torch.from_numpy(WMat).to(torch.float),\n",
    "    P =          torch.from_numpy(PlantHarvest),\n",
    "    W_type = 'raw'\n",
    "    ),\n",
    "    batch_size = 50,\n",
    "    shuffle = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56edc272",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = g2fc_datawrapper()\n",
    "X.set_split()\n",
    "X.load_all(name_list = ['obs_env_lookup', 'YMat', 'PlantHarvest', 'WMat',], store=True) \n",
    "\n",
    "training_dataloader = DataLoader(BigDataset(\n",
    "    y =          X.get('YMat',           ops_string='filter:val:train asarray from_numpy float cuda:0'),\n",
    "    lookup_obs = X.get('val:train',      ops_string='                 asarray from_numpy             '),\n",
    "    lookup_env = X.get('obs_env_lookup', ops_string='filter:val:train asarray from_numpy             '),\n",
    "    W =          X.get('WMat',           ops_string='                         from_numpy float cuda:0'),\n",
    "    P =          X.get('PlantHarvest',   ops_string='                         from_numpy             '),\n",
    "    W_type = 'raw'\n",
    "    ),\n",
    "    batch_size = 50,\n",
    "    shuffle = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X.get('obs_env_lookup', ops_string='filter:val:train asarray from_numpy             ').shape,\n",
    "X.get('obs_env_lookup', ops_string='  asarray from_numpy             ').shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb5b3db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1597e70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab930e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X.get('YMat', ops_string='asarray from_numpy float cuda:0')[:, None].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704ae63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_from = '../nbs_artifacts/01.03_g2fc_prep_matrices/'\n",
    "\n",
    "np.load(load_from+'PlantHarvestNames.npy')\n",
    "PlantHarvest = np.load(load_from+'PlantHarvest.npy')\n",
    "\n",
    "WMat = np.load(load_from+'WMat.npy')\n",
    "WMatNames = np.load(load_from+'WMatNames.npy')\n",
    "\n",
    "# WMat_hilb = np.load(load_from+'WMat_hilb.npy')\n",
    "\n",
    "load_from = '../nbs_artifacts/01.03_g2fc_prep_matrices/'\n",
    "phno_geno = pd.read_csv(load_from+'phno_geno.csv')\n",
    "phno = phno_geno\n",
    "\n",
    "obs_geno_lookup = np.load(load_from+'obs_geno_lookup.npy') # Phno_Idx  Geno_Idx  Is_Phno_Idx\n",
    "obs_env_lookup = np.load(load_from+'obs_env_lookup.npy')   # Phno_Idx  Env_Idx   Is_Phno_Idx\n",
    "YMat = np.load(load_from+'YMat.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f58532",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8b17d74",
   "metadata": {},
   "source": [
    "### Weather (Variable in season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6a8976",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_from = '../nbs_artifacts/01.03_g2fc_prep_matrices/'\n",
    "\n",
    "np.load(load_from+'PlantHarvestNames.npy')\n",
    "PlantHarvest = np.load(load_from+'PlantHarvest.npy')\n",
    "\n",
    "WMat = np.load(load_from+'WMat.npy')\n",
    "WMatNames = np.load(load_from+'WMatNames.npy')\n",
    "\n",
    "# WMat_hilb = np.load(load_from+'WMat_hilb.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f244bb4",
   "metadata": {},
   "source": [
    "### Response and lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e59a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_from = '../nbs_artifacts/01.03_g2fc_prep_matrices/'\n",
    "phno_geno = pd.read_csv(load_from+'phno_geno.csv')\n",
    "phno = phno_geno\n",
    "\n",
    "obs_geno_lookup = np.load(load_from+'obs_geno_lookup.npy') # Phno_Idx  Geno_Idx  Is_Phno_Idx\n",
    "obs_env_lookup = np.load(load_from+'obs_env_lookup.npy')   # Phno_Idx  Env_Idx   Is_Phno_Idx\n",
    "YMat = np.load(load_from+'YMat.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8874cdbb",
   "metadata": {},
   "source": [
    "### Set up Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_from = '../nbs_artifacts/01.06_g2fc_cluster_genotypes/'\n",
    "\n",
    "split_info = read_split_info(\n",
    "    load_from = '../nbs_artifacts/01.06_g2fc_cluster_genotypes/',\n",
    "    json_prefix = '2023:9:5:12:8:26')\n",
    "\n",
    "temp = phno.copy()\n",
    "temp[['Female', 'Male']] = temp['Hybrid'].str.split('/', expand = True)\n",
    "\n",
    "test_dict = find_idxs_split_dict(\n",
    "    obs_df = temp, \n",
    "    split_dict = split_info['test'][0]\n",
    ")\n",
    "\n",
    "temp = temp.loc[test_dict['train_idx'], ] # restrict before re-aplying\n",
    "\n",
    "val_dict = find_idxs_split_dict(\n",
    "    obs_df = temp, \n",
    "    split_dict = split_info['validate'][0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5916931e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = val_dict['train_idx']\n",
    "test_idx  = val_dict['test_idx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c90807",
   "metadata": {},
   "outputs": [],
   "source": [
    "YMat_cs = calc_cs(YMat[train_idx])\n",
    "y_cs = apply_cs(YMat, YMat_cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d5f603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging Data -----------------------------------------------------------------------------------\n",
    "\n",
    "train_idx = val_dict['train_idx']\n",
    "test_idx  = val_dict['test_idx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b1ba16",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataloader = DataLoader(BigDataset(\n",
    "    y = torch.from_numpy(                                 y_cs[train_idx])[:, None].to(torch.float),\n",
    "    lookup_obs = torch.from_numpy(np.asarray([i for i in range(train_idx.size)])),\n",
    "    lookup_env = torch.from_numpy(   np.asarray(obs_env_lookup[train_idx])),\n",
    "    W = torch.from_numpy(WMat).to(torch.float),\n",
    "    P = torch.from_numpy(PlantHarvest),\n",
    "    W_type = 'raw'\n",
    "    ),\n",
    "    batch_size = 50,\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "validation_dataloader = DataLoader(BigDataset(\n",
    "    y = torch.from_numpy(                                 y_cs[test_idx])[:, None].to(torch.float),\n",
    "    lookup_obs = torch.from_numpy(np.asarray([i for i in range(test_idx.size)])),\n",
    "    lookup_env = torch.from_numpy(np.asarray(   obs_env_lookup[test_idx])),\n",
    "    W = torch.from_numpy(WMat).to(torch.float),\n",
    "    P = torch.from_numpy(PlantHarvest),\n",
    "    W_type = 'raw'\n",
    "    ),\n",
    "    batch_size = 50,\n",
    "    shuffle = False \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779aabc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(training_dataloader))[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f02a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ResNet1d(\n",
    "        block = BasicBlock1d, #: Type[Union[BasicBlock, Bottleneck]],\n",
    "        layers = [2, 2, 2, 2], #: List[int],\n",
    "        # num_classes: int = 1000,\n",
    "        zero_init_residual = False,\n",
    "        groups = 1,\n",
    "        width_per_group = 64,\n",
    "        replace_stride_with_dilation = None,\n",
    "        norm_layer = None,\n",
    "        input_channels = 17\n",
    "    )(next(iter(training_dataloader))[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddbb44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet1d(\n",
    "#         block = BasicBlock1d, #: Type[Union[BasicBlock, Bottleneck]],\n",
    "#         layers = [2, 2, 2, 2], #: List[int],\n",
    "#         # num_classes: int = 1000,\n",
    "#         zero_init_residual = False,\n",
    "#         groups = 1,\n",
    "#         width_per_group = 64,\n",
    "#         replace_stride_with_dilation = None,\n",
    "#         norm_layer = None,\n",
    "#         input_channels = 17\n",
    "#     )(torch.randn(([50, 17, 365])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4748f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module for training subnetworks.\n",
    "class plDNN_wthr(pl.LightningModule):\n",
    "    def __init__(self, mod):\n",
    "        super().__init__()\n",
    "        self.mod = mod\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        y_i, w_i = batch\n",
    "        pred = self.mod(w_i)\n",
    "        loss = F.mse_loss(pred, y_i)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            weight_list=[(name, param) for name, param in model.named_parameters() if name.split('.')[-1] == 'weight']\n",
    "            for l in weight_list:\n",
    "                self.log((\"train_mean\"+l[0]), l[1].mean())\n",
    "                self.log((\"train_std\"+l[0]), l[1].std())        \n",
    "        return(loss)\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        y_i, w_i = batch\n",
    "        pred = self.mod(w_i)\n",
    "        loss = F.mse_loss(pred, y_i)\n",
    "        self.log('val_loss', loss)        \n",
    "     \n",
    "    def configure_optimizers(self, **kwargs):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), **kwargs)\n",
    "        return optimizer    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e38fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet1d(\n",
    "        block = BasicBlock1d, #: Type[Union[BasicBlock, Bottleneck]],\n",
    "        layers = [2, 2, 2, 2], #: List[int],\n",
    "        # num_classes: int = 1000,\n",
    "        zero_init_residual = False,\n",
    "        groups = 1,\n",
    "        width_per_group = 64,\n",
    "        replace_stride_with_dilation = None,\n",
    "        norm_layer = None,\n",
    "        input_channels = 17\n",
    "    ).to('cuda')\n",
    "\n",
    "\n",
    "max_epoch = 10\n",
    "DNNW = plDNN_wthr(model)     \n",
    "optimizer = DNNW.configure_optimizers()\n",
    "\n",
    "logger = TensorBoardLogger(\"tb_logs\", name=\"w-cnn1-res-4rep2-from-pytorch\")\n",
    "trainer = pl.Trainer(max_epochs=max_epoch, logger=logger)\n",
    "\n",
    "trainer.fit(model=DNNW, train_dataloaders=training_dataloader, val_dataloaders=validation_dataloader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99e8e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = ResNet1d(\n",
    "        block = BasicBlock1d, #: Type[Union[BasicBlock, Bottleneck]],\n",
    "        layers = [4, 4, 4, 4], #: List[int],\n",
    "        # num_classes: int = 1000,\n",
    "        zero_init_residual = False,\n",
    "        groups = 1,\n",
    "        width_per_group = 64,\n",
    "        replace_stride_with_dilation = None,\n",
    "        norm_layer = None,\n",
    "        input_channels = 17\n",
    "    ).to('cuda')\n",
    "\n",
    "\n",
    "max_epoch = 10\n",
    "DNNW = plDNN_wthr(model)     \n",
    "optimizer = DNNW.configure_optimizers()\n",
    "\n",
    "logger = TensorBoardLogger(\"tb_logs\", name=\"w-cnn1-res-4rep4-from-pytorch\")\n",
    "trainer = pl.Trainer(max_epochs=max_epoch, logger=logger)\n",
    "\n",
    "trainer.fit(model=DNNW, train_dataloaders=training_dataloader, val_dataloaders=validation_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4818768",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91483e6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2c27741",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1787697f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c65f3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4051e898",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5ce5d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c965259a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
