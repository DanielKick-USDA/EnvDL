---
title: Reused Objects
jupyter: python3
---

```{python}
import sqlalchemy
```

```{python}
import numpy as np
import torch 
from torch.utils.data import Dataset, DataLoader
from torch import optim
from torch import nn
from torch.nn import init # for init.kaiming_normal_
import torch.nn.functional as F

# import torcheval
from torcheval.metrics import MeanSquaredError, Mean


from torch.optim.lr_scheduler import ExponentialLR
```

```{python}
import fastcore.all as fc
from fastprogress import progress_bar,master_bar
```

```{python}
from fastai.vision.all import *
```

```{python}
import plotly.express as px
from captum.attr import IntegratedGradients
```

```{python}
def_device = 'cuda'
# def_device = 'cpu'
```



## Callbacks

```{python}
class CancelFitException(Exception): pass
class CancelBatchException(Exception): pass
class CancelEpochException(Exception): pass

class Callback(): order = 0

def run_cbs(cbs, method_nm, learn=None):
    for cb in sorted(cbs, key=attrgetter('order')):
        method = getattr(cb, method_nm, None)
        if method is not None: method(learn)
            
# class CompletionCB(Callback):
#     def before_fit(self, learn): self.count = 0
#     def after_batch(self, learn): self.count += 1
#     def after_fit(self, learn): print(f'Completed {self.count} batches')
```

```{python}
# cbs = [CompletionCB()]
# run_cbs(cbs, 'before_fit')
# run_cbs(cbs, 'after_batch')
# run_cbs(cbs, 'after_fit')
```

```{python}
# uses exception to exit fitting after only one batch
#     
class SingleBatchCB(Callback):
    order = 1
    def after_batch(self, learn): raise CancelFitException()
```

```{python}
class MetricsCB(Callback):
    def __init__(self, *ms, **metrics):
        for o in ms: metrics[type(o).__name__] = o
        self.metrics = metrics
        self.all_metrics = copy(metrics)
        self.all_metrics['loss'] = self.loss = Mean()

    def _log(self, d): print(d)
    def before_fit(self, learn): learn.metrics = self
    def before_epoch(self, learn): [o.reset() for o in self.all_metrics.values()]

    def after_epoch(self, learn):
        log = {k:f'{v.compute():.3f}' for k,v in self.all_metrics.items()}
        log['epoch'] = learn.epoch
        log['train'] = 'train' if learn.model.training else 'eval'
        self._log(log)

    def after_batch(self, learn):
        x,y,*_ = to_cpu(learn.batch)
        for m in self.metrics.values(): m.update(to_cpu(learn.preds), y)
        self.loss.update(to_cpu(learn.loss), weight=len(x))
```

```{python}
class DeviceCB(Callback):
    def __init__(self, device=def_device): fc.store_attr()
    def before_fit(self, learn):
        if hasattr(learn.model, 'to'): learn.model.to(self.device)
    def before_batch(self, learn): learn.batch = to_device(learn.batch, device=self.device)
```

```{python}
class TrainCB(Callback):
    def __init__(self, n_inp=1): self.n_inp = n_inp # NB: I added self.n_inp after the lesson. This allows us to train models with more than one input or output.
    def predict(self, learn): learn.preds = learn.model(*learn.batch[:self.n_inp])
    def get_loss(self, learn): learn.loss = learn.loss_func(learn.preds, *learn.batch[self.n_inp:])
    def backward(self, learn): learn.loss.backward()
    def step(self, learn): learn.opt.step()
    def zero_grad(self, learn): learn.opt.zero_grad()
```

```{python}
class ProgressCB(Callback):
    order = MetricsCB.order+1
    def __init__(self, plot=False): self.plot = plot
    def before_fit(self, learn):
        learn.epochs = self.mbar = master_bar(learn.epochs)
        self.first = True
        if hasattr(learn, 'metrics'): learn.metrics._log = self._log
        self.losses = []
        self.val_losses = []

    def _log(self, d):
        if self.first:
            self.mbar.write(list(d), table=True)
            self.first = False
        self.mbar.write(list(d.values()), table=True)

    def before_epoch(self, learn): learn.dl = progress_bar(learn.dl, leave=False, parent=self.mbar)
    def after_batch(self, learn):
        learn.dl.comment = f'{learn.loss:.3f}'
        if self.plot and hasattr(learn, 'metrics') and learn.training:
            self.losses.append(learn.loss.item())
            if self.val_losses: self.mbar.update_graph([[fc.L.range(self.losses), self.losses],[fc.L.range(learn.epoch).map(lambda x: (x+1)*len(learn.dls.train)), self.val_losses]])
    
    def after_epoch(self, learn): 
        if not learn.training:
            if self.plot and hasattr(learn, 'metrics'): 
                self.val_losses.append(learn.metrics.all_metrics['loss'].compute())
                self.mbar.update_graph([[fc.L.range(self.losses), self.losses],[fc.L.range(learn.epoch+1).map(lambda x: (x+1)*len(learn.dls.train)), self.val_losses]])
```

```{python}
# class LRFinderCB(Callback):
#     def __init__(self, lr_mult=1.3): fc.store_attr()
    
#     def before_fit(self, learn):
#         self.lrs,self.losses = [],[]
#         self.min = math.inf

#     def after_batch(self, learn):
#         if not learn.training: raise CancelEpochException()
#         self.lrs.append(learn.opt.param_groups[0]['lr'])
#         loss = to_cpu(learn.loss)
#         self.losses.append(loss)
#         if loss < self.min: self.min = loss
#         if loss > self.min*3: raise CancelFitException()
#         for g in learn.opt.param_groups: g['lr'] *= self.lr_mult
```

```{python}
# Updated version

class LRFinderCB(Callback):
    def __init__(self, gamma=1.3, max_mult=3): fc.store_attr()
    
    def before_fit(self, learn):
        self.sched = ExponentialLR(learn.opt, self.gamma)
        self.lrs,self.losses = [],[]
        self.min = math.inf

    def after_batch(self, learn):
        if not learn.training: raise CancelEpochException()
        self.lrs.append(learn.opt.param_groups[0]['lr'])
        loss = to_cpu(learn.loss)
        self.losses.append(loss)
        if loss < self.min: self.min = loss
        if math.isnan(loss) or (loss > self.min*self.max_mult):
            raise CancelFitException()
        self.sched.step()

    def cleanup_fit(self, learn):
        plt.plot(self.lrs, self.losses)
        plt.xscale('log')
```



## Metrics

```{python}
class Metric:
    def __init__(self): self.reset()
    def reset(self): self.vals,self.ns = [],[]
    def add(self, inp, targ=None, n=1):
        self.last = self.calc(inp, targ)
        self.vals.append(self.last)
        self.ns.append(n)
    @property
    def value(self):
        ns = tensor(self.ns)
        return (tensor(self.vals)*ns).sum()/ns.sum()
    def calc(self, inps, targs): return inps
```

```{python}
class Accuracy(Metric):
    def calc(self, inps, targs): return (inps==targs).float().mean()
```

## Misc

```{python}
def to_cpu(x):
    # not sure what Mapping is-- not a keyword as far as I can tell
#     if isinstance(x, Mapping): return {k:to_cpu(v) for k,v in x.items()}
    if isinstance(x, dict): return {k:to_cpu(v) for k,v in x.items()}
    if isinstance(x, list): return [to_cpu(o) for o in x]
    if isinstance(x, tuple): return tuple(to_cpu(list(x)))
    res = x.detach().cpu()
    return res.float() if res.dtype==torch.float16 else res
```

```{python}
class with_cbs:
    def __init__(self, nm): self.nm = nm
    def __call__(self, f):
        def _f(o, *args, **kwargs):
            try:
                o.callback(f'before_{self.nm}')
                f(o, *args, **kwargs)
                o.callback(f'after_{self.nm}')
            except globals()[f'Cancel{self.nm.title()}Exception']: pass
            finally: o.callback(f'cleanup_{self.nm}')
        return _f
```

## Learner Class

```{python}
class Learner():
    def __init__(self, model, dls=(0,), loss_func=F.mse_loss, lr=0.1, cbs=None, opt_func=optim.SGD):
        cbs = fc.L(cbs)
        fc.store_attr()

    @with_cbs('batch')
    def _one_batch(self):
        self.predict()
        self.callback('after_predict')
        self.get_loss()
        self.callback('after_loss')
        if self.training:
            self.backward()
            self.callback('after_backward')
            self.step()
            self.callback('after_step')
            self.zero_grad()

    @with_cbs('epoch')
    def _one_epoch(self):
        for self.iter,self.batch in enumerate(self.dl): self._one_batch()

    def one_epoch(self, training):
        self.model.train(training)
        self.dl = self.dls.train if training else self.dls.valid
        self._one_epoch()

    @with_cbs('fit')
    def _fit(self, train, valid):
        for self.epoch in self.epochs:
            if train: self.one_epoch(True)
            if valid: torch.no_grad()(self.one_epoch)(False)

    def fit(self, n_epochs=1, train=True, valid=True, cbs=None, lr=None):
        cbs = fc.L(cbs)
        # `add_cb` and `rm_cb` were added in lesson 18
        for cb in cbs: self.cbs.append(cb)
        try:
            self.n_epochs = n_epochs
            self.epochs = range(n_epochs)
            if lr is None: lr = self.lr
            if self.opt_func: self.opt = self.opt_func(self.model.parameters(), lr)
            self._fit(train, valid)
        finally:
            for cb in cbs: self.cbs.remove(cb)

    def __getattr__(self, name):
        if name in ('predict','get_loss','backward','step','zero_grad'): return partial(self.callback, name)
        raise AttributeError(name)

    def callback(self, method_nm): run_cbs(self.cbs, method_nm, self)
    
    @property
    def training(self): return self.model.training
```

```{python}
class TrainLearner(Learner):
    def predict(self): self.preds = self.model(self.batch[0])
    def get_loss(self): self.loss = self.loss_func(self.preds, self.batch[1])
    def backward(self): self.loss.backward()
    def step(self): self.opt.step()
    def zero_grad(self): self.opt.zero_grad()
```

```{python}
class MomentumLearner(TrainLearner):
    def __init__(self, model, dls, loss_func, lr=None, cbs=None, opt_func=optim.SGD, mom=0.85):
        self.mom = mom
        super().__init__(model, dls, loss_func, lr, cbs, opt_func)

    def zero_grad(self):
        with torch.no_grad():
            for p in self.model.parameters(): p.grad *= self.mom
```


# Example Usage on Simple Data

```{python}
class simpleDS(Dataset):
    def __init__(self,  x, y):
        self.y = y
        self.x = x
    def __len__(self):
        return self.x.shape[0]
    def __getitem__(self, idx):
        return self.x[idx, ], self.y[idx]
```

## Linear response

```{python}
# Make up some data
a0=3
a1=3

n_obs=1000
x_important = torch.randn(n_obs)
xs=torch.concat([x_important[:,None], torch.randn(n_obs,9)],axis=1)
ys=a0+a1*x_important
ys=ys[:, None]
px.scatter(x=x_important, y=ys[:, 0])
```

```{python}
bs = 10
train_dl = DataLoader(simpleDS(xs, ys), batch_size = bs, shuffle = True)
valid_dl = DataLoader(simpleDS(xs, ys), batch_size = bs, shuffle = True)
dls = DataLoaders(train_dl, valid_dl, device=def_device)
model=nn.Sequential(*[nn.Linear(10,1)])
cbs = [DeviceCB()]
learn = MomentumLearner(model, dls, F.mse_loss, lr=1e-5, cbs=cbs)
learn.fit(1, cbs=LRFinderCB())
```

```{python}
learn = MomentumLearner(model, dls, F.mse_loss, lr=0.002, cbs=[DeviceCB(), 
#                                                                metrics
#                                                                , ProgressCB(plot=True)
                                                              ]
                       )
learn.fit(5)

weight_bias_list=[(name, param) for name, param in model.named_parameters()]
weight_bias_list
```

```{python}
layer0w=torch.Tensor.numpy(weight_bias_list[0][1].detach().cpu())
layer0b=torch.Tensor.numpy(weight_bias_list[1][1].detach().cpu())
px.imshow(layer0w)
```

```{python}
ig = IntegratedGradients(model)
attributions, delta = ig.attribute(xs.to('cuda'), torch.zeros(xs.shape).to('cuda'), target=0, return_convergence_delta=True)
```

```{python}
attr = torch.Tensor.numpy(attributions.detach().cpu())
px.imshow(attr[0:5, ])
```

```{python}
px.imshow(np.abs(attr[0:5, ]))
```

## Non-linear Response

```{python}
# harder version with nonlinear response
a0 =  10
a1s = torch.randn(2, 1)
a2s = torch.randn(2, 1)
xs = torch.randn(n_obs, 2)
ys = a0 + ((xs@a1s)**1+(xs@a1s)**2)
```

```{python}
x1x2y = torch.concat([xs, ys], axis = 1)
# px.scatter_3d(x = x1x2y[:, 0], y = x1x2y[:, 1], z = x1x2y[:, 2], color = x1x2y[:, 2])
```

```{python}
train_dl = DataLoader(simpleDS(xs, ys), batch_size = 10, shuffle = True)
valid_dl = DataLoader(simpleDS(xs, ys), batch_size = 10, shuffle = True)
dls = DataLoaders(train_dl, valid_dl, device='cpu')
```

```{python}
# underpowered linear model
# model=nn.Sequential(*[nn.Linear(2,1)])
# learn = MomentumLearner(model, dls, F.mse_loss, lr=0.002, cbs=[DeviceCB()])
# learn.fit(2)

# with torch.no_grad():
#     yhats = (learn.model(torch.Tensor(x1x2y[:, 0:2]).to('cuda'))).detach().cpu()
# px.scatter_3d(x = x1x2y[:, 0], y = x1x2y[:, 1], z = yhats[:, 0], color = yhats[:, 0])
```

```{python}
# Higher capacity  model
model=nn.Sequential(*[nn.Linear(2,10), nn.ReLU(), nn.Linear(10, 1)])
learn = MomentumLearner(model, dls, F.mse_loss, lr=0.002, cbs=[DeviceCB()])
learn.fit(5)

with torch.no_grad():
    yhats = (learn.model(torch.Tensor(x1x2y[:, 0:2]).to('cuda'))).detach().cpu()

# px.scatter_3d(x = x1x2y[:, 0], y = x1x2y[:, 1], z = yhats[:, 0], color = yhats[:, 0])
```

```{python}
# Initialize with kaiming_normal weights
```

```{python}
model=nn.Sequential(*[nn.Linear(2,10), nn.ReLU(), nn.Linear(10, 1)])
learn = MomentumLearner(model, dls, F.mse_loss, lr=0.002#, cbs=[DeviceCB(), metrics, ProgressCB(plot=True)]
                       )
learn.fit(2)
```

```{python}
model.apply(lambda m: print(type(m).__name__));
```

```{python}
def init_weights(m):
    if isinstance(m, (nn.Linear,nn.Linear)): init.kaiming_normal_(m.weight)
```

```{python}
model=nn.Sequential(*[nn.Linear(2,10), nn.ReLU(), nn.Linear(10, 1)])
model.apply(init_weights);
learn = MomentumLearner(model, dls, F.mse_loss, lr=0.002, cbs=[DeviceCB()])
```

```{python}
with torch.no_grad():
    yhats = Tensor.numpy(learn.model(train_dl.dataset.x).detach().cpu())
```

```{python}
learn.fit(20)

with torch.no_grad():
    yhats2 = Tensor.numpy(learn.model(train_dl.dataset.x.to('cuda')).detach().cpu())
```

```{python}
px.histogram(yhats-yhats2)
```


# ApsimX Data

## Loading Data

```{python}
load_from = '../nbs_artifacts/99.99_apsimx_agg/'
met_np = np.load(load_from+'met_np.npy')
met_np_lonlatyear = np.load(load_from+'met_np_lonlatyear.npy')
```

```{python}
soils_i = np.load(load_from+'soils_np_i.npy')
soils_i.shape
```

```{python}
cult_i = np.load(load_from+'SimCult.np.npy')
cult_i.shape
```

```{python}
cult_i_Names = np.load(load_from+'SimCultNames.np.npy')
cult_i_Names
```

```{python}
engine=sqlalchemy.create_engine(f'sqlite:///'+load_from+'res.db')
results = pd.read_sql('select * from Results', engine)
engine.dispose()
results.shape
```

```{python}
# to start look only at one location
mask = (results.Soils_Idx == 309
   ) & (results.Met_Idx == 63993.0)

ys = np.array(results.loc[mask, 'yield_Kgha'])
xs = cult_i[list(results.loc[mask, 'Cult_Idx']), ]
```

```{python}
ys = (ys - np.mean(ys))/np.std(ys)
xs = (xs - np.mean(xs, axis = 0))/np.std(xs, axis = 0)
```


```{python}
# Not much of an effect _except_ for index 4
px.scatter(x = xs[:, 1], y = ys)
```

```{python}
# 'Phenology.GrainFilling.Target.FixedValue', makes sense why it would be the primary driver.
px.scatter(x = xs[:, 4], y = ys)
```

```{python}
ys = torch.from_numpy(ys[:, None]).to(torch.float)
xs = torch.from_numpy(xs).to(torch.float)
```

```{python}
xs = xs[:, 4:5]
```

```{python}
px.scatter(x = xs, y = ys)
```

```{python}
bs=100
```

```{python}
train_dl = DataLoader(
    simpleDS(
        xs[1000:, ], ys[1000:]
    ), #.to(device),
    batch_size = bs, shuffle = True)

valid_dl = DataLoader(
    simpleDS(
        xs[0:1000:, ], ys[0:1000:]
    ), #.to(device),
    batch_size = bs, shuffle = True)
```

```{python}
[e.shape for e in next(iter(train_dl))]
```

```{python}
# dls = DataLoaders(train_dl, valid_dl, device='cpu')
dls = DataLoaders(train_dl, valid_dl, device=def_device)
```

## Testing Learners

```{python}
def get_model(): return nn.Sequential(nn.Linear(10, 10), nn.ReLU(), nn.Linear(10, 1))        
```

```{python}
learn = Learner(nn.Sequential(nn.Linear(1, 16), nn.ReLU(), 
#                               nn.Linear(16, 32), nn.ReLU(), 
#                               nn.Linear(32, 16), nn.ReLU(), 
                              nn.Linear(16, 1)), 
                dls, F.mse_loss, lr=0.02)
# learn.fit(1)
```

```{python}
with torch.no_grad():
    yhat = learn.model(dls.train_ds.x)
    
px.scatter(x = torch.Tensor.numpy(dls.train_ds.y)[:, 0], 
           y = torch.Tensor.numpy(yhat)[:, 0])
```

```{python}
learn.fit(50) # learn.fit isn't updating the predicitons?

with torch.no_grad():
    yhat2 = learn.model(dls.train_ds.x)
    
px.scatter(x = torch.Tensor.numpy(dls.train_ds.y)[:, 0], 
           y = torch.Tensor.numpy(yhat2)[:, 0])
```








## TrainLearner + MomentumLearner

```{python}
model =  get_model()
model = model.to(def_device)

metrics = MetricsCB(accuracy = MeanSquaredError())
cbs = [#TrainCB(), # <- # NB: No TrainCB
    DeviceCB(), metrics, ProgressCB(plot=True)]
learn = MomentumLearner(model, dls, F.mse_loss, lr=0.2, cbs = cbs)
learn.fit(2)
```

## LRFinderCB


```{python}
lrfind = LRFinderCB()

cbs = [DeviceCB(), lrfind]
learn = MomentumLearner(model, dls, F.mse_loss, lr=0.2, cbs = cbs)
learn.fit(1)
```

```{python}
plt.plot(lrfind.lrs, lrfind.losses)
plt.xscale('log')
```

## Adding Schedulers


```{python}
model =get_model().to(def_device)
```

```{python}
# I don't have a clue as to why but this produces a linear response the first time it's run
# the second time it behaves normally.

cbs = [DeviceCB()]
learn = MomentumLearner(model, dls, F.mse_loss, lr=1e-5, cbs=cbs)
learn.fit(3, cbs=LRFinderCB())
```

```{python}
# lr_find was added in lesson 18. It's just a shorter way of using LRFinderCB.
@fc.patch
def lr_find(self:Learner, gamma=1.3, max_mult=3, start_lr=1e-5, max_epochs=10):
    self.fit(max_epochs, lr=start_lr, cbs=LRFinderCB(gamma=gamma, max_mult=max_mult))
```

```{python}
model = get_model()
MomentumLearner(model, dls, F.mse_loss, cbs=cbs).lr_find()
```

```{python}
MomentumLearner(model, dls, F.mse_loss, cbs=cbs).lr_find()
```

# Activations

```{python}
def lin_layers(): return [nn.Linear(10, 10), nn.ReLU(), nn.Linear(10, 1)]
```

```{python}
def fit(model, epochs = 1, xtra_cbs = None):
    # fc.L is a Fastcore list. List that you can index with masks or a list of indices
    learn = Learner(model, dls, F.mse_loss, lr=0.2, cbs = cbs+fc.L(xtra_cbs))
    learn.fit(epochs)
    return learn
```

```{python}
metrics = MetricsCB(accuracy=MeanSquaredError())
cbs = [TrainCB(), DeviceCB(), metrics, ProgressCB(plot=True)]

# model = get_model()

learn = fit(nn.Sequential(*lin_layers()), 4)
```

```{python}
# aside: what do the parameters look like?
```

```{python}
# for name, param in model.named_parameters():
#     print(name, param)
```

```{python}
weight_bias_list=[(name, param) for name, param in model.named_parameters()]
import plotly.express as px
px.imshow(torch.Tensor.numpy(weight_bias_list[0][1].detach().cpu()))
```

```{python}
import plotly.express as px
px.imshow(torch.Tensor.numpy(weight_bias_list[0][1].detach().cpu()))
```

```{python}
np.std(torch.Tensor.numpy(weight_bias_list[0][1].detach().cpu()))
```

```{python}
px.imshow(np.std(torch.Tensor.numpy(weight_bias_list[0][1].detach().cpu()), axis=0)[None,:])
```


## Hooks

### Hooks, Manual

```{python}
class SequentialModel(nn.Module):
    def __init__(self, *layers):
        super().__init__()
        self.layers = nn.ModuleList(layers)
        self.act_means = [[] for _ in layers]
        self.act_stds  = [[] for _ in layers]
        
    def __call__(self, x):                              # This is the big difference
        for i,l in enumerate(self.layers):              # same sort of idea as in forward
            x = l(x)                                    # but activation mean and std 
            self.act_means[i].append(to_cpu(x).mean())  # is tracked
            self.act_stds [i].append(to_cpu(x).std ())  #
        return x                                        #
    
    def __iter__(self): return iter(self.layers)
```

```{python}
model = SequentialModel(*lin_layers())
learn = fit(model, 1)
```

```{python}
for l in model.act_means: plt.plot(l)
plt.legend(range(5));
```

```{python}
for l in model.act_stds: plt.plot(l)
plt.legend(range(5));
```

### Hooks, Pytorch

```{python}
# A hook attaches to a layer and needs
# 1. module
# 2. input
# 3. output
def append_stats(i, mod, inp, outp):
    act_means[i].append(to_cpu(outp).mean())
    act_stds [i].append(to_cpu(outp).std())
```

```{python}
model = SequentialModel(*lin_layers())

act_means = [[] for _ in model]
act_stds  = [[] for _ in model]
```

```{python}
# for each layer, register a hook.
for i,m in enumerate(model): m.register_forward_hook(partial(append_stats, i))
```

```{python}
fit(model, 1)
```

```{python}
for o in act_means: plt.plot(o)
plt.legend(range(5));
```

### Hook, class

```{python}
class Hook():
    def __init__(self, m, f): self.hook = m.register_forward_hook(partial(f, self))
    def remove(self): self.hook.remove()
    def __del__(self): self.remove() # Important to remove hooks when deleted to free up memory when the model is deleted.
```

```{python}
def append_stats(hook, mod, inp, outp):
    if not hasattr(hook,'stats'): hook.stats = ([],[])
    acts = to_cpu(outp)
    hook.stats[0].append(acts.mean())
    hook.stats[1].append(acts.std())
```

```{python}
model = nn.Sequential(*lin_layers())
```

```{python}
hooks = [Hook(l, append_stats) for l in model[:5].children()]
```

```{python}
learn = fit(model)
```

```{python}
for h in hooks:
    plt.plot(h.stats[0])
    h.remove()
plt.legend(range(5));
```

### Hook_s_, class

```{python}
class DummyCtxMgr:
    def __enter__(self, *args):
        print("let's go!")
        return self
    def __exit__ (self, *args): print("all done!")
    def hello(self): print("hello.")

with DummyCtxMgr() as dcm: dcm.hello()
```

```{python}
class DummyList(list):
    def __delitem__(self, i):
        print(f"Say bye to item {i}")
        super().__delitem__(i)
        
dml = DummyList([1,3,2])
print(dml)
del(dml[2])
dml
```

```{python}
# updated version, previous wasn't based on list.
class Hooks(list):
    def __init__(self, ms, f): super().__init__([Hook(m, f) for m in ms])
    def __enter__(self, *args): return self
    def __exit__ (self, *args): self.remove()
    def __del__(self): self.remove()
    def __delitem__(self, i):
        self[i].remove()
        super().__delitem__(i)
    def remove(self):
        for h in self: h.remove()
```

```{python}
model = nn.Sequential(*lin_layers())
```

```{python}
with Hooks(model, append_stats) as hooks:
    fit(model)
    fig,axs = plt.subplots(1,2, figsize=(10,4))
    for h in hooks:
        for i in 0,1: axs[i].plot(h.stats[i])
    plt.legend(range(6));
```

### HooksCallback (allows filtering)

```{python}
class HooksCallback(Callback):
    def __init__(self, hookfunc, mod_filter=fc.noop, on_train=True, on_valid=False, mods=None):
        fc.store_attr()
        super().__init__()
    
    def before_fit(self, learn):
        if self.mods: mods=self.mods
        else: mods = fc.filter_ex(learn.model.modules(), self.mod_filter)
        self.hooks = Hooks(mods, partial(self._hookfunc, learn))

    def _hookfunc(self, learn, *args, **kwargs):
        if (self.on_train and learn.training) or (self.on_valid and not learn.training): self.hookfunc(*args, **kwargs)

    def after_fit(self, learn): self.hooks.remove()
    def __iter__(self): return iter(self.hooks)
    def __len__(self): return len(self.hooks)
```

```{python}
hc = HooksCallback(append_stats, mod_filter=fc.risinstance(nn.Linear))
```

```{python}
model = nn.Sequential(*lin_layers())
fit(model, xtra_cbs=[hc]);
```

```{python}
fig,axs = plt.subplots(1,2, figsize=(10,4))
for h in hc:
    for i in 0,1: axs[i].plot(h.stats[i])
plt.legend(range(6));
```

### Histograms

```{python}
def append_stats(hook, mod, inp, outp):
    if not hasattr(hook,'stats'): hook.stats = ([],[],[])
    acts = to_cpu(outp)
    hook.stats[0].append(acts.mean())
    hook.stats[1].append(acts.std())
    hook.stats[2].append(acts.abs().histc(40,0,10))
```

```{python}
model = nn.Sequential(*lin_layers())
hc = HooksCallback(append_stats, mod_filter=fc.risinstance(nn.Linear))
fit(model, xtra_cbs=[hc]);
```


```{python}
model[0].parameters
```

```{python}
import matplotlib.pyplot as plt
```

```{python}
def get_hist(h): return torch.stack(h.stats[2]).t().float().log1p()
```

```{python}
fig, axes = get_grid(len(hc), figsize=(11,5))
```

```{python}
h, _ = hc

[len(e) for e in h.stats]
```

```{python}
h_concat = [
    torch.concat([e.reshape(1) for e in h.stats[0]]),
    torch.concat([e.reshape(1) for e in h.stats[1]]),
    torch.concat([e[:, None] for e in h.stats[2]], axis = 1)
]
[e.shape for e in h_concat]
```

```{python}
plt.imshow(torch.Tensor.numpy(h_concat[0])[:, None])
```

```{python}
plt.imshow(torch.Tensor.numpy(h_concat[1])[:, None])
```

```{python}
plt.imshow(torch.Tensor.numpy(h_concat[2]))
```

## Initalizations


## Init with LSUV

```{python}
def _lsuv_stats(hook, mod, inp, outp):
#     acts = to_cpu(outp)
    acts = outp
    hook.mean = acts.mean()
    hook.std = acts.std()
    # added safety to not fully normalize instead of crashing
    # in test case the linear weight matrix is becoming inf/-inf
    if (torch.isnan(hook.std) or
        hook.std == 0):
        hook.std = 1
    
#     print(acts.std())

def lsuv_init(model, m, m_in, xb):
    print(m)
    h = Hook(m, _lsuv_stats)
    with torch.no_grad():
        while model(xb) is not None and (abs(h.std-1)>1e-3 or abs(h.mean)>1e-3):
            m_in.bias -= h.mean
            m_in.weight.data /= h.std
    h.remove()
```

```{python}
model=nn.Sequential(*[nn.Linear(2,10), nn.ReLU(), nn.Linear(10, 1)])
relus = [o for o in model.modules() if isinstance(o, nn.ReLU)]
lins = [o for o in model.modules() if isinstance(o, nn.Linear)]
```

```{python}
# weight_bias_list=[(name, param) for name, param in model.named_parameters()]; weight_bias_list
```

```{python}
model
```

```{python}
xb,yb = next(iter(dls.train))
xb
for ms in zip(relus,lins): lsuv_init(model, *ms, xb) #.to(def_device))
```

```{python}
weight_bias_list=[(name, param) for name, param in model.named_parameters()]; weight_bias_list
```


```{python}
learn = MomentumLearner(model, dls, F.mse_loss, lr=0.002, 
                        cbs=[DeviceCB(), metrics, ProgressCB(plot=True)]
                       )
learn.fit(2)
```

```{python}
weight_bias_list=[(name, param) for name, param in model.named_parameters()]; weight_bias_list
```

```{python}
weight_list = [#torch.Tensor.numpy(
    e[1].detach().cpu() for e in weight_bias_list if e[0].split('.')[1] == 'weight']
weight_list[0]
```


```{python}
px.imshow(torch.Tensor.numpy(
    torch.concat(
        [e.histc(20,0,10)[None, :] for e in weight_list], 
        axis = 0)))
```

```{python}
np.histogram([1, 2, 1], bins=[0, 1, 2, 3])
```

```{python}
np.histogram(weight_list[0], 
             bins=[0, 1, 10],
             density=True)
```







#

```{python}
# class apsimxDataset(Dataset):
#     def __init__(self, lookup_smc, ph, ys, soil, met, cult, device):
#         self.lookup_smc = lookup_smc
#         self.ph = ph
#         self.ys = ys
#         self.soil = soil
#         self.met = met
#         self.cult = cult
#         self.device = device
        
#     def __len__(self):
#         return self.ys.shape[0]
    
#     def __getitem__(self, idx):
#         s_i, m_i, c_i = self.lookup_smc[idx, ]
        
#         met_plant = torch.tensor(
#             [0.0 if ((i < int(self.ph[idx, 0])) or (i > int(self.ph[idx, 1]))) else 1.0
#             for i in range(365)])
#         if self.device == 'cuda':
#             met_plant = met_plant.to('cuda')
        
#         return self.ys[idx, ], self.soil[s_i, ], torch.concat([self.met[m_i, ], met_plant[:, None]], axis = 1), self.cult[c_i, ]

# device = 'cuda'
```

```{python}
class apsimxMet(Dataset):
    def __init__(self, met):
        self.met = met
    def __len__(self):
        return self.met.shape[0]
    def __getitem__(self, idx):
        return self.met[idx, ]

    
```

```{python}
# training_dataloader = DataLoader(apsimxMet(
#     met = torch.from_numpy(met_np).to(torch.float), #.to(device),
# ), 
#                                  batch_size = 25,
#                                  shuffle = True)
```

```{python}
data = DataLoaders(DataLoader(apsimxMet(
                met = torch.from_numpy(met_np[0:38, ]).to(torch.float), #.to(device),
            ), batch_size = 25, shuffle = True),
                   
            DataLoader(apsimxMet(
                met = torch.from_numpy(met_np[38:76, ]).to(torch.float), #.to(device),
            ), batch_size = 25, shuffle = True)
           )
```

```{python}
mod
```

```{python}
learn = Learner(data, Net(), loss_func=F.nll_loss, opt_func=Adam, metrics=accuracy)
```

```{python}
from torch import nn
mod = 

nn.Sequential(
    nn.Flatten(),
    nn.Linear(365*8, 1024),
    nn.ReLU(), 
    nn.Linear(1024, 365*8)
)


# mod(next(iter(training_dataloader)))
```

```{python}
# restrict to one location
met_np = met_np[0:38, ]
```


```{python}
import plotly.express as px
px.imshow(met_np[0].T[:, 0:30])
```

```{python}
cs = [met_np.mean(axis = 0), met_np.std(axis = 0)]
met_np = (met_np - cs[0])/cs[1]
```

```{python}
[e.shape for e in cs]
```


```{python}
px.imshow(next(iter(training_dataloader))[0].detach().cpu().T[:, 0:30])
```

```{python}
next(iter(training_dataloader)).shape
```


```{python}
from torch import nn
mod = nn.Sequential(
    nn.Flatten(),
    nn.Linear(365*8, 512),
    nn.ReLU(), 
    nn.Linear(512, 10),
    nn.ReLU(), 
    nn.Linear(10, 1)
)

mod(next(iter(training_dataloader)))
```



```{python}
class myLearn:
    def __init__(
        self, 
        model, 
        dl, # this is the basic dataloader
        loss_func,
        lr, 
        opt_func= torch.optim.SGD):
        self.model = model
        self.dl = dl
        self.loss_func = loss_func
        self.lr = lr
        self.opt_fun = opt_fun
    
    def one_batch():
```

```{python}
class Learner:
    def __init__(self, model, dls, loss_func, lr, opt_func=optim.SGD): fc.store_attr()

    def one_batch(self):
        self.xb,self.yb = to_device(self.batch)
        self.preds = self.model(self.xb)
        self.loss = self.loss_func(self.preds, self.yb)
        if self.model.training:
            self.loss.backward()
            self.opt.step()
            self.opt.zero_grad()
        with torch.no_grad(): self.calc_stats()

    def calc_stats(self):
        acc = (self.preds.argmax(dim=1)==self.yb).float().sum()
        self.accs.append(acc)
        n = len(self.xb)
        self.losses.append(self.loss*n)
        self.ns.append(n)

    def one_epoch(self, train):
        self.model.training = train
        dl = self.dls.train if train else self.dls.valid
        for self.num,self.batch in enumerate(dl): self.one_batch()
        n = sum(self.ns)
        print(self.epoch, self.model.training, sum(self.losses).item()/n, sum(self.accs).item()/n)
    
    def fit(self, n_epochs):
        self.accs,self.losses,self.ns = [],[],[]
        self.model.to(def_device)
        self.opt = self.opt_func(self.model.parameters(), self.lr)
        self.n_epochs = n_epochs
        for self.epoch in range(n_epochs):
            self.one_epoch(True)
            with torch.no_grad(): self.one_epoch(False)
```



```{python}
# import torch
# from torch.utils.data import Dataset
# from torch.utils.data import DataLoader
# from torch import nn

# import torch.nn.functional as F # used in VAE



# lookup_smc = torch.from_numpy(np.array(res_lookup).astype(int))
# ph   = torch.from_numpy(np.array(res_plantharvest))
# ys   = torch.from_numpy(np.array(res_ys.loc[:, 'yield_Kgha']))
# soil = torch.from_numpy(np.array(soils_np_i)).swapaxes(1,2)
# met  = torch.from_numpy(np.array(met_np))
# cult = torch.from_numpy(np.array(cult_np))


# def cs_tensor(M):
#     mu_std = [M.mean(axis = 0), M.std(axis = 0)]
#     mu_std[1][mu_std[1] == 0] = 1
#     M_new = (M-mu_std[0])/mu_std[1]
#     return(mu_std, M_new)

# ys_cs,   ys   = cs_tensor(M = ys) 
# soil_cs, soil = cs_tensor(M = soil) 
# met_cs,  met  = cs_tensor(M = met) 
# cult_cs, cult = cs_tensor(M = cult) 





# training_dataloader = DataLoader(
#     apsimxDataset(
# #         lookup_smc = lookup_smc,
# #         ph = ph,
# #         ys = ys.to(torch.float),
# #         soil = soil.to(torch.float),
# #         met = met.to(torch.float),
# #         cult = cult.to(torch.float),
# #         device = 'cpu'
#         lookup_smc = lookup_smc.to(device),
#         ph = ph.to(device),
#         ys = ys.to(device).to(torch.float),
#         soil = soil.to(device).to(torch.float),
#         met = met.to(device).to(torch.float),
#         cult = cult.to(device).to(torch.float),
#         device = 'cuda'
#     ),
#     batch_size = 500,
#     shuffle = True
# )



# ys, soil, met, cult = next(iter(training_dataloader))
# [e.shape for e in [ys, soil, met, cult]]

# class invApsimxNet(nn.Module):
#     def __init__(self):
#         super(invApsimxNet, self).__init__()
# #         self.ynet = nn.Sequential(nn.Flatten())
#         self.snet = nn.Sequential(
#             nn.Conv1d(24, out_channels=36,
#                       kernel_size= 3, stride= 2, padding  = 1),
#             nn.BatchNorm1d(36),
#             nn.LeakyReLU(), 
#             nn.Conv1d(36, out_channels=64,
#                       kernel_size= 3, stride= 2, padding  = 1),
#             nn.BatchNorm1d(64),
#             nn.LeakyReLU(), 
#             nn.Conv1d(64, out_channels=128,
#                       kernel_size= 3, stride= 2, padding  = 1),
#             nn.BatchNorm1d(128),
#             nn.LeakyReLU(), 
#             nn.Conv1d(128, out_channels=256,
#                       kernel_size= 3, stride= 2, padding  = 1),
#             nn.BatchNorm1d(256),
#             nn.LeakyReLU(),  
#             nn.Conv1d(256, out_channels=512,
#                       kernel_size= 3, stride= 2, padding  = 1),
#             nn.BatchNorm1d(512),
#             nn.LeakyReLU(), 
#             nn.Flatten() # 3584 out
# )
#         self.mnet = nn.Sequential(
#             nn.LSTM(
#                 input_size = 9,
#                 hidden_size= 1, # hidden state features
#                 num_layers = 1, # number of lstms stacked
#                 bias = True,
#                 batch_first = True, 
#                 dropout = 0, # dropouts between layers
#                 bidirectional = False
#             )
#         )
#         self.mnet_f = nn.Flatten()
        
#         self.xnet = nn.Sequential(
#             nn.Linear(3950, 512),
#             nn.ReLU(),
#             nn.Linear(512, 64),
#             nn.ReLU(),
#             nn.Linear(64, 10)
#         )

#     def forward(self, ys, soil, met):
# #         y_out = self.ynet(ys)
#         y_out = ys
#         s_out = self.snet(soil)
#         m_out, _ = self.mnet(met)
#         m_out = self.mnet_f(m_out)
#         x_out = self.xnet(torch.concat([y_out[:, None], s_out, m_out], axis = 1))
#         return(x_out)
# # net = invApsimxNet()
# # net = invApsimxNet().to(device)
# # net(ys, soil, met).shape

# def train_loop(dataloader, model, loss_fn, optimizer, silent = False):
#     import torch
#     from torch.utils.data import Dataset
#     from torch.utils.data import DataLoader
#     size = len(dataloader.dataset)
#     for batch, (ys, soil, met, cult) in enumerate(dataloader):
#         # Compute prediction and loss
#         pred = model(ys, soil, met)
#         loss = loss_fn(pred, cult)

#         # Backpropagation
# #         torch.autograd.set_detect_anomaly(True)
#         optimizer.zero_grad()
# #         loss.backward(retain_graph=True)
#         loss.backward()
#         optimizer.step()

#         if batch % 100 == 0:
#             loss, current = loss.item(), batch * len(ys)
#             if not silent:
#                 print(f"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]")

# def train_error(dataloader, model, loss_fn, silent = False):
#     import torch
#     from torch.utils.data import Dataset
#     from torch.utils.data import DataLoader
    
#     size = len(dataloader.dataset)
#     num_batches = len(dataloader)
#     train_loss = 0

#     with torch.no_grad():
#         for ys, soil, met, cult in dataloader:
#             pred = model(ys, soil, met)
#             train_loss += loss_fn(pred, cult).item() # <----------------------
            
#     train_loss /= num_batches
#     return(train_loss)


# def test_loop(dataloader, model, loss_fn, silent = False):
#     import torch
#     from torch.utils.data import Dataset
#     from torch.utils.data import DataLoader
    
#     size = len(dataloader.dataset)
#     num_batches = len(dataloader)
#     test_loss = 0

#     with torch.no_grad():
#         for ys, soil, met, cult in dataloader:
#             pred = model(ys, soil, met)                
#             test_loss += loss_fn(pred, cult).item() # <-----------------------

#     test_loss /= num_batches
#     if not silent:
#         print(f"Test Error: Avg loss: {test_loss:>8f}")
#     return(test_loss)


# def train_nn(
#     cache_path,
#     training_dataloader,
#     testing_dataloader,
#     model,
#     learning_rate = 1e-3,
#     batch_size = 64,
#     epochs = 500,
#     model_prefix = 'model',
#     save_pt = False
# ):
#     import numpy as np
#     import pandas as pd
#     import torch
#     from torch import nn
#     from tqdm import tqdm
    
#     # Initialize the loss function
#     loss_fn = nn.MSELoss()

# #     optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)
#     optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas= [0.98752, 0.972311])

#     loss_df = pd.DataFrame([i for i in range(epochs)], columns = ['Epoch'])
#     loss_df['TrainMSE'] = np.nan
#     loss_df['TestMSE']  = np.nan

#     for t in tqdm(range(epochs)):        
# #         print(f"Epoch {t+1}\n-------------------------------")
#         train_loop(training_dataloader, model, loss_fn, optimizer, silent = True)

#         loss_df.loc[loss_df.index == t, 'TrainMSE'
#                    ] = train_error(training_dataloader, model, loss_fn, silent = True)
        
#         loss_df.loc[loss_df.index == t, 'TestMSE'
#                    ] = test_loop(testing_dataloader, model, loss_fn, silent = True)
        
#         if (t+1)%5 == 0: # Cache in case training is interupted. 
#             # print(loss_df.loc[loss_df.index == t, ['TrainMSE', 'TestMSE']])
#             if save_pt:
#                 torch.save(model.state_dict(), 
#                            cache_path+'/'+model_prefix+'_'+str(t)+'_'+str(epochs)+'.pt') # convention is to use .pt or .pth
        
#     return([model, loss_df])


# def yhat_loop(dataloader, model):
#     import numpy as np
#     import pandas as pd
#     import torch
    
#     size = len(dataloader.dataset)
#     num_batches = len(dataloader)
    
#     y_true = np.array([])
#     y_pred = np.array([])
    
#     with torch.no_grad():
#         for ys, soil, met, cult in dataloader:
#             yhat_i = model(ys, soil, met)
#             y_pred = np.append(y_pred, np.array(yhat_i.cpu()))
#             y_true = np.append(y_true, np.array(cult.cpu()))
    
#     out = np.concatenate([y_true[:, None], y_pred[:, None]], axis = 1) 
#     out = pd.DataFrame(out, columns = ['y_true', 'y_pred'])
#     return(out)

# model = invApsimxNet().to(device)

# model, loss_df = train_nn(
#         cache_path,
#         training_dataloader,
#         training_dataloader,
#         model,
#         learning_rate = 1e-3,
#         batch_size = 500,
#         epochs = 1
#     )
```











