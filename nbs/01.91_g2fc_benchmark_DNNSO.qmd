---
title: Benchmark DNNSO
jupyter: python3
---

```{python}
# import time
# time.sleep(30*60)
```


> 


```{python}
import os, json, re

import numpy as np
import pandas as pd
# pd.set_option('display.max_columns', None)

import plotly.express as px
import plotly.io as pio
pio.templates.default = "plotly_white"

from EnvDL.core import * # includes remove_matching_files
from EnvDL.dna import *
from EnvDL.dlfn import *

from tqdm import tqdm
```

```{python}
# dataloader_batch_size = 8 #16 #64
# run_epochs = 200

use_gpu_num = 0

# Imports --------------------------------------------------------------------
import torch
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
from torch import nn

device = "cuda" if torch.cuda.is_available() else "cpu"
if use_gpu_num in [0, 1]: 
    torch.cuda.set_device(use_gpu_num)
print(f"Using {device} device")
```

```{python}
cache_path = '../nbs_artifacts/01.91_g2fc_benchmark_DNNSO/'
ensure_dir_path_exists(dir_path = cache_path)
```

## Load data

```{python}
load_from = '../nbs_artifacts/01.03_g2fc_prep_matrices/'
phno_geno = pd.read_csv(load_from+'phno_geno.csv')
phno = phno_geno

obs_env_lookup  = np.load(load_from+'obs_env_lookup.npy')
obs_geno_lookup = np.load(load_from+'obs_geno_lookup.npy') # Phno_Idx	Geno_Idx	Is_Phno_Idx
YMat = np.load(load_from+'YMat.npy')
SMat = np.load(load_from+'SMat.npy')
WMat = np.load(load_from+'WMat.npy')
# PlantHarvestNames = np.load(load_from+'PlantHarvestNames.npy') 
PlantHarvest = np.load(load_from+'PlantHarvest.npy')  # 'DOY_Planted', 'DOY_Harvested'
```

### Screeplot

```{python}
load_from = '../data/zma/g2fc/genotypes_G3_filter/'

GScree = pd.read_table(load_from+'Eigenvalues_Imputed_5_Genotype_Data_All_Years_G3_Hz_0.0675.txt',
                     sep = '\t',
                     low_memory = False)
GScree

fig = px.line(GScree, x = 'PC', y = 'cumulative proportion', log_x=True)
pr_bins = [(2+i)/10 for i in range(8)]
for i in range(len(pr_bins)-1):
    x_start = GScree.loc[(GScree['cumulative proportion'] >= pr_bins[i]), 'PC'].min()
    x_stop = GScree.loc[(GScree['cumulative proportion'] >= pr_bins[i+1]), 'PC'].min()
   
    if i%2 == 1:
        fig.add_vrect(x0=x_start, 
                      x1=x_stop, 
                      fillcolor="green", opacity=0.25, line_width=0)
    else:
        fig.add_vrect(x0=x_start, 
                      x1=x_stop, 
                      fillcolor="white", opacity=0.25, line_width=0)
fig.show()
```

### PCA Transformed Data

```{python}
GMat = pd.read_table(load_from+'PC_Imputed_5_Genotype_Data_All_Years_G3_Hz_0.0675.txt',
                     sep = '\t',
                     skiprows=2,
                    low_memory = False)
GMat.shape # two more entries than the data I've been using.
```

```{python}
# need to check taxa match
assert 0 == len([e for e in list(set(phno_geno['Hybrid'])) if e not in list(GMat['Taxa'])])
```

```{python}
# Get the desired genotypes in the desired order
temp = pd.concat([phno_geno['Hybrid'], pd.DataFrame(obs_geno_lookup)], axis = 1)
temp = temp.loc[:, ['Hybrid', 1]].drop_duplicates().sort_values(1).reset_index(drop = True)
temp
```

```{python}
# Reorder PCA data, then convert to numpy
GMat = temp.merge(GMat.rename(columns = {'Taxa':'Hybrid', 'index':'Gpca_Idx'})).drop(columns = 1)
GMatHybrid = GMat['Hybrid']
GMat = GMat.loc[:, [i for i in list(GMat) if i != 'Hybrid']]
```

```{python}
GMat = np.asarray(GMat)
```

```{python}
# We should now be able to use obs_geno_lookup to lookup PCA values.
assert GMat.shape[0] == len(set(obs_geno_lookup[:, 1]))
```


### Custom Dataloader for G S W

```{python}
window_start = PlantHarvest[:, 0]-76 # 75 before planting date
window_end   = PlantHarvest[:, 0]+212 # 212 after (288 total per W.npy in paper)
window_end - window_start
```

```{python}
# the increased range of planting dates causes the window to extend beyond the target year. 
[window_start.min(), 
 (window_end.max() - 364)] #pd.to_datetime() is 0 indexed
```

```{python}
window_start = PlantHarvest[:, 0]-60
window_end   = PlantHarvest[:, 0]+194
window_end - window_start
```

```{python}
assert window_start.min() >= 0 
```

```{python}
assert window_end.max() <= 364 #pd.to_datetime() is 0 indexed
```


```{python}
# WMat.shape
```

```{python}
# # obs_env_lookup  = np.load(load_from+'obs_env_lookup.npy')
# # obs_geno_lookup = np.load(load_from+'obs_geno_lookup.npy') # Phno_Idx	Geno_Idx	Is_Phno_Idx
# (obs_env_lookup.shape,
# obs_geno_lookup.shape)
```



```{python}
class GSWDataset(Dataset): # for any G containing matix with many (phno) to one (geno)
    def __init__(self, 
                 y, 
                 G, # not on gpu
                 S,
                 W,
                 
                 planting, # should be the vector so [:, 0]
                 days_before_planting,
                 days_after_planting,
                 
                 idx_original,
                 idx_lookup_geno,
                 idx_lookup_env,
                 
                 transform = None, target_transform = None,
                 device = 'cuda',
                 **kwargs 
                ):

        self.device = device
        self.y = y 
        self.G = G
        self.S = S
        self.W = W
        
        self.window_start = planting - days_before_planting
        self.window_end   = planting + days_after_planting
        
        self.idx_original = idx_original
#         self.idx_lookup = idx_lookup
        self.idx_lookup_geno = idx_lookup_geno
        self.idx_lookup_env  = idx_lookup_env
        
        self.transform = transform
        self.target_transform = target_transform    
        
    def __len__(self):
        return len(self.y)
    
    def __getitem__(self, idx):
        y_idx =self.y[idx]
            
        #                 |array containing correct index in deduplicated g 
        #                 |               index in phno    
        uniq_g_idx = self.idx_lookup_geno[
            self.idx_original[idx], 1]
        g_idx = self.G[uniq_g_idx, :]
        
        
        #                 |array containing correct index in deduplicated w 
        #                 |               index in phno    
        uniq_env_idx = self.idx_lookup_env[self.idx_original[idx], 1]
        s_idx = self.S[uniq_env_idx, :]
        w_idx = self.W[uniq_env_idx, :, self.window_start[idx]:self.window_end[idx]]
        
        # send all to gpu        
        if (self.device != 'cpu'):
            if y_idx.device.type == 'cpu':
                y_idx = y_idx.to(self.device) 
                
            if g_idx.device.type == 'cpu':
                g_idx = g_idx.to(self.device) 
                
            if s_idx.device.type == 'cpu':
                s_idx = s_idx.to(self.device)                 
                
            if w_idx.device.type == 'cpu':
                w_idx = w_idx.to(self.device)         
        
        if self.transform:
            y_idx = self.transform(y_idx)
            g_idx = self.transform(g_idx)
            s_idx = self.transform(s_idx)
            w_idx = self.transform(w_idx)
            
        return y_idx, g_idx, s_idx, w_idx
    
# training_dataloader = DataLoader(
#     GSWDataset(
#         y = torch.from_numpy(YMat[[0, 1], ]), 
#         G = torch.from_numpy(GMat), 
#         S = torch.from_numpy(SMat),
#         W = torch.from_numpy(WMat),
#         planting = PlantHarvest[:, 0],
#         days_before_planting = 60,
#         days_after_planting = 194,
#         idx_original = torch.from_numpy(np.array([0, 1]))[:, None],
#         idx_lookup_geno = obs_geno_lookup,
#         idx_lookup_env = obs_env_lookup,
#         device = 'cuda'),
#     batch_size = 1,
#     shuffle = True
# )
# next(iter(training_dataloader))
```

```{python}
## Create train/test validate indicies from json
load_from = '../nbs_artifacts/01.06_g2fc_cluster_genotypes/'
```

```{python}
def read_json(json_path):
    with open(json_path, 'r') as fp:
        dat = json.load(fp)
    return(dat)

def read_split_info(
    load_from = '../nbs_artifacts/01.06_g2fc_cluster_genotypes/',
    json_prefix = '2023:9:5:12:8:26'):
    jsons = [e for e in os.listdir(load_from) if re.match('^'+json_prefix+'.+\.json$', e)]
    vals = [e for e in jsons if re.match('.+val\d+\.json$', e)]
    vals.sort()
    out = {}
    out['test'] = [read_json(json_path = load_from+json_prefix+'-test.json')]
    out['test_file'] = [json_prefix+'-test.json']
    out['validate'] = [read_json(json_path = load_from+val) for val in vals]
    out['validate_files'] = [val for val in vals]
    return(out)


def find_idxs_split_dict(
    obs_df, # assumes presence of Year, Female, Male
    split_dict # from read_split_info() output. Should be a test of validate dict.
):

    temp = obs_df
    test_mask = ((temp.Year.isin(split_dict['test_years'])) & 
                 ((temp.Female.isin(split_dict['test_parents'])) |
                  (temp.Male.isin(split_dict['test_parents']))))
    temp['Split'] = ''
    temp.loc[test_mask, 'Split'] = 'Test'

    train_mask = (~(temp.Year.isin(split_dict['test_years'])) & 
                 (~((temp.Female.isin(split_dict['test_parents'])) |
                  (temp.Male.isin(split_dict['test_parents'])))))
    temp.loc[train_mask, 'Split'] = 'Train'

    temp_test  = (temp.Split == 'Test') # should be the same as with the mask above
    temp_train = (temp.Split == 'Train') # should be the same as with the mask above

    # Confirm that there's no overlap in parents or years
    temp_test_parents  = set(temp.loc[temp_test, 'Female']+temp.loc[temp_test, 'Male'])
    temp_train_parents = set(temp.loc[temp_train, 'Female']+temp.loc[temp_train, 'Male'])

    temp_test_years  = set(temp.loc[temp_test, 'Year'])
    temp_train_years = set(temp.loc[temp_train, 'Year'])

    assert [] == [e for e in temp_test_parents if e in temp_train_parents]
    assert [] == [e for e in temp_train_parents if e in temp_test_parents]
    assert [] == [e for e in temp_test_years if e in temp_train_years]
    assert [] == [e for e in temp_train_years if e in temp_test_years]

    return({
        'test_idx': temp.loc[test_mask, ].index, 
        'train_idx': temp.loc[train_mask, ].index} )
```

```{python}
split_info = read_split_info(
    load_from = '../nbs_artifacts/01.06_g2fc_cluster_genotypes/',
    json_prefix = '2023:9:5:12:8:26')
```

```{python}
temp = phno.copy()
temp[['Female', 'Male']] = temp['Hybrid'].str.split('/', expand = True)
```

```{python}
test_dict = find_idxs_split_dict(
    obs_df = temp, 
    split_dict = split_info['test'][0]
)
# test_dict
```

```{python}
# since this is applying predefined model structure no need for validation.
# This is included for my future reference when validation is needed.
temp = temp.loc[test_dict['train_idx'], ] # restrict before re-aplying

val_dict = find_idxs_split_dict(
    obs_df = temp, 
    split_dict = split_info['validate'][0]
)
# val_dict
```

```{python}
test_dict
```

```{python}
train_idx = test_dict['train_idx']
test_idx  = test_dict['test_idx']
```





## One Hot Encoded

```{python}
# rng = np.random.default_rng(9230473) # note, must use rng.shuffle(arr) below for this to take effect.
```

```{python}
# pct_cumsum_thresh = .9

# # make a df to aid in creating train/test splits
# # the plan is to shuffle the rows of the df, calculate the cumulative sum of the percents obs, then 
# # the entries above and below a given percent will be the train/test.
# obs_per_hybrid = phno.assign(n = 1).groupby('Hybrid').count().reset_index()
# obs_per_hybrid = obs_per_hybrid.loc[:, ['Hybrid', 'n']]
# obs_per_hybrid['pct'] = obs_per_hybrid.n / obs_per_hybrid.n.sum()
# obs_per_hybrid['pct_cumsum'] = np.nan
# obs_per_hybrid['random_order'] = 0

# obs_per_hybrid
```

```{python}
# # fill in the random values to sort on
# arr = np.arange(obs_per_hybrid.shape[0])
# rng.shuffle(arr)
# obs_per_hybrid.random_order = arr

# obs_per_hybrid = obs_per_hybrid.sort_values('random_order')
# obs_per_hybrid['pct_cumsum'] = obs_per_hybrid.pct.cumsum()
# obs_per_hybrid
```

```{python}
# # Convert back into phno indices
# train_hybrids = list(obs_per_hybrid.loc[(obs_per_hybrid.pct_cumsum <= pct_cumsum_thresh), 'Hybrid'])
# test_hybrids  = list(obs_per_hybrid.loc[(obs_per_hybrid.pct_cumsum >  pct_cumsum_thresh), 'Hybrid'])

# train_idx = phno.loc[(phno.Hybrid.isin(train_hybrids)), ].index
# test_idx  = phno.loc[(phno.Hybrid.isin(test_hybrids)), ].index
```

```{python}
[len(train_idx), len(test_idx)]
```

```{python}
# confirm all observation idxs are have genomic information
assert [] == [e for e in list(train_idx)+list(test_idx) if e not in obs_geno_lookup[:, 0]]
```

```{python}
YMat_cs = calc_cs(YMat[train_idx])
y_cs = apply_cs(YMat, YMat_cs)
```

```{python}
training_dataloader = DataLoader(
    GSWDataset(
        y = torch.from_numpy(y_cs[train_idx])[:, None].to(torch.float32), #torch.from_numpy(YMat[[0, 1], ]), 
        G = torch.from_numpy(GMat).to(torch.float32), 
        S = torch.from_numpy(SMat).to(torch.float32),
        W = torch.from_numpy(WMat).to(torch.float32),
        planting = PlantHarvest[:, 0],
        days_before_planting = 60,
        days_after_planting = 194,
        idx_original = torch.from_numpy(np.array(train_idx)),#[:, None],
        idx_lookup_geno = obs_geno_lookup,
        idx_lookup_env = obs_env_lookup,
        device = 'cuda'),
    batch_size = 1000,
    shuffle = True
)
# next(iter(training_dataloader))
```

```{python}
testing_dataloader = DataLoader(
    GSWDataset(
        y = torch.from_numpy(y_cs[test_idx])[:, None].to(torch.float32), #torch.from_numpy(YMat[[0, 1], ]), 
        G = torch.from_numpy(GMat).to(torch.float32), 
        S = torch.from_numpy(SMat).to(torch.float32),
        W = torch.from_numpy(WMat).to(torch.float32),
        planting = PlantHarvest[:, 0],
        days_before_planting = 60,
        days_after_planting = 194,
        idx_original = torch.from_numpy(np.array(test_idx)),#[:, None],
        idx_lookup_geno = obs_geno_lookup,
        idx_lookup_env = obs_env_lookup,
        device = 'cuda'),
    batch_size = 1000,
    shuffle = True
)
# next(iter(testing_dataloader))
```

```{python}
y_idx, g_idx, s_idx, w_idx = next(iter(training_dataloader))
```

```{python}
class NeuralNetwork(nn.Module):
    def __init__(self):
        super(NeuralNetwork, self).__init__()    

        def Linear_block(in_size, out_size, drop_pr):
            block = nn.Sequential(
                nn.Linear(in_size, out_size),
                nn.ReLU(),
                nn.Dropout(drop_pr)
            )
            return(block)         
        
        
        def Conv1D_x2_Max_block(in_channels, out_channels, kernel_size, stride, maxpool_size):
            block = nn.Sequential(
                nn.Conv1d(
                    in_channels= in_channels, # second channel
                    out_channels= out_channels,
                    kernel_size= kernel_size,
                    stride= stride
                ), 
                nn.Conv1d(
                    in_channels= out_channels, 
                    out_channels= out_channels,
                    kernel_size= kernel_size,
                    stride= stride
                ), 
                nn.BatchNorm1d(out_channels),
                nn.MaxPool1d((maxpool_size), stride=stride)
            )
            return(block)
        
        self.g_network =nn.Sequential(
            Linear_block(in_size= 4881, out_size= 196, drop_pr= 0.15214),
            Linear_block(in_size= 196,   out_size= 27, drop_pr= 0.06061)
        ) 
        
        self.s_network =nn.Sequential(
            Linear_block(in_size= 23, out_size= 19, drop_pr= 0.21342),
            Linear_block(in_size= 19, out_size= 27, drop_pr= 0.18589)
        )        
    
        self.w_network =nn.Sequential(
            Conv1D_x2_Max_block(
                in_channels = 16, 
                out_channels = 370, 
                kernel_size = 3,
                maxpool_size = 2,
                stride = 1),
            Conv1D_x2_Max_block(
                in_channels = 370, 
                out_channels = 303, 
                kernel_size = 3,
                maxpool_size = 2,
                stride = 1)
        )
        
        self.w_flatten =nn.Flatten()
        
        self.x_network =nn.Sequential(
            Linear_block(in_size= 73986, out_size= 10,  drop_pr= 0.10201),
            Linear_block(in_size= 10,    out_size= 25,  drop_pr= 0.14809),
            Linear_block(in_size= 25,    out_size= 126, drop_pr= 0.01536),
            Linear_block(in_size= 126,   out_size= 204, drop_pr= 0.15658),
            Linear_block(in_size= 204,   out_size= 45,  drop_pr= 0.2428),
            Linear_block(in_size= 45,    out_size= 134, drop_pr= 0.19048),
            Linear_block(in_size= 134,   out_size= 1,   drop_pr= 0.0)
        ) 
    
    def forward(self, g, s, w):
        g_out = self.g_network(g)
        s_out = self.s_network(s)
        w_out = self.w_network(w)
        w_out = self.w_flatten(w_out)
        x_out = self.x_network(torch.concat([g_out, s_out, w_out], axis = 1))
        return x_out

model = NeuralNetwork().to(device)
# model(g_idx, s_idx, w_idx).shape
```


```{python}
def train_loop_yx(dataloader, model, loss_fn, optimizer, silent = False):
    import torch
    from torch.utils.data import Dataset
    from torch.utils.data import DataLoader
    size = len(dataloader.dataset)
    for batch, (y_i, g_i, s_i, w_i) in enumerate(dataloader):
        # Compute prediction and loss
        pred = model(g_i, s_i, w_i )
        loss = loss_fn(pred, y_i)

        # Backpropagation
#         torch.autograd.set_detect_anomaly(True)
        optimizer.zero_grad()
#         loss.backward(retain_graph=True)
        loss.backward()
        optimizer.step()

        if batch % 100 == 0:
            loss, current = loss.item(), batch * len(y_i)
            if not silent:
                print(f"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]")
```


```{python}
def train_error_yx(dataloader, model, loss_fn, silent = False):
    import torch
    from torch.utils.data import Dataset
    from torch.utils.data import DataLoader
    
    size = len(dataloader.dataset)
    num_batches = len(dataloader)
    train_loss = 0

    with torch.no_grad():
        for y_i, g_i, s_i, w_i in dataloader:
            pred = model(g_i, s_i, w_i)
            train_loss += loss_fn(pred, y_i).item() # <----------------------
            
    train_loss /= num_batches
    return(train_loss)
```

```{python}
def test_loop_yx(dataloader, model, loss_fn, silent = False):
    import torch
    from torch.utils.data import Dataset
    from torch.utils.data import DataLoader
    
    size = len(dataloader.dataset)
    num_batches = len(dataloader)
    test_loss = 0

    with torch.no_grad():
        for y_i, g_i, s_i, w_i in dataloader:
            pred = model(g_i, s_i, w_i)                
            test_loss += loss_fn(pred, y_i).item() # <-----------------------

    test_loss /= num_batches
    if not silent:
        print(f"Test Error: Avg loss: {test_loss:>8f}")
    return(test_loss)
```

```{python}
def train_nn_yx(
    cache_path,
    training_dataloader,
    testing_dataloader,
    model,
    learning_rate = 1e-3,
    batch_size = 64,
    epochs = 500,
    model_prefix = 'model',
    save_pt = False
):
    import numpy as np
    import pandas as pd
    import torch
    from torch import nn
    from tqdm import tqdm
    
    # Initialize the loss function
    loss_fn = nn.MSELoss()

#     optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas= [0.975893, 0.994607])

    loss_df = pd.DataFrame([i for i in range(epochs)], columns = ['Epoch'])
    loss_df['TrainMSE'] = np.nan
    loss_df['TestMSE']  = np.nan

    for t in tqdm(range(epochs)):        
#         print(f"Epoch {t+1}\n-------------------------------")
        train_loop_yx(training_dataloader, model, loss_fn, optimizer, silent = True)

        loss_df.loc[loss_df.index == t, 'TrainMSE'
                   ] = train_error_yx(training_dataloader, model, loss_fn, silent = True)
        
        loss_df.loc[loss_df.index == t, 'TestMSE'
                   ] = test_loop_yx(testing_dataloader, model, loss_fn, silent = True)
        
        if (t+1)%5 == 0: # Cache in case training is interupted. 
            # print(loss_df.loc[loss_df.index == t, ['TrainMSE', 'TestMSE']])
            if save_pt:
                torch.save(model.state_dict(), 
                           cache_path+'/'+model_prefix+'_'+str(t)+'_'+str(epochs)+'.pt') # convention is to use .pt or .pth
        
    return([model, loss_df])
```

```{python}
# changed optimizer, batch size, numEpoch

dataloader_batch_size = 192 #1000
run_epochs = 711
epochs_run = 0
```

```{python}
# model, loss_df = train_nn_yx(
#     cache_path,
#     training_dataloader,
#     training_dataloader, # For demo, the training and testing data are the same.
#     model,
#     learning_rate = 1e-3,
#     batch_size = dataloader_batch_size,
#     epochs = (run_epochs - epochs_run)
# )

# px.line(loss_df, x = 'Epoch', y = 'TrainMSE')
```

```{python}
def yhat_loop_yx(dataloader, model):
    import numpy as np
    import pandas as pd
    import torch
    
    size = len(dataloader.dataset)
    num_batches = len(dataloader)
    
    y_true = np.array([])
    y_pred = np.array([])
    
    with torch.no_grad():
        for y_i, g_i, s_i, w_i in dataloader:
            yhat_i = model(g_i, s_i, w_i)
            y_pred = np.append(y_pred, np.array(yhat_i.cpu()))
            y_true = np.append(y_true, np.array(y_i.cpu()))
    
    out = np.concatenate([y_true[:, None], y_pred[:, None]], axis = 1) 
    out = pd.DataFrame(out, columns = ['y_true', 'y_pred'])
    return(out)
```

```{python}
# dataloader_batch_size = 50
# run_epochs = 100

# don't run if either of these exist because there may be cases where we want the results but not the model
import re

if not os.path.exists(cache_path+'/model.pt'): 
    # Shared setup (train from scratch and load latest)
    model = NeuralNetwork()

    # find the biggest model to save
    saved_models = os.listdir(cache_path)
    saved_models = [e for e in saved_models if re.match('model*', e)]

    if saved_models == []:
        epochs_run = 0
    else:
        saved_models = [e for e in saved_models if e != 'model.pt']
        # if there are saved models reload and resume training
        saved_models_numbers = [int(e.replace('model_', ''
                                    ).replace('.pt', ''
                                    ).split('_')[0]) for e in saved_models]
        # saved_models
        epochs_run = max(saved_models_numbers)+1 # add 1 to account for 0 index
        latest_model = [e for e in saved_models if re.match(
            '^model_'+str(epochs_run-1)+'_.*\.pt$', e)][0] # subtract 1 to convert back
        model.load_state_dict(torch.load(cache_path+latest_model))
        print('Resuming Training: '+str(epochs_run)+'/'+str(run_epochs)+' epochs run.')
    
    model.to(device)    

    model, loss_df = train_nn_yx(
        cache_path,
        training_dataloader,
        testing_dataloader,
        model,
        learning_rate = 1e-3,
        batch_size = dataloader_batch_size,
        epochs = (run_epochs - epochs_run)
    )
    
    # experimental outputs:
    # 1. Model
    torch.save(model.state_dict(), cache_path+'/model.pt') # convention is to use .pt or .pth

    # 2. loss_df    
    # If this is resuming training, load and extend the existing loss dataframe
    if os.path.exists(cache_path+'/loss_df.csv'):
        loss_df_on_disk = pd.read_csv(cache_path+'/loss_df.csv')
        epoch_offset = 1 + loss_df_on_disk['Epoch'].max()
        loss_df['Epoch'] = loss_df['Epoch'] + epoch_offset
        loss_df = pd.concat([loss_df_on_disk, loss_df])
    loss_df.to_csv(cache_path+'/loss_df.csv', index=False)  
    
    # 3. predictions 
    yhats = pd.concat([
        yhat_loop_yx(testing_dataloader, model).assign(Split = 'Test'),
        yhat_loop_yx(training_dataloader, model).assign(Split = 'Train')], axis = 0)

    yhats.to_csv(cache_path+'/yhats.csv', index=False)
```


```{python}
estimate_iterations(sec_per_it = 84)
```

```{python}
# remove_matching_files(
#     cache_path,
#     match_regex_list = ['model\.pt'],
#     dry_run = False
# )
```

### Standard Visualizations

```{python}
scale_dict = {'y1':YMat_cs}
import plotly.graph_objects as go
```

```{python}
naieve_yhat = training_dataloader.dataset.y.mean()

naieve_MSE_Train = reverse_cs( 
    np.array(((naieve_yhat - training_dataloader.dataset.y)**2)).mean(),
    scale_dict['y1']
)

naieve_MSE_Test = reverse_cs( 
    np.array(((naieve_yhat - testing_dataloader.dataset.y)**2)).mean(),
    scale_dict['y1']
)

naieve_MSE_Train, naieve_MSE_Test



loss_df = pd.read_csv(cache_path+'/loss_df.csv')

loss_df.TrainMSE = reverse_cs(loss_df.TrainMSE, scale_dict['y1'])
loss_df.TestMSE  = reverse_cs(loss_df.TestMSE , scale_dict['y1'])


fig = go.Figure()
fig.add_trace(go.Scatter(x=loss_df.Epoch, y=loss_df.TestMSE,
                    mode='lines', name='Test'))
fig.add_trace(go.Scatter(x=loss_df.Epoch, y=loss_df.TrainMSE,
                    mode='lines', name='Train'))

fig.add_trace(go.Scatter(x=loss_df.Epoch, y=[naieve_MSE_Test  for e in range(len(loss_df.Epoch))], 
                         mode='lines', name='Naieve Test'))
fig.add_trace(go.Scatter(x=loss_df.Epoch, y=[naieve_MSE_Train for e in range(len(loss_df.Epoch))], 
                         mode='lines', name='Naieve Train'))
fig.show()
```

```{python}
yhats = pd.read_csv(cache_path+'/yhats.csv')

# px.scatter(yhats, x = 'y_true', y = 'y_pred', color = 'Split')
```

```{python}
yhats.y_true = reverse_cs(yhats.y_true, scale_dict['y1'])
yhats.y_pred = reverse_cs(yhats.y_pred, scale_dict['y1'])

# px.scatter(yhats, x = 'y_true', y = 'y_pred', color = 'Split', trendline="ols")
```

```{python}
yhats['Error'] = yhats.y_pred - yhats.y_true

px.histogram(yhats, x = 'Error', color = 'Split',
             marginal="box", # can be `rug`, `violin`
             nbins= 50)
```

```{python}
# os._exit(00)
```

