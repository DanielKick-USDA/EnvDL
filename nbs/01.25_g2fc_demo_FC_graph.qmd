---
title: Demonstrate building a FC network with arbitrary graph structure
jupyter: python3
---


> This is the foundation for building a network based on gene pathways or similar.


```{python}
import os

import numpy as np
import pandas as pd

import plotly.express as px
import plotly.io as pio
pio.templates.default = "plotly_white"

from tqdm import tqdm

import torch
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
from torch import nn

from graphviz import Digraph
import torchviz

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using {device} device")

# Functions pulled from EnvDL.dlfn ----
def train_error_yx(dataloader, model, loss_fn, silent = False):
    import torch
    from torch.utils.data import Dataset
    from torch.utils.data import DataLoader
    size = len(dataloader.dataset)
    num_batches = len(dataloader)
    train_loss = 0

    with torch.no_grad():
        for y_i, xs_i in dataloader:
            pred = model(xs_i)
            
            # ensure both are on cuda
            if pred.device.type == 'cpu':
                pred = pred.to('cuda')
            if y_i.device.type == 'cpu':
                y_i = y_i.to('cuda')
            
            train_loss += loss_fn(pred, y_i).item()
            
    train_loss /= num_batches
    return(train_loss)

def train_loop_yx(dataloader, model, loss_fn, optimizer, silent = False):
    import torch
    from torch.utils.data import Dataset
    from torch.utils.data import DataLoader
    size = len(dataloader.dataset)
    for batch, (y_i, xs_i) in enumerate(dataloader):
        # Compute prediction and loss
        pred = model(xs_i)
        
        # ensure both are on cuda
        if pred.device.type == 'cpu':
            pred = pred.to('cuda')
        if y_i.device.type == 'cpu':
            y_i = y_i.to('cuda')
        
        loss = loss_fn(pred, y_i)
        
        # Backpropagation
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if batch % 100 == 0:
            loss, current = loss.item(), batch * len(y_i) 
            if not silent:
                print(f"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]")

def test_loop_yx(dataloader, model, loss_fn, silent = False):
    import torch
    from torch.utils.data import Dataset
    from torch.utils.data import DataLoader
    
    size = len(dataloader.dataset)
    num_batches = len(dataloader)
    test_loss = 0

    with torch.no_grad():
        for y_i, xs_i in dataloader:
            pred = model(xs_i)
            
            # ensure both are on cuda
            if pred.device.type == 'cpu':
                pred = pred.to('cuda')
            if y_i.device.type == 'cpu':
                y_i = y_i.to('cuda')
                
            test_loss += loss_fn(pred, y_i).item() 

    test_loss /= num_batches
    if not silent:
        print(f"Test Error: Avg loss: {test_loss:>8f}")
    return(test_loss)

def train_nn_yx(
    training_dataloader,
    testing_dataloader,
    model,
    batch_size = 64,
    epochs = 500,
    model_prefix = 'model',
    save_model = False,
    **kwargs # can include 'silent' for train loop or 'save_on' for saving frequency
):
    import numpy as np
    import pandas as pd
    import torch
    from torch import nn
    from tqdm import tqdm
    
    if 'optimizer' not in kwargs:
        optimizer = torch.optim.SGD(model.parameters(), lr=kwargs['learning_rate'])
    else:
        optimizer = kwargs['optimizer']
        
    if 'save_on' in kwargs:
        save_on = kwargs['save_on']
    else:
        save_on = 5       
    
    # Initialize the loss function
    loss_fn = nn.MSELoss()     

    loss_df = pd.DataFrame([i for i in range(epochs)], columns = ['Epoch'])
    loss_df['TrainMSE'] = np.nan
    loss_df['TestMSE']  = np.nan

    for t in tqdm(range(epochs)):        
        if 'silent' in kwargs:
            train_loop_yx(training_dataloader, model, loss_fn, optimizer, silent = kwargs['silent'])
        else:
            train_loop_yx(training_dataloader, model, loss_fn, optimizer, silent = True)

        loss_df.loc[loss_df.index == t, 'TrainMSE'
                   ] = train_error_yx(training_dataloader, model, loss_fn, silent = True)
        
        loss_df.loc[loss_df.index == t, 'TestMSE'
                   ] = test_loop_yx(testing_dataloader, model, loss_fn, silent = True)
                
    return([model, loss_df])
```

## Example Data


```{python}
n_obs = 100 # 100 obs for each group
y_true = torch.from_numpy(np.concatenate([
        np.zeros((n_obs, )),
        np.ones( (n_obs, ))], 0)) + .1* torch.rand(2*n_obs,)
```

```{python}
input_tensor_dict = {
    'a1_input': torch.from_numpy(np.concatenate([
        np.zeros((n_obs, 4, 3)),
        np.ones( (n_obs, 4, 3))], 0)),
    'a2_input': torch.from_numpy(np.concatenate([
        np.zeros((n_obs, 4, 2)),  
        np.ones( (n_obs, 4, 2))], 0))}

x_list_temp = [input_tensor_dict[key].to(torch.float) for key in input_tensor_dict.keys()]
```

```{python}
class ListDataset(Dataset): 
    def __init__(self, 
                 y, 
                 x_list,
                 transform = None, target_transform = None,
                 **kwargs 
                ):
        self.device = device
        self.y = y 
        self.x_list = x_list
        self.transform = transform
        self.target_transform = target_transform    
        
    def __len__(self):
        return len(self.y)
    
    def __getitem__(self, idx):
        y_idx =self.y[idx]
        x_idx =[x[idx, ] for x in self.x_list] 
        if self.target_transform:
            y_idx = self.transform(y_idx)
            x_idx = [self.transform(x) for x in x_idx]
            
        return y_idx, x_idx
    
training_dataloader = DataLoader(ListDataset(
        y = y_true[:, None].to(torch.float32),
        x_list = [e.to(torch.float32) for e in x_list_temp]
    ),
    batch_size = 20,
    shuffle = True)
```

## Define example graph

```{python}
kegg_connections = {
    'y_hat':['c1', 'c2'],
    'c1':['b1'],
    'c2':['b2'],
    'b1':['a1_input', 'b2'],
    'b2':['a2_input'],
    'a1_input': [],
    'a2_input': []
}
```

```{python}
dot = Digraph()
for key in tqdm(kegg_connections.keys()):
    dot.node(key)
    for value in kegg_connections[key]:
        # edge takes a head/tail whereas edges takes name pairs concatenated (A, B -> AB)in a list
        dot.edge(value, key)    

dot
```

Version with the node names masked for size 

```{python}
node_list = list(kegg_connections.keys())
for e in kegg_connections.keys():
    node_list += kegg_connections[e]
    
node_list = list(set(node_list))
node_list.sort()
node_list
```

```{python}
default_output_size = 20
output_size_dict = dict(zip(node_list, 
                        [default_output_size for i in range(len(node_list))]))
output_size_dict['y_hat'] = 1 
output_size_dict
```

```{python}
input_size_dict = kegg_connections.copy()

no_dependants = [e for e in kegg_connections.keys() if kegg_connections[e] == []]

# use the expected output sizes from `output_size_dict` to fill in the non-data sizes
tensor_ndim = len(input_tensor_dict[list(input_tensor_dict.keys())[0]].shape)
for e in tqdm(input_size_dict.keys()):
    # overwrite named connections with the output size of those connections
    # if the entry is in no_dependants it's data so it's size needs to be grabbed from the input_tensor_dict
    input_size_dict[e] = [
        (list(input_tensor_dict[ee].shape)[1]*list(input_tensor_dict[ee].shape)[2]) # <- NOTE! THIS ASSUMES ONLY DENSE CONNECTIONS (i.e. only the 1st dim is needed)  
        if ee in no_dependants
        else output_size_dict[ee] for ee in input_size_dict[e]]

# Now walk over entries and overwrite with the sum of the inputs
for e in tqdm(input_size_dict.keys()):
    input_size_dict[e] = np.sum(input_size_dict[e])
```

```{python}
input_size_dict
```

```{python}
dot = Digraph()
for key in tqdm(kegg_connections.keys()):
    key_label = str(key)+'\nin: '+str(input_size_dict[key])+'\nout: '+str(output_size_dict[key])
    dot.node(key, key_label)
    for value in kegg_connections[key]:
        # edge takes a head/tail whereas edges takes name pairs concatednated (A, B -> AB)in a list
        dot.edge(value, key)    

dot
```

## Get order to run

```{python}
# start by finding the top level -- all those keys which are theselves not values
# helper function to get all keys and all value from a dict. Useful for when keys don't have unique values.
def find_uniq_keys_values(input_dict):
    all_keys = list(input_dict.keys())
    all_values = []
    for e in all_keys:
        all_values.extend(input_dict[e])
    all_values = list(set(all_values))

    return({'all_keys': all_keys,
           'all_values': all_values})

# find the dependancies for run order from many dependancies to none
# wrapper function to find the nodes that aren't any other nodes dependancies.
def find_top_nodes(all_key_value_dict):
    return([e for e in all_key_value_dict['all_keys'] if e not in all_key_value_dict['all_values']])

```

```{python}
# find the dependancies for run order from many dependancies to none
temp = kegg_connections.copy()

dependancy_order = []
# Then iterate
for ith in range(100): #TODO <- this should be set as a input parameter
    top_nodes = find_top_nodes(all_key_value_dict = find_uniq_keys_values(input_dict = temp))
    if top_nodes == []:
        break
    else:
        dependancy_order += top_nodes    
        # remove nodes from the graph that are at the 'top' level and haven't already been removed
        for key in [e for e in dependancy_order if e in temp.keys()]:
             temp.pop(key)

                
# reverse to get the order that the nodes should be called
dependancy_order.reverse()                
dependancy_order
```

## Train and Examine

```{python}
# Working version ====
class NeuralNetwork(nn.Module):
    def __init__(self, 
                 example_dict, # contains the node (excluding input tensors)
                 example_dict_input_size, # contains the input sizes (including the tensors)
                 example_dict_output_size,
                 input_tensor_names,
                 dependancy_order
                ):
        super(NeuralNetwork, self).__init__()
        def Linear_block(in_size, out_size, drop_pr):
            block = nn.Sequential(
                nn.Linear(in_size, out_size),
                nn.ReLU(),
                nn.Dropout(drop_pr))
            return(block)   
        
        # fill in the list in dependancy order. 
        layer_list = []
        for key in dependancy_order:
            if key in input_tensor_names:
                layer_list += [
                    nn.Flatten()
                ]
            elif key != 'y_hat':
                layer_list += [
                    Linear_block(in_size=example_dict_input_size[key], 
                                 out_size=example_dict_output_size[key], 
                                 drop_pr=0)
                              ]
            else:
                layer_list += [
                    nn.Linear(example_dict_input_size[key], 
                              example_dict_output_size[key])
                              ]
                
        self.nn_layer_list = nn.ModuleList(layer_list)

        # things for get_input_node in forward to work.
        self.example_dict = example_dict
        self.input_tensor_names = input_tensor_names
        self.dependancy_order = dependancy_order
        
        self.input_tensor_lookup = dict(zip(input_tensor_names, 
                                            [i for i in range(len(input_tensor_names))]))
        self.result_list = []
        self.result_list_lookup = {}
            
    def forward(self, x):
        # Note: x will be a list. input_tensor_lookup will contain the name: list index pairs.
        # I use a dict instead of a list comprehension here because there could be an arbitrarily
        # large number of inputs in the list. 
        def get_input_node(self, input_node, get_x):  
            return(self.result_list[self.result_list_lookup[input_node]])
        
        # trying reinstantiating to get around inplace replacement issue.
        self.result_list = []
        self.result_list_lookup = {}
        for key in self.dependancy_order:
            input_nodes = self.example_dict[key]
            nn_layer_list_idx = [i for i in range(len(dependancy_order)) if dependancy_order[i]==key][0]

            # I realllllly hope these are being copied by reference...
            # must be first before growing result_list
            self.result_list_lookup[key] = len(self.result_list_lookup)                
            if key in self.input_tensor_names: # If the input node is an input (flatten) layer
                self.result_list = self.result_list + [self.nn_layer_list[nn_layer_list_idx](
                    x[self.input_tensor_lookup[key]]
                ).clone()]

            else:
                self.result_list = self.result_list + [self.nn_layer_list[nn_layer_list_idx](torch.concat(
                    [get_input_node(self, input_node = e, get_x = x) for e in input_nodes], 
                    -1)).clone()]

        return self.result_list[self.result_list_lookup['y_hat']]
```


```{python}
model = NeuralNetwork(example_dict = kegg_connections, 
                      example_dict_input_size = input_size_dict,
                      example_dict_output_size = output_size_dict,
                      input_tensor_names = list(input_tensor_dict.keys()),
                      dependancy_order = dependancy_order) 


model, loss_df = train_nn_yx(
    training_dataloader,
    training_dataloader, # For demo, the training and testing data are the same.
    model,
    learning_rate = 1e-3,
    batch_size = 100,
    epochs = 200
)

px.line(loss_df, x = 'Epoch', y = 'TrainMSE')
```

```{python}
yhat = model(next(iter(training_dataloader))[1])
torchviz.make_dot(yhat.mean(), params=dict(model.named_parameters()))
```

# Repeat as a Learner Class

```{python}
# https://github.com/glassroom/torch_lsuv_init
def LSUV_(model, data, apply_only_to=['Conv', 'Linear', 'Bilinear'],
          std_tol=0.1, max_iters=10, do_ortho_init=True, logging_FN=print):
    r"""
    Applies layer sequential unit variance (LSUV), as described in
    `All you need is a good init` - Mishkin, D. et al (2015):
    https://arxiv.org/abs/1511.06422

    Args:
        model: `torch.nn.Module` object on which to apply LSUV.
        data: sample input data drawn from training dataset.
        apply_only_to: list of strings indicating target children
            modules. For example, ['Conv'] results in LSUV applied
            to children of type containing the substring 'Conv'.
        std_tol: positive number < 1.0, below which differences between
            actual and unit standard deviation are acceptable.
        max_iters: number of times to try scaling standard deviation
            of each children module's output activations.
        do_ortho_init: boolean indicating whether to apply orthogonal
            init to parameters of dim >= 2 (zero init if dim < 2).
        logging_FN: function for outputting progress information.

    Example:
        >>> model = nn.Sequential(nn.Linear(8, 2), nn.Softmax(dim=1))                                                                                                                                                                                                                                            
        >>> data = torch.randn(100, 8)
        >>> LSUV_(model, data)
    """

    matched_modules = [m for m in model.modules() if any(substr in str(type(m)) for substr in apply_only_to)]

    if do_ortho_init:
        logging_FN(f"Applying orthogonal init (zero init if dim < 2) to params in {len(matched_modules)} module(s).")
        for m in matched_modules:
            for p in m.parameters():                
                torch.nn.init.orthogonal_(p) if (p.dim() >= 2) else torch.nn.init.zeros_(p)

    logging_FN(f"Applying LSUV to {len(matched_modules)} module(s) (up to {max_iters} iters per module):")

    def _compute_and_store_LSUV_stats(m, inp, out):
        m._LSUV_stats = { 'mean': out.detach().mean(), 'std': out.detach().std() }

    was_training = model.training
    model.train()  # sets all modules to training behavior
    with torch.no_grad():
        for i, m in enumerate(matched_modules):
            with m.register_forward_hook(_compute_and_store_LSUV_stats):
                for t in range(max_iters):
                    _ = model(data)  # run data through model to get stats
                    mean, std = m._LSUV_stats['mean'], m._LSUV_stats['std']
                    if abs(std - 1.0) < std_tol:
                        break
                    m.weight.data /= (std + 1e-6)
            logging_FN(f"Module {i:2} after {(t+1):2} itr(s) | Mean:{mean:7.3f} | Std:{std:6.3f} | {type(m)}")
            delattr(m, '_LSUV_stats')

    if not was_training: model.eval()
```


```{python}
model = NeuralNetwork(example_dict = kegg_connections, 
                      example_dict_input_size = input_size_dict,
                      example_dict_output_size = output_size_dict,
                      input_tensor_names = list(input_tensor_dict.keys()),
                      dependancy_order = dependancy_order) 


LSUV_(model, data = next(iter(training_dataloader))[1])

model, loss_df = train_nn_yx(
    training_dataloader,
    training_dataloader, # For demo, the training and testing data are the same.
    model,
    learning_rate = 1e-3,
    batch_size = 100,
    epochs = 200
)

px.line(loss_df, x = 'Epoch', y = 'TrainMSE')
```

## Can I get summary stats for the weights over training?

```{python}
model = NeuralNetwork(example_dict = kegg_connections, 
                      example_dict_input_size = input_size_dict,
                      example_dict_output_size = output_size_dict,
                      input_tensor_names = list(input_tensor_dict.keys()),
                      dependancy_order = dependancy_order) 


LSUV_(model, data = next(iter(training_dataloader))[1])

training_dataloader
testing_dataloader= training_dataloader
learning_rate = 1e-3
batch_size = 100
epochs = 200
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)
#train_nn_yx
save_on = 5       

# Initialize the loss function
loss_fn = nn.MSELoss()     

loss_df = pd.DataFrame([i for i in range(epochs)], columns = ['Epoch'])
loss_df['TrainMSE'] = np.nan
loss_df['TestMSE']  = np.nan


training_stat_dict = {}

for t in tqdm(range(epochs)):        

    train_loop_yx(training_dataloader, model, loss_fn, optimizer, silent = True)

    loss_df.loc[loss_df.index == t, 'TrainMSE'
               ] = train_error_yx(training_dataloader, model, loss_fn, silent = True)

    loss_df.loc[loss_df.index == t, 'TestMSE'
               ] = test_loop_yx(testing_dataloader, model, loss_fn, silent = True)
    
    weight_bias_list=[(name, param) for name, param in model.named_parameters()]; 
    for e in weight_bias_list: 
        if e[0].split('.')[-1] == 'weight':
            with torch.no_grad():
                if e[0] not in training_stat_dict.keys():
                    training_stat_dict[e[0]] = {'mean':[e[1].mean()],
                                                'std' :[e[1].std()] }
                else:
                    training_stat_dict[e[0]]['mean'].extend([e[1].mean()])
                    training_stat_dict[e[0]]['std'].extend([e[1].std()])


# training_stat_dict


# model, loss_df
```

```{python}
for key in training_stat_dict.keys():
    training_stat_dict[key]['mean'] = torch.concat([e[None, None] for e in training_stat_dict[key]['mean']])
    training_stat_dict[key]['std'] = torch.concat([e[None, None] for e in training_stat_dict[key]['std']])
    
```

```{python}
from plotly.subplots import make_subplots
import plotly.graph_objects as go

fig = make_subplots(
    rows=1, cols=2,
    subplot_titles=("Layer Means", "Layer Stds"))

for key in training_stat_dict.keys():
    fig.add_trace(
        go.Scatter(
            x = [x for x in range(epochs)], 
            y = torch.Tensor.numpy(training_stat_dict[key]['mean'])[:, 0],
            mode="lines",
            # line=go.scatter.Line(color="gray"),
            name  = key.replace('nn_layer_list.', ''),
            showlegend=False),
        row=1, col=1
    )
    
    fig.add_trace(
        go.Scatter(
            x = [x for x in range(epochs)], 
            y = torch.Tensor.numpy(training_stat_dict[key]['std'])[:, 0],
            mode="lines",
            # line=go.scatter.Line(color="gray"),
            name  = key.replace('nn_layer_list.', ''),
            showlegend=False),
        row=1, col=2
    )
    
fig.update_layout(#height=500, width=700,
    title_text="Layerwise Weight Statistics over Training")
fig.show()
```

## Using slices of a tensor instead of a list of tensors

```{python}
# length of all the genes -> slice positions
# print([e.shape[-1] for e in x_list_temp])

slice_pos = np.cumsum([e.shape[-1] for e in x_list_temp])
# set up an array with the start/end of each gene slice in each row
slice_idx = np.zeros((slice_pos.shape[0], 2))
slice_idx[1:, 0] = slice_pos[:-1]
slice_idx[ :, 1] = slice_pos

slice_idx = slice_idx.astype(int)
print(slice_idx)

x_list_temp_cat = torch.concat(x_list_temp, axis = 2)

# here's how to slice one of these concatenated tensors to get the desired part. 
x_list_temp_cat[:, :, slice_idx[0, 0]:slice_idx[0, 1]].shape
```

```{python}
# Don't even have to subclass the dataloader, just drop the single tensor into a list.
training_dataloader = DataLoader(ListDataset(
        y = y_true[:, None].to(torch.float32),
        x_list = [e.to(torch.float32) for e in [x_list_temp_cat]]
    ),
    batch_size = 20,
    shuffle = True)
```


```{python}
# Working version ====
class NeuralNetworkV2(nn.Module):
    def __init__(self, 
                 example_dict, # contains the node (excluding input tensors)
                 example_dict_input_size, # contains the input sizes (including the tensors)
                 example_dict_output_size,
                 input_tensor_names,
                 input_tensor_slice_idxs,
                 dependancy_order
                ):
        super(NeuralNetworkV2, self).__init__()
        def Linear_block(in_size, out_size, drop_pr):
            block = nn.Sequential(
                nn.Linear(in_size, out_size),
                nn.ReLU(),
                nn.Dropout(drop_pr))
            return(block)   
        
        # fill in the list in dependancy order. 
        layer_list = []
        for key in dependancy_order:
            if key in input_tensor_names:
                layer_list += [
                    nn.Flatten()
                ]
            elif key != 'y_hat':
                layer_list += [
                    Linear_block(in_size=example_dict_input_size[key], 
                                 out_size=example_dict_output_size[key], 
                                 drop_pr=0)
                              ]
            else:
                layer_list += [
                    nn.Linear(example_dict_input_size[key], 
                              example_dict_output_size[key])
                              ]
                
        self.nn_layer_list = nn.ModuleList(layer_list)

        # things for get_input_node in forward to work.
        self.example_dict = example_dict
        self.input_tensor_names = input_tensor_names
        self.dependancy_order = dependancy_order
        
        self.input_tensor_lookup = dict(zip(input_tensor_names, 
                                            [i for i in range(len(input_tensor_names))]))
        
        # I can use the lookup in the original class to get the right slice indexes!
        self.input_tensor_slice_idxs = input_tensor_slice_idxs


        self.result_list = []
        self.result_list_lookup = {}
            
    def forward(self, x):
        # Note: x will be a list. input_tensor_lookup will contain the name: list index pairs.
        # I use a dict instead of a list comprehension here because there could be an arbitrarily
        # large number of inputs in the list. 
        def get_input_node(self, input_node, get_x):  
            return(self.result_list[self.result_list_lookup[input_node]])
        
        # trying reinstantiating to get around inplace replacement issue.
        self.result_list = []
        self.result_list_lookup = {}
        for key in self.dependancy_order:
            input_nodes = self.example_dict[key]
            nn_layer_list_idx = [i for i in range(len(dependancy_order)) if dependancy_order[i]==key][0]

            # I realllllly hope these are being copied by reference...
            # must be first before growing result_list
            self.result_list_lookup[key] = len(self.result_list_lookup)                
            if key in self.input_tensor_names: # If the input node is an input (flatten) layer
                gene_idx = self.input_tensor_lookup[key]
                
                self.result_list = self.result_list + [self.nn_layer_list[nn_layer_list_idx](
                    # instead of looking up a list index now x is sliced
                    x[0][:, :, self.input_tensor_slice_idxs[gene_idx, 0]:self.input_tensor_slice_idxs[gene_idx, 1]]
                ).clone()] # clone could be the source of my problems too.

            else:
                self.result_list = self.result_list + [self.nn_layer_list[nn_layer_list_idx](torch.concat(
                    [get_input_node(self, input_node = e, get_x = x) for e in input_nodes], 
                    -1)).clone()]

        return self.result_list[self.result_list_lookup['y_hat']]
```


```{python}
model = NeuralNetworkV2(example_dict = kegg_connections, 
                      example_dict_input_size = input_size_dict,
                      example_dict_output_size = output_size_dict,
                      input_tensor_names = list(input_tensor_dict.keys()),
                      input_tensor_slice_idxs= slice_idx,
                      dependancy_order = dependancy_order) 
```

```{python}
model, loss_df = train_nn_yx(
    training_dataloader,
    training_dataloader, # For demo, the training and testing data are the same.
    model,
    learning_rate = 1e-3,
    batch_size = 100,
    epochs = 200
)

px.line(loss_df, x = 'Epoch', y = 'TrainMSE')
# no apparant time savings here.
```


