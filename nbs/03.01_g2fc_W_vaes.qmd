---
title: Create Embeddings for Weather data
jupyter: python3
---


> Based on 99.99 03.24


```{python}
# Hacky way to schedule. Here I'm setting these to sleep until the gpus should be free.
# At the end of the notebooks  os._exit(00) will kill the kernel freeing the gpu. 
#                          Hours to wait
# import time; time.sleep( 6 * (60*60))
```

```{python}
import os, json, re

import numpy as np
import pandas as pd

import plotly.express as px
import plotly.io as pio
pio.templates.default = "plotly_white"
from sklearn.manifold import TSNE # for visualizing embeddings

import hilbertcurve
from hilbertcurve.hilbertcurve import HilbertCurve

from EnvDL.core import * # includes remove_matching_files
from EnvDL.dna  import *
from EnvDL.dlfn import * # includes LSUV_

from tqdm import tqdm
```

```{python}
use_gpu_num = 0

# Imports --------------------------------------------------------------------
import torch
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
from torch import nn
import torch.nn.functional as F # F.mse_loss

import einops # for einops.rearrange

import lightning.pytorch as pl
from lightning.pytorch.loggers import TensorBoardLogger

device = "cuda" if torch.cuda.is_available() else "cpu"
if use_gpu_num in [0, 1]: 
    torch.cuda.set_device(use_gpu_num)
print(f"Using {device} device")
```

```{python}
# # Imports for the Adaptive Experimentation (Ax) platform
# from ax.service.ax_client import AxClient, ObjectiveProperties
# from ax.utils.measurement.synthetic_functions import hartmann6
# from ax.utils.notebook.plotting import render, init_notebook_plotting

# import pickle as pkl
# # Because the search space is nested, it's not storable with the builtin methods for saving/reading json (not tested with sql)
# # pickling seems to work just fine.

# init_notebook_plotting()
```

```{python}
cache_path = '../nbs_artifacts/03.01_g2fc_W_vaes/'
ensure_dir_path_exists(dir_path = cache_path)
```

## Load data

### Genomic data

```{python}
# load_from = '../nbs_artifacts/01.05_g2fc_demo_model/'
# parsed_kegg_gene_entries = get_cached_result(load_from+'filtered_kegg_gene_entries.pkl')
# ACGT_gene_slice_list = get_cached_result(load_from+'ACGT_gene_slice_list.pkl')
```

```{python}
# load_from = '../nbs_artifacts/01.03_g2fc_prep_matrices/'
# ACGT = np.load(load_from+'ACGT.npy')
# ACGT_hilb = np.load(load_from+'ACGT_hilb.npy')
```

```{python}
# # Optional -- Since I used percents, ACGT can be represented as ACG without loss of information
# ACGT = ACGT[:, 0:3, :]
# ACGT_hilb = ACGT_hilb[:, 0:3, :, :]
# ACGT_gene_slice_list = [e[:, 0:3, :] for e in ACGT_gene_slice_list]
```


### Soil and Management ("Static" in season)

```{python}
# load_from = '../nbs_artifacts/01.03_g2fc_prep_matrices/'

# # #                            mgmtMatNames.npy
# # mgmtMat = np.load(load_from+'mgmtMat.npy')
# SMat = np.load(load_from+'SMat.npy')
```

### Weather (Variable in season)

```{python}
# load_from = '../nbs_artifacts/01.03_g2fc_prep_matrices/'

# np.load(load_from+'PlantHarvestNames.npy')
# PlantHarvest = np.load(load_from+'PlantHarvest.npy')

# WMat = np.load(load_from+'WMat.npy')
# WMatNames = np.load(load_from+'WMatNames.npy')

# WMat_hilb = np.load(load_from+'WMat_hilb.npy')
```

### Response and lookup

```{python}
# load_from = '../nbs_artifacts/01.03_g2fc_prep_matrices/'
# phno_geno = pd.read_csv(load_from+'phno_geno.csv')
# phno = phno_geno

# obs_geno_lookup = np.load(load_from+'obs_geno_lookup.npy') # Phno_Idx  Geno_Idx  Is_Phno_Idx
# obs_env_lookup = np.load(load_from+'obs_env_lookup.npy')   # Phno_Idx  Env_Idx   Is_Phno_Idx
# YMat = np.load(load_from+'YMat.npy')
```

## Demo retrieval

```{python}
# # Indexes
# idx = 0

# geno_idx = obs_geno_lookup[idx, 1]
# env_idx = obs_env_lookup[idx, 1]
```

```{python}
# # Response
# YMat[idx]
```

```{python}
# # Weather (plant in ground) ------------------------------------------------------------------------
# # generate planting information
# ## Basic tensor
# WPlant = np.zeros(365)
# WPlant[PlantHarvest[idx, 0]:PlantHarvest[idx, 1]] = 1

# ## Hilbert
# WPlant_hilb = np_3d_to_hilbert(WPlant[None, :, None])
# WPlant_hilb = WPlant_hilb.squeeze(axis = 3)
# WPlant_hilb[np.isnan(WPlant_hilb)] = 0
# # px.imshow(WPlant_hilb.squeeze())
# # compare with
# # px.imshow(WMat_hilb[env_idx][8, :, :])
```

```{python}
# # Weather (all) ------------------------------------------------------------------------------------
# ## Basic tensor
# np.concatenate([WMat[env_idx], WPlant[None, :]], axis = 0).shape
# ## Hilbert
# np.concatenate([WMat_hilb[env_idx], WPlant_hilb], axis = 0).shape
```


```{python}
# # Soil
# SMat[env_idx]
```

```{python}
# # Genome -------------------------------------------------------------------------------------------
# ACGT[geno_idx].shape

# ACGT_hilb[geno_idx].shape
```

```{python}
# # if separate tensors are needed
# [e[geno_idx] for e in ACGT_gene_slice_list ][0:2]
```

```{python}
# # if one tensor is needed
# np.concatenate([e[geno_idx] for e in ACGT_gene_slice_list ], axis = 1)
```


### Set up Dataset







# Setup Autoencoder Networks

### Reference Weather Info
based on 01.30_g2fc_W_only, 99.99

```{python}
# Process Grid of Collected POWER data (with imputation for missings). 
# After inductive tinkering this is the best lightgbm 
# Depth     |   32 | 
# num_leaves|16384 | 
# num_round |  200 | 
# rmse      | .117 |
# run time  | 4m33 |

load_path = '../nbs_artifacts/01.04_g2fc_gps_grid_nasa_power/power_data/'

if os.path.exists(cache_path+'HistWthr.npy'):
    usa_grid = np.load(cache_path+'HistWthr.npy', allow_pickle=True)
    usa_grid_names = np.load(cache_path+'HistWthrNames.npy')
else:
    cached_files = [e for e in os.listdir(load_path) if re.match('.+\.pkl', e)]
    usa_grid = pd.concat([pd.read_pickle(load_path+e) for e in cached_files])
    
    temp = usa_grid.loc[:, ['Date']]
    temp = temp.drop_duplicates()
    # sub-optimal way to do this but working on the column is not behaving as expected
    temp.loc[:, 'DOY'] = [pd.Period(e).day_of_year for e in list(temp['Date'])]
    temp['Year'] = temp.Date.str[0:4] 

    usa_grid = usa_grid.merge(temp.reset_index(drop = True))

    # fix missing values (coded as -999 by NASA)

    # trim to 365 days
    mask = (usa_grid.DOY <= 365)
    usa_grid = usa_grid.loc[mask, ].copy()

    ## Impute missing values ---------------------------------------------------------------------------
    M = usa_grid.copy()
    # overwrite -999s with na
    cols_with_missing = [
        'GWETTOP',
        'ALLSKY_SFC_SW_DWN',
        'ALLSKY_SFC_PAR_TOT',
        'GWETROOT',
        'GWETPROF',
        'ALLSKY_SFC_SW_DNI']

    for e in cols_with_missing:
        mask = (M[e] == -999)
        M.loc[mask, [e]] = np.nan
        
    imputation_order = [
        'ALLSKY_SFC_SW_DNI',
        'GWETTOP',
        'ALLSKY_SFC_SW_DWN',
        'ALLSKY_SFC_PAR_TOT',
        'GWETROOT',
        'GWETPROF']

    import lightgbm as lgb

    temp = M.copy()
    temp = temp.drop(columns = ['Date', 'Year'])#.dropna()
    temp_names = list(temp)

    csd = {}
    for e in temp_names:
        csd[e]= [np.nanmean(temp[e]), np.nanstd(temp[e])]

    for e in temp_names:
        temp[e] = ((temp[e] - csd[e][0])/ csd[e][1])

    res_dict = {}

    for impute in tqdm(imputation_order):
        # only look at the columns that are complete
        y = np.array(temp[impute])
        x = np.array(temp.drop(columns=imputation_order))

        train_data = lgb.Dataset(
            np.array(x),
            label=np.array(y)
        )

        param = {
            'max_depth': 32,
            'num_leaves': 16384, # should be less than or equal to 2^max_depth
            'objective': 'mse'}
        param['metric'] = 'mse'
        num_round = 200
        bst = lgb.train(
            param, 
            train_data, num_round, 
            valid_sets=[train_data]
            )

        ypred = bst.predict(np.array(x))
        mask = ~np.isnan(y)
        y[~mask] = ypred[~mask]
        res_dict[impute] = y

    for imputation in imputation_order:
        M.loc[:, imputation] = (res_dict[imputation]*csd[imputation][1])+csd[imputation][0]

    M = M.drop(columns=['Date'])

    np.save(cache_path+'HistWthrNames.npy', list(M))
    np.save(cache_path+'HistWthr.npy',  np.array(M))
```

```{python}
# usa_grid_names
usa_grid = usa_grid.astype(np.float32)
```

#### Add in hypothetical planting/harvesting dates & reformat

```{python}
# Finding values for sampling function
if False:
    px.scatter(
        pd.DataFrame(PlantHarvest, columns = ['Plant', 'Harvest']).drop_duplicates(), 
        x = 'Plant', y = 'Harvest')

    from sklearn.linear_vae import LinearRegression

    X, y = PlantHarvest[:, 0][:, None], PlantHarvest[:, 1]
    fm = LinearRegression().fit(X, y)
    sigma = (fm.predict(X) - y).std()
    fm.intercept_, fm.coef_, sigma

    # sigma is not fixed, but that's okay. I'll just end up over sampling less plausable combinations
    px.scatter(x = PlantHarvest[:, 0], 
              y = (fm.predict(X) - y))
```

```{python}
# Propose plausable plant harvest
def propose_plantharvest(
    first_doy = 60,
    last_doy = 170,
    range_pr = 1.1,
    rng = np.random.default_rng()):
    range_doy = range_pr*(last_doy - first_doy)
    # update with the expanded range
    first_doy = round(((last_doy + first_doy)/2)-(range_doy/2))
    last_doy  = round(((last_doy + first_doy)/2)+(range_doy/2))
    first_doy = first_doy if first_doy >=   0 else   0
    last_doy  = last_doy  if last_doy  <= 365 else 365

    def check_sim(X, y_hat):
        res = (
            (X >= 0)&
            (X <= 365)&
            (y_hat >= 0)&
            (y_hat <= 365)&
            ((y_hat - X) > 0)&
            ((y_hat - X) <= 365))
        return res

    def sim_harvest(first_doy, last_doy):
        # coefs from lm fit on plant/harvest date
        X = rng.integers(last_doy-first_doy)+first_doy
        y_hat = 1.19149787*(X) + 136.23721389467627
        y_hat = y_hat + 16.664816855810002*rng.standard_normal(1)
        y_hat = round(y_hat[0])
        return X, y_hat

    X, y_hat = 0, 0
    while not check_sim(X= X, y_hat= y_hat):
        X, y_hat = sim_harvest(first_doy, last_doy)

    return(X, y_hat)
```

```{python}
# ith_plantharvest = propose_plantharvest(
#     first_doy = 60,
#     last_doy = 170,
#     range_pr = 1.1,
#     rng = np.random.default_rng())
# ith_plantharvest = list(ith_plantharvest)
# ith_plantharvest
```

```{python}
def format_usa_grid_obs(
    usa_grid = usa_grid, 
    ith_lat  = 43.8968753814697, 
    ith_lon  = -106.692895889282, 
    ith_year = 1981,
    ith_plantharvest = [169, 327],
    **kwargs # W_type = 'hilbert'
):
    if 'W_type' in list(kwargs.keys()):
        W_type = kwargs['W_type']
    else:
        W_type = 'raw'  
    # get observation
    mask = ((usa_grid[:, 0] == ith_lat) &
            (usa_grid[:, 1] == ith_lon) &
            (usa_grid[:, 19] == ith_year))
    WTemp = usa_grid[mask, 2:18]

    # switch to match WMat's shape
    WTemp = WTemp.swapaxes(0,1)
    WTemp = WTemp[None, :, :]

    # spike in a simulated planting and harvesting time
    ## Basic tensor
    WPlant = np.zeros(365)
    WPlant[ith_plantharvest[0]:ith_plantharvest[1]] = 1
    WTemp = np.concatenate([WTemp, WPlant[None, None, :]], axis = 1)

    ## Hilbert
    if W_type == 'hilbert':
        WTemp = np_3d_to_hilbert(np.swapaxes(WTemp, 1, 2))
        WTemp = WTemp.swapaxes(1, 3)
        WTemp = WTemp.swapaxes(2, 3)
        WTemp[np.isnan(WTemp)] = 0

    return(WTemp)
```


```{python}
format_usa_grid_obs(
    usa_grid = usa_grid, 
    ith_lat = 43.8968753814697, 
    ith_lon = -106.692895889282, 
    ith_year = 1981,
    ith_plantharvest = [169, 327],
    # kwargs
    W_type = 'raw'
).shape

# format_usa_grid_obs(
#     usa_grid = usa_grid, 
#     ith_lat = 43.8968753814697, 
#     ith_lon = -106.692895889282, 
#     ith_year = 1981,
#     ith_plantharvest = [169, 327],
#     # kwargs
#     W_type = 'hilbert'
# ).shape
```

```{python}
def propose_usa_grid_obs(
    idx, # = 0,
    obs_lookup, # = train_obs_uids,
    ith_plantharvest, # = list(propose_plantharvest(first_doy = 60, last_doy = 170, range_pr = 1.1,
                      #                             rng = np.random.default_rng())),
    W_type #= 'raw'
):
    mask = (obs_lookup.index == idx)
    ith_lat  = float(train_obs_uids.loc[mask, 'Latitude'])
    ith_lon  = float(train_obs_uids.loc[mask, 'Longitude'])
    ith_year = float(train_obs_uids.loc[mask, 'Year'])

    out = format_usa_grid_obs(
        usa_grid = usa_grid, 
        ith_lat = ith_lat, 
        ith_lon = ith_lon, 
        ith_year = ith_year,
        ith_plantharvest = ith_plantharvest,
        # kwargs
        W_type = W_type
    )    
    return(out)
```


With `propose_usa_grid_obs()` gets most of it's data from `usa_grid` but also takes hypothetical planting/harvest dates to generate realisitic data.

```{python}
train_obs_uids = pd.DataFrame(
    np.concatenate([
        usa_grid[0:1, 0:2], 
        usa_grid[0:1, 19][:, None]
    ], axis = 1), 
    columns=['Latitude', 'Longitude', 'Year'])

res = propose_usa_grid_obs(
    idx = 0,
    obs_lookup = train_obs_uids,
    ith_plantharvest = list(propose_plantharvest(first_doy = 60, last_doy = 170, range_pr = 1.1,
                                                 rng = np.random.default_rng())),
    W_type = 'raw'
)
px.imshow(res.squeeze())
```

```{python}
res = propose_usa_grid_obs(
    idx = 0,
    obs_lookup = train_obs_uids,
    ith_plantharvest = list(propose_plantharvest(first_doy = 60, last_doy = 170, range_pr = 1.1,
                                                 rng = np.random.default_rng())),
    W_type = 'hilbert'
)
print('showing first and last channel')
px.imshow(np.concatenate([
    res.squeeze()[0, :, :],
    res.squeeze()[-1, :, :]
]
))
```

# Setup Training Data

```{python}
def _setup_usa_grid_tensor():
    tmp = pd.DataFrame(usa_grid, columns= usa_grid_names).sort_values(['Latitude', 'Longitude', 'Year', 'DOY'])
    tmp = tmp.reset_index(drop=True)
    # create lookup 
    tmp_lookup = tmp.loc[:, ['Longitude', 'Latitude', 'Year']].copy()
    tmp_lookup = tmp_lookup.drop_duplicates().reset_index(drop=True)
    tmp_lookup = torch.from_numpy(np.array(tmp_lookup))

    # Reshape
    tmp = np.array(tmp)
    # data is set up as 

    # Lat/lon | year | doy
    #         |      | doy
    #         | year 

    # so to get the right reshaping 
    # the key to imagining this is to visualize each axis in turn.
    # The matrix is 'cut' once per lat lon pair. 
    # These are 'stacked' and then cut once per year and set in piles
    # the remaining values are the channels
    tmp2 = tmp.reshape(tmp.shape[0]//365, 365, -1)
    # trim down to the right channels
    tmp2 = tmp2[:, :, 2:18]
    # and swap for pytorch
    tmp2 = tmp2.swapaxes(1,2)
    return([tmp_lookup, tmp2])


usaMatMetadata, usaMat = _setup_usa_grid_tensor()
```

```{python}
# px.scatter(x = usaMatMetadata[:, 0], y = usaMatMetadata[:, 1])
```

```{python}
# subclass to make up a reasonable p (plant harvest) on the fly
class SimBigDataset(BigDataset):

    def get_W(self, idx):
        W_idx = self.W[idx, :, :]

        # simulate and add in planting/harvest dates
        ith_plantharvest = list(propose_plantharvest(
            first_doy = 60, 
            last_doy = 170, 
            range_pr = 1.1, rng = np.random.default_rng()))

        WPlant = torch.zeros(365)[None, :]
        WPlant[:, ith_plantharvest[0]:ith_plantharvest[1]] = 1       
        
        # send to cuda if needed.
        W_idx_device = W_idx.get_device()
        if W_idx_device > -1: WPlant = WPlant.to(W_idx_device)

        # print([e.shape for e in [W_idx, WPlant]])
        W_idx = torch.concatenate([W_idx, WPlant], axis = 0)

        if self.W_type == 'raw':
            pass
        elif self.W_type == 'hilbert':
            # print(W_idx.shape)
            W_idx = W_idx.swapaxes(0, 1)
            # print(W_idx.shape)
            W_idx = torch_2d_to_hilbert(in_seq=W_idx)
            # fix axes             
            W_idx = W_idx.swapaxes(0,1).swapaxes(0, 2)
            # fill missing values
            W_idx[torch.isnan(W_idx)] = 0

            if W_idx_device > -1: W_idx = W_idx.to(W_idx_device)


        if self.transform:
            W_idx = self.transform(W_idx)
        return(W_idx)
```

```{python}
# training_dataloader = DataLoader(
#     SimBigDataset(
#         lookup_obs= torch.linspace(0, 99, 100)[:, None], 
#         # lookup_env = torch.concat(
#         #     [torch.linspace(0, 99, 100)[:, None],
#         #      torch.linspace(0, 99, 100)[:, None]], axis = 1
#         #      ), 

#         W = torch.from_numpy(usaMat), 
#         # W_type = 'raw'),
#         W_type = 'hilbert'),
#     batch_size = 50,
#     shuffle = True)

# xs = next(iter(training_dataloader))[0]
# xs.shape
# # xs[0, -1, :] - xs[1, -1, :]
```

```{python}
sites_drawn = 1000

obs_per_env = 42
obs_total = usaMat.shape[0]

drawn = np.random.choice(int(obs_total/obs_per_env), sites_drawn)*obs_per_env
drawn.sort()

mask = [False for i in range(obs_total)]
j = 0
for i in range(obs_total):
    if i < drawn[j]:
        # print(i, False)         
        pass

    elif (i >= drawn[j]) & (i < drawn[j]+obs_per_env):
        # print(i, True)
        mask[i] = True

    if i+1 == drawn[j]+obs_per_env:
        j += 1  
```

```{python}
# n_sites = 1000
# mask = [True if i < (42*n_sites) else False for i in range(usaMat.shape[0])]
mask = usaMatMetadata[:, 2] < 2013

training_metadata = usaMatMetadata[mask, ]

tmp = usaMat[mask, :, :]
cs_dict = {'histW':calc_cs(tmp)}
tmp = apply_cs(tmp, cs_dict['histW'])


exporatory_dataloader = DataLoader(
    SimBigDataset(
        lookup_obs= torch.linspace(0, tmp.shape[0], tmp.shape[0])[:, None].to('cuda'), 
        W = torch.from_numpy(tmp).to('cuda'), 
        W_type = 'raw'),
    batch_size = 50,
    shuffle = True)


exporatory_dataloader_hilb = DataLoader(
    SimBigDataset(
        lookup_obs= torch.linspace(0, tmp.shape[0], tmp.shape[0])[:, None].to('cuda'), 
        W = torch.from_numpy(tmp).to('cuda'), 
        W_type = 'hilbert'),
    batch_size = 50,
    shuffle = True)
```

# Setup VAE Classes

## Fitting Dense vae

```{python}
# TODO, move this to a module
class Linear_res_block(nn.Module):
    def __init__(self, in_size, out_size, drop_pr):
        super(Linear_res_block, self).__init__()
        self.squish = nn.Linear(in_size, out_size)
        self.block1 = nn.Sequential(
            nn.Linear(out_size, out_size),
            nn.ReLU(),
            nn.Dropout(drop_pr)
        )
        self.block2 = nn.Sequential(
            nn.Linear(out_size, out_size),
            nn.ReLU(),
            nn.Dropout(drop_pr)
        )
    def forward(self, x):
        squish_residual = self.squish(x)
        out = self.block1(squish_residual)
        out = self.block2(out)
        out += squish_residual
        return out  
```


```{python}
# This one is designed to go from (3x125891) -> ~1258.91  -> ~125.891 -> ~12.5891 -> 1
layer_sizes = [1024, 512]
layer_drops = [0.1 for e in layer_sizes]

num_layers = len(layer_sizes)

params = {
    'num_layers':num_layers,
    f"in_1_of_{num_layers}": 17*365
}

for i in range(num_layers):
    params[f"out_{ i + 1}_of_{num_layers}"] = layer_sizes[i]
    params[f"drop_{ i + 1}_of_{num_layers}"] = layer_drops[i]

params
```

```{python}
# need a way to convert the parameterization to the reverse of it
def _reverse_vae_fc_params(params):

    reversed_params = {}

    num_layers = params['num_layers']


    size_keys = [f'in_1_of_{num_layers}'] + [f'out_{i+1}_of_{num_layers}' for i in range(params['num_layers'])]
    dropout_keys = [f'drop_{i+1}_of_{num_layers}' for i in range(params['num_layers'])]

    size_values = [params[e] for e in size_keys]
    size_values.reverse()

    dropout_values = [params[e] for e in dropout_keys]
    dropout_values.reverse()


    reversed_params['num_layers'] = num_layers

    for i in range(len(size_keys)):
        reversed_params[size_keys[i]] = size_values[i]   

    for i in range(len(dropout_keys)):
        reversed_params[dropout_keys[i]] = dropout_values[i]  

    return(reversed_params)

reversed_params =  _reverse_vae_fc_params(params)
```

```{python}
class FcEncoder(nn.Module):
    def __init__(self, input_shape, parameterization):
        super(FcEncoder, self).__init__()
        self.input_shape = input_shape

        self.encoder_flatten = nn.Flatten()

        # self.encoder = nn.ModuleList([
        #     Linear_block(17*365, 1, 0)
        # ])
        module_list = []
        max_layer = parameterization['num_layers']
        for i in range(max_layer):
            if i  == 0:
                name_in = f"in_{i+1}_of_{max_layer}"
            else:
                name_in = f"out_{i}_of_{max_layer}"
            name_out = f"out_{i+1}_of_{max_layer}"
            name_drop= f"drop_{i+1}_of_{max_layer}"

            if i == 0:
                module_list += [nn.Flatten()]
            

            module_list += [
                Linear_res_block(
                    in_size  = parameterization[name_in], 
                    out_size = parameterization[name_out], 
                    drop_pr  = parameterization[name_drop])]
            
            # if (i+1) == max_layer:
            #     module_list += [nn.Linear(parameterization[name_out], 1)]
                
        self.encoder = nn.ModuleList(module_list)



        x = torch.empty(self.input_shape)
        with torch.no_grad():
            if len(x.shape) > 2:
                x = self.encoder_flatten(x)
            for mod in self.encoder:
                x = mod(x)
        self.output_shape_flat = x.shape        

    def forward(self, x):
        if len(x.shape) > 2:
            x = self.encoder_flatten(x)

        for mod in self.encoder:
            x = mod(x)
        return(x)
    
x = next(iter(exporatory_dataloader))[0]
vae_en = FcEncoder(input_shape = next(iter(exporatory_dataloader))[0].shape, parameterization=params ).to('cuda')
res = vae_en(x)
res.shape
```

```{python}
class vaeReparam(nn.Module):
    def __init__(self, in_dims, latent_dims):
        super(vaeReparam, self).__init__()
        self.fc_mu = nn.Sequential(nn.Linear(in_dims, latent_dims))
        self.fc_log_var = nn.Sequential(nn.Linear(in_dims, latent_dims))

    def forward(self, res):
        #TODO flatten outside of a layer like so? torch.flatten(torch.zeros((2,2)))
        x = nn.Flatten()(res)
        mu = self.fc_mu(x)
        log_var = self.fc_log_var(x)
        return([mu, log_var])

# vae_rp = vaeReparam(vae_en.output_shape_flat[1], 256).to('cuda')
vae_rp = vaeReparam(512, 256).to('cuda')
mu, log_var = vae_rp(res)  
mu.shape
```

```{python}
class vaeSample(nn.Module):
    def __init__(self):
        super(vaeSample, self).__init__()

    def forward(self, mu, log_var):
        std = torch.exp(log_var/2)
        p = torch.distributions.Normal(torch.zeros_like(mu), torch.ones_like(std))
        q = torch.distributions.Normal(mu, std)
        z = q.rsample()
        return p, q, z

vae_samp = vaeSample().to('cuda')
p, q, z = vae_samp(mu, log_var)
```

```{python}
class vaeExpand(nn.Module):
    def __init__(self, latent_dims, out_dims
                #  , out_shape
                 ):
        super(vaeExpand, self).__init__()
        self.fc_expand = nn.Sequential(nn.Linear(latent_dims, out_dims))

    def forward(self, res):
        res = self.fc_expand(res)
        return(res)


# vae_ex = vaeExpand(256, vae_en.output_shape_flat[1]).to('cuda')
vae_ex = vaeExpand(256, 512).to('cuda')
res2 = vae_ex(z)
res2.shape
```

```{python}
class FcDecoder(nn.Module):
    def __init__(
        self,
        output_shape,
        parameterization
    ):
        super(FcDecoder, self).__init__()
        self.output_shape = output_shape

        # self.decoder = nn.ModuleList([
        #     Linear_block(1, 17*365, 0)
        #     ])
        module_list = []
        max_layer = parameterization['num_layers']
        for i in range(max_layer):
            if i  == 0:
                name_in = f"in_{i+1}_of_{max_layer}"
            else:
                name_in = f"out_{i}_of_{max_layer}"
            name_out = f"out_{i+1}_of_{max_layer}"
            name_drop= f"drop_{i+1}_of_{max_layer}"

            if i == 0:
                module_list += [nn.Flatten()]
            

            module_list += [
                Linear_res_block(
                    in_size  = parameterization[name_in], 
                    out_size = parameterization[name_out], 
                    drop_pr  = parameterization[name_drop])]
            
            # if (i+1) == max_layer:
            #     module_list += [nn.Linear(parameterization[name_out], 1)]
                
        self.decoder = nn.ModuleList(module_list)
 
    def forward(self, x):
        for mod in self.decoder:
            x = mod(x)
        x = x.reshape([-1]+list(self.output_shape)[1:])
        return(x)
                
vae_dc = FcDecoder(output_shape= next(iter(exporatory_dataloader))[0].shape, parameterization=reversed_params) .to('cuda')
vae_dc(res).shape, vae_dc(res2).shape
```

```{python}
tmp = [2**i for i in range(6, 13)]
tmp.reverse()
tmp
```

```{python}
# layer_sizes = [4096, 2048, 1024, 512, 256, 128, 64]
layer_sizes = [#4096, 2048, 1024, 
    512, 256]
layer_drops = [0.1 for e in layer_sizes]

num_layers = len(layer_sizes)

params = {
    'num_layers':num_layers,
    f"in_1_of_{num_layers}": 17*365
}

for i in range(num_layers):
    params[f"out_{ i + 1}_of_{num_layers}"] = layer_sizes[i]
    params[f"drop_{ i + 1}_of_{num_layers}"] = layer_drops[i]

reversed_params =  _reverse_vae_fc_params(params)
```

```{python}
# latent_dims = 256

# # vae_en   = FcEncoder(input_shape = next(iter(training_dataloader)).shape )
# vae_en = FcEncoder(input_shape = next(iter(exporatory_dataloader))[0].shape, parameterization=params ).to('cuda')
# vae_rp   = vaeReparam(vae_en.output_shape_flat[1], latent_dims)
# vae_samp = vaeSample()
# vae_ex   = vaeExpand(latent_dims, vae_en.output_shape_flat[1])
# # vae_dc   = FcDecoder(output_shape= next(iter(training_dataloader)).shape )
# vae_dc = FcDecoder(output_shape= next(iter(exporatory_dataloader))[0].shape, parameterization=reversed_params) .to('cuda')


# vae_en = vae_en.to('cuda')
# vae_rp = vae_rp.to('cuda')
# vae_samp = vae_samp.to('cuda')
# vae_ex = vae_ex.to('cuda')
# vae_dc = vae_dc.to('cuda')
```

```{python}
class hWthrVAE(pl.LightningModule):
    def __init__(self, enc, rpm, smp, ex, dcd, kl_coeff = 0.1):
        super().__init__()
        self.enc = enc
        self.rpm = rpm
        self.smp = smp
        self.ex = ex
        self.dcd = dcd
        
        self.kl_coeff = kl_coeff

    def forward(self, x):
        mu, log_var = self.rpm(self.enc(x))
        p, q, z = self.smp(mu, log_var)
        xhat = self.dcd(self.ex(z))
        return(xhat)

    def embed(self, x):
        mu, log_var = self.rpm(self.enc(x))
        p, q, z = self.smp(mu, log_var)
        return(z)
    
    def training_step(self, batch, batch_idx):
        x = batch[0]        
        mu, log_var = self.rpm(self.enc(x))
        p, q, z = self.smp(mu, log_var)
        xprime = self.ex(z)
        xhat = self.dcd(xprime)
        # calculate loss
        recon_loss = F.mse_loss(xhat, x, reduction='mean')
        log_qz = q.log_prob(z)
        log_pz = p.log_prob(z)
        kl = log_qz - log_pz
        kl = kl.mean()
        kl *= self.kl_coeff
        loss = kl + recon_loss
        self.log('train_loss', loss)
        return loss#, logs
     
    def configure_optimizers(self, **kwargs):
        optimizer = torch.optim.Adam(self.parameters(), **kwargs)
        return optimizer  
```

```{python}
# max_epoch = 20
# logger = TensorBoardLogger("tb_vae_logs", name="fcn-e256-l2-ch512-256")

# test_vae = hWthrVAE(
#     enc= vae_en,
#     rpm= vae_rp,
#     smp= vae_samp,
#     ex = vae_ex,
#     dcd= vae_dc
# )#.to('cuda')

# optimizer = test_vae.configure_optimizers()

# trainer = pl.Trainer(max_epochs=max_epoch, logger=logger)

# trainer.fit(model=test_vae, train_dataloaders=exporatory_dataloader)
```

#### Check Predictions

```{python}
# xs = next(iter(exporatory_dataloader))[0]
# xs = xs.detach().cpu()
# xhats = trainer.model(xs).detach().cpu()
```

```{python}
# px.imshow(
# np.concatenate([
#     reverse_cs(torch.Tensor.numpy(xs[0])[0:16, ], cs_dict['histW']),
#     torch.Tensor.numpy(xs[0])[-1, ][None, ]
#     ], axis = 0)
# )
```

```{python}
# px.imshow(
# np.concatenate([
#     reverse_cs(torch.Tensor.numpy(xhats[0])[0:16, ], cs_dict['histW']),
#     torch.Tensor.numpy(xhats[0])[-1, ][None, ]
#     ], axis = 0)
# )
```

```{python}
# px.imshow(xhats[0]-xs[0])
```

#### Visualize Embeddings

```{python}
# xs = next(iter(exporatory_dataloader))[0]
# xs = xs.detach().cpu()
# xhats = trainer.model(xs).detach().cpu()
```

```{python}
# embeddings = []

# for i, x in enumerate(exporatory_dataloader):
#     with torch.no_grad():
#         embeddings += [trainer.model.embed(x[0].to('cpu'))]

# embeddings = torch.concat(embeddings, axis = 0)
```

```{python}
# X_embedded = TSNE(n_components=2, learning_rate='auto', init='random', perplexity=3).fit_transform(embeddings)
```

```{python}
# temp = pd.concat([pd.DataFrame(training_metadata, columns=['Longitude', 'Latitude', 'Year']), 
#                   pd.DataFrame(X_embedded, columns = ['TSNE1', 'TSNE2'])], axis = 1)

# temp['LatLon']=temp['Latitude'].astype(str)+'_'+temp['Longitude'].astype(str)
```

```{python}
# px.scatter(temp, x = 'Longitude', y = 'Latitude')
```

```{python}
# px.scatter(temp, x = 'TSNE1', y = 'TSNE2', color='LatLon')
```

## Fitting Conv1d model

```{python}
# TODO move to module and replace below version of this class
class ResBlock(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size = 3, stride = 1, padding = 1, downsample = None, conv_type = 2):
        super(ResBlock, self).__init__()
        self.conv_type = conv_type

        if conv_type == 1:
            self.conv1 = nn.Sequential(
                            nn.Conv1d(in_channels, out_channels, kernel_size = kernel_size, stride = stride, padding = padding),
                            nn.BatchNorm1d(out_channels),
                            nn.ReLU())
            self.conv2 = nn.Sequential(
                            nn.Conv1d(out_channels, out_channels, kernel_size = kernel_size, stride = stride, padding = padding),
                            nn.BatchNorm1d(out_channels))

        if conv_type == 2:
            self.conv1 = nn.Sequential(
                            nn.Conv2d(in_channels, out_channels, kernel_size = kernel_size, stride = stride, padding = padding),
                            nn.BatchNorm2d(out_channels),
                            nn.ReLU())
            self.conv2 = nn.Sequential(
                            nn.Conv2d(out_channels, out_channels, kernel_size = kernel_size, stride = stride, padding = padding),
                            nn.BatchNorm2d(out_channels))
            
        self.downsample = downsample
        self.relu = nn.ReLU()
        self.out_channels = out_channels

    def forward(self, x):
        residual = x
        out = self.conv1(x)     # Note, these are both conv2d _or_ conv1d depending on above
        out = self.conv2(out)   #
        if self.downsample:
            residual = self.downsample(x)
        out += residual
        out = self.relu(out)
        return out
```

```{python}
class ResConv1dEncoder(nn.Module):
    def __init__(
        self, 
        input_shape #= torch.Size([40, 1, 28, 28])
        ):
        super(ResConv1dEncoder, self).__init__()
        self.encoder = nn.ModuleList([
            # ResBlock( in_channels=17,   out_channels=17,   kernel_size=3,      stride=1,     padding=1),
            nn.Conv1d(in_channels=17,   out_channels=2*17, kernel_size=6, stride=2, padding=2, bias=False),

            ResBlock( in_channels=2*17, out_channels=2*17, kernel_size=3,      stride=1,     padding=1, conv_type = 1),
            nn.Conv1d(in_channels=2*17, out_channels=4*17, kernel_size=6, stride=2, padding=2, bias=False),

            ResBlock( in_channels=4*17, out_channels=4*17, kernel_size=3,      stride=1,     padding=1, conv_type = 1),
            nn.Conv1d(in_channels=4*17, out_channels=8*17, kernel_size=6, stride=2, padding=2, bias=False),

            ResBlock( in_channels=8*17, out_channels=8*17, kernel_size=3,      stride=1,     padding=1, conv_type = 1),
            nn.Conv1d(in_channels=8*17, out_channels=16*17, kernel_size=6, stride=2, padding=2, bias=False)            
            ])

        # Find tensor sizes for each block and level
        self.input_shape = input_shape
        self.intermediate_shapes = [input_shape]

        x = torch.empty(self.input_shape)
        with torch.no_grad():
            for mod in self.encoder:
                x = mod(x)
                self.intermediate_shapes += [x.shape]
        
        # self.output_shape = self.intermediate_shapes.pop()
        self.output_shape = self.intermediate_shapes[-1]
        self.output_shape_flat = nn.Flatten()(x).shape
        
    def forward(self, x):
        for mod in self.encoder:
            # print(mod)
            x = mod(x)
            # print(x.shape)
        return(x)
    
x = next(iter(exporatory_dataloader))[0]
vae_en = ResConv1dEncoder(input_shape = next(iter(exporatory_dataloader))[0].shape ).to('cuda')
res = vae_en(x)
vae_en.intermediate_shapes, res.shape

```

```{python}
next(iter(exporatory_dataloader))[0].shape
```

```{python}
vae_rp = vaeReparam(vae_en.output_shape_flat[1], 256).to('cuda')
mu, log_var = vae_rp(res)  
mu.shape
```

```{python}
vae_samp = vaeSample().to('cuda')
p, q, z = vae_samp(mu, log_var)
```

```{python}
vae_ex = vaeExpand(256, vae_en.output_shape_flat[1]).to('cuda')
res2 = vae_ex(z)
res2.shape
```

```{python}
class ResConv1dDecoder(nn.Module):
    def __init__(
        self,
        enc_intermediate_shapes    
    ):
        super(ResConv1dDecoder, self).__init__()
        self.decoder = nn.ModuleList([
            
            nn.ConvTranspose1d(in_channels=16*17 , out_channels=8*17, kernel_size=6, stride=2, padding=2, bias=False),    
            ResBlock(          in_channels=8*17 , out_channels=8*17, kernel_size=3,      stride=1,     padding=1, conv_type = 1),
            
            nn.ConvTranspose1d(in_channels=8*17 , out_channels=4*17, kernel_size=6, stride=2, padding=2, bias=False),
            ResBlock(          in_channels=4*17 , out_channels=4*17, kernel_size=3,      stride=1,     padding=1, conv_type = 1),
            
            nn.ConvTranspose1d(in_channels=4*17 , out_channels=2*17, kernel_size=6, stride=2, padding=2, bias=False),
            ResBlock(          in_channels=2*17 , out_channels=2*17, kernel_size=3,      stride=1,     padding=1, conv_type = 1),
            
            nn.ConvTranspose1d(in_channels=2*17,  out_channels=17, kernel_size=6, stride=2, padding=2, bias=False),
            ])
        
        # overwrite first value with -1
        vals = [list(e) for e in enc_intermediate_shapes.copy()]
        vals = [tuple([-1]+e[1:]) for e in vals]
        # self.input_shape = vals[-1] #.pop()
        self.input_shape = vals.pop()
        vals.reverse()
        self.intermediate_shapes = vals#[1:] # Onve value here is the input 
        # print(self.intermediate_shapes)
 
    def forward(self, x):
        # x = x.reshape((-1, 8, 4, 4))
        x = x.reshape(self.input_shape)
        # print(x.shape)
        for i in range(len(self.decoder)):
            # upsampling only happens in the nn.ConvTranspos2d layers so if we're running a ResBlock, we'll run it without an output_size
            if 'ResBlock' in str(type(self.decoder[i])):
                x = self.decoder[i](x)
            else:    
                # print(self.intermediate_shapes[i])
                x = self.decoder[i](x, output_size = self.intermediate_shapes[i])
            # print('_')
            # print(x.shape)
        return(x)
  
vae_dc = ResConv1dDecoder(enc_intermediate_shapes= vae_en.intermediate_shapes.copy()).to('cuda')
vae_dc(res).shape
```

```{python}
# latent_dims = 256

# vae_en   = ResConv1dEncoder(input_shape = next(iter(exporatory_dataloader))[0].shape )
# vae_rp   = vaeReparam(vae_en.output_shape_flat[1], latent_dims)
# vae_samp = vaeSample()
# vae_ex   = vaeExpand(latent_dims, vae_en.output_shape_flat[1])
# vae_dc   = ResConv1dDecoder(enc_intermediate_shapes= vae_en.intermediate_shapes.copy())

# vae_en = vae_en.to('cuda')
# vae_rp = vae_rp.to('cuda')
# vae_samp = vae_samp.to('cuda')
# vae_ex = vae_ex.to('cuda')
# vae_dc = vae_dc.to('cuda')
```

```{python}
# max_epoch = 20

# logger = TensorBoardLogger("tb_vae_logs", name="conv1-e256")

# test_vae = hWthrVAE(
#     enc= vae_en,
#     rpm= vae_rp,
#     smp= vae_samp,
#     ex = vae_ex,
#     dcd= vae_dc
# )#.to('cuda')

# optimizer = test_vae.configure_optimizers()

# trainer = pl.Trainer(max_epochs=max_epoch, logger=logger)

# trainer.fit(model=test_vae, train_dataloaders=exporatory_dataloader)
```

```{python}
# # How much better is a bigger latent space?
# latent_dims = 512

# vae_en   = ResConv1dEncoder(input_shape = next(iter(exporatory_dataloader))[0].shape )
# vae_rp   = vaeReparam(vae_en.output_shape_flat[1], latent_dims)
# vae_samp = vaeSample()
# vae_ex   = vaeExpand(latent_dims, vae_en.output_shape_flat[1])
# vae_dc   = ResConv1dDecoder(enc_intermediate_shapes= vae_en.intermediate_shapes.copy())

# vae_en = vae_en.to('cuda')
# vae_rp = vae_rp.to('cuda')
# vae_samp = vae_samp.to('cuda')
# vae_ex = vae_ex.to('cuda')
# vae_dc = vae_dc.to('cuda')


# max_epoch = 20

# logger = TensorBoardLogger("tb_vae_logs", name="conv1-e512")

# test_vae = hWthrVAE(
#     enc= vae_en,
#     rpm= vae_rp,
#     smp= vae_samp,
#     ex = vae_ex,
#     dcd= vae_dc
# )#.to('cuda')

# optimizer = test_vae.configure_optimizers()

# trainer = pl.Trainer(max_epochs=max_epoch, logger=logger)

# trainer.fit(model=test_vae, train_dataloaders=exporatory_dataloader)
```

```{python}
# What about a bigger latent space and a larger batch size? Both seem to help.

mask = usaMatMetadata[:, 2] < 2013

training_metadata = usaMatMetadata[mask, ]

tmp = usaMat[mask, :, :]
cs_dict = {'histW':calc_cs(tmp)}
tmp = apply_cs(tmp, cs_dict['histW'])

exporatory_dataloader = DataLoader(
    SimBigDataset(
        lookup_obs= torch.linspace(0, tmp.shape[0], tmp.shape[0])[:, None].to('cuda'), 
        W = torch.from_numpy(tmp).to('cuda'), 
        W_type = 'raw'),
    batch_size = 512,
    shuffle = True)
```

```{python}

# latent_dims = 512

# vae_en   = ResConv1dEncoder(input_shape = next(iter(exporatory_dataloader))[0].shape )
# vae_rp   = vaeReparam(vae_en.output_shape_flat[1], latent_dims)
# vae_samp = vaeSample()
# vae_ex   = vaeExpand(latent_dims, vae_en.output_shape_flat[1])
# vae_dc   = ResConv1dDecoder(enc_intermediate_shapes= vae_en.intermediate_shapes.copy())

# vae_en = vae_en.to('cuda')
# vae_rp = vae_rp.to('cuda')
# vae_samp = vae_samp.to('cuda')
# vae_ex = vae_ex.to('cuda')
# vae_dc = vae_dc.to('cuda')


# max_epoch = 100

# logger = TensorBoardLogger("tb_vae_logs", name="conv1-e512-b512")

# test_vae = hWthrVAE(
#     enc= vae_en,
#     rpm= vae_rp,
#     smp= vae_samp,
#     ex = vae_ex,
#     dcd= vae_dc
# )#.to('cuda')

# optimizer = test_vae.configure_optimizers()

# trainer = pl.Trainer(max_epochs=max_epoch, logger=logger)

# trainer.fit(model=test_vae, train_dataloaders=exporatory_dataloader)
```

```{python}
# what about an even deeper network? 
# I tried this with standard res blocks and they crashed. I'm trying it using fixup blocks. I read through the paper and found this helpful implementation through papers with code
# https://github.com/hongyi-zhang/Fixup/blob/master/cifar/models/fixup_resnet_cifar.py#L106
```

```{python}
class ResBlockConv1Fixup(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias = False, downsample = None):
        super(ResBlockConv1Fixup, self).__init__()
        self.l1_conv  = nn.Conv1d(in_channels= in_channels,
                                  out_channels= out_channels,
                                  kernel_size= kernel_size,
                                  stride= stride,
                                  padding= padding,
                                  bias= bias)
        self.l1_bias1 = nn.Parameter(torch.zeros(1))
        self.l1_bias2 = nn.Parameter(torch.zeros(1))
        
        self.l2_conv  = nn.Conv1d(in_channels= out_channels,
                                  out_channels= out_channels,
                                  kernel_size= kernel_size,
                                  stride= 1,
                                  padding= padding,
                                  bias= bias)
        self.l2_bias1 = nn.Parameter(torch.zeros(1))
        self.l2_bias2 = nn.Parameter(torch.zeros(1))
        self.l2_scale = nn.Parameter(torch.ones(1))    
        self.downsample = downsample 

    def forward(self, x):
        xin = x
        # long path
        out =    self.l1_conv(  x + self.l1_bias1) 
        # print(xin.shape, out.shape)
        out =         F.relu( out + self.l1_bias2)
        # print(xin.shape, out.shape)
        out =    self.l2_conv(out + self.l2_bias1)
        # print(xin.shape, out.shape)
        out = out * self.l2_scale + self.l2_bias2
        # short path
        if self.downsample is not None:
            xin = self.downsample(x+self.l1_bias1)
            xin = torch.cat((xin, torch.zeros_like(xin)), 1)
        
        # print(xin.shape, out.shape)
        out = F.relu(out + xin)

        return(out)
    
class ResNetConv1Fixup(nn.Module):
    def _make_layer_block(self, block, out_channels, num_blocks, stride = 1):
        downsample = None
        if stride != 1: 
            downsample = nn.AvgPool1d(1, stride = stride)

        layer_list = []
        layer_list.append(block(self.in_channels, out_channels, stride = stride, downsample = downsample))
        # update to the new number of channels
        self.in_channels = out_channels
        for _ in range(1, num_blocks):
            layer_list.append(block(out_channels, out_channels))

        return nn.Sequential(*layer_list)


    def __init__(self, block: nn.Module, layers: list, outputs: int = 1):
        super(ResNetConv1Fixup, self).__init__()
        self.num_layers = sum(layers)
        self.in_channels = 32 # note this is into the layer blocks not in here.
        self.conv   = nn.Conv1d(in_channels= 17, out_channels= 32, kernel_size=3, stride=1, padding=1, bias = False)
        self.bias1  = nn.Parameter(torch.zeros(1))

        self.block1 = self._make_layer_block(block, out_channels=32, num_blocks=layers[0], stride= 1) 
        self.block2 = self._make_layer_block(block, out_channels=64, num_blocks=layers[1], stride= 2) 
        self.block3 = self._make_layer_block(block, out_channels=128,num_blocks=layers[2], stride= 2) 

        self.avgpool= nn.AdaptiveAvgPool1d((1))
        self.bias2  = nn.Parameter(torch.zeros(1))
        self.fc     = nn.Linear(128, outputs)

        for mod in self.modules():
            if isinstance(mod, ResBlockConv1Fixup):        
                # original version 
                # conv_scaling = np.sqrt(2 / (mod.l1_conv.weight.shape[0] * np.prod(mod.l1_conv.weight.shape[2:]))) * self.num_layers ** (-0.5)
                # remade in pytorch
                conv_scaling = torch.sqrt(2 / (torch.tensor(mod.l1_conv.weight.shape[0]) * torch.prod(torch.tensor(mod.l1_conv.weight.shape[2:])))) * self.num_layers ** (-0.5)
                nn.init.normal_(mod.l1_conv.weight, mean = 0, std= conv_scaling)
                nn.init.constant_(mod.l2_conv.weight, 0)
            elif isinstance(mod, nn.Linear):
                nn.init.constant_(mod.weight, 0)
                nn.init.constant_(mod.bias, 0)

    def forward(self, x):
        print(x.shape)
        x = F.relu(self.conv(x) + self.bias1)
        print(x.shape)
        x = self.block1(x)
        print(x.shape)
        x = self.block2(x)
        print(x.shape)
        x = self.block3(x)
        print(x.shape)
        x = self.avgpool(x)
        print(x.shape)
        x = x.view(x.size(0), -1)
        print(x.shape)
        x = self.fc(x + self.bias2)
        print(x.shape)
        return x

model = ResNetConv1Fixup(ResBlockConv1Fixup, layers = [3,3,3], outputs=1)
model(torch.randn([512, 17, 365])).shape
```

```{python}
# goal structure for vae

x = torch.randn([512, 17, 384]) # pad by 19 days

mods = nn.ModuleList([
    nn.Conv1d(in_channels= 17, out_channels= 17, kernel_size=4, stride=2, padding=1, bias = False),
    nn.Conv1d(in_channels= 17, out_channels= 17, kernel_size=4, stride=2, padding=1, bias = False),
    nn.Conv1d(in_channels= 17, out_channels= 17, kernel_size=4, stride=2, padding=1, bias = False),
    nn.Conv1d(in_channels= 17, out_channels= 17, kernel_size=4, stride=2, padding=1, bias = False),
    nn.Conv1d(in_channels= 17, out_channels= 17, kernel_size=4, stride=2, padding=1, bias = False),
    nn.Conv1d(in_channels= 17, out_channels= 17, kernel_size=4, stride=2, padding=1, bias = False),
    nn.Conv1d(in_channels= 17, out_channels= 17, kernel_size=4, stride=2, padding=1, bias = False),

    nn.Conv1d(in_channels= 17, out_channels= 17, kernel_size=3, stride=1, padding=1, bias = False),

    torch.nn.Upsample(scale_factor=2), #'nearest', 'linear', 'bilinear', 'bicubic' and 'trilinear'. Default: 'nearest'
    nn.Conv1d(in_channels= 17, out_channels= 17, kernel_size=3, stride=1, padding=1, bias = False),

    torch.nn.Upsample(scale_factor=2), 
    nn.Conv1d(in_channels= 17, out_channels= 17, kernel_size=3, stride=1, padding=1, bias = False),

    torch.nn.Upsample(scale_factor=2), 
    nn.Conv1d(in_channels= 17, out_channels= 17, kernel_size=3, stride=1, padding=1, bias = False),

    torch.nn.Upsample(scale_factor=2), 
    nn.Conv1d(in_channels= 17, out_channels= 17, kernel_size=3, stride=1, padding=1, bias = False),

    torch.nn.Upsample(scale_factor=2), 
    nn.Conv1d(in_channels= 17, out_channels= 17, kernel_size=3, stride=1, padding=1, bias = False),

    torch.nn.Upsample(scale_factor=2), 
    nn.Conv1d(in_channels= 17, out_channels= 17, kernel_size=3, stride=1, padding=1, bias = False),

    torch.nn.Upsample(scale_factor=2), 
    nn.Conv1d(in_channels= 17, out_channels= 17, kernel_size=3, stride=1, padding=1, bias = False),
])

for mod in mods:
    l = x.shape[-1]
    x = mod(x)
    if isinstance(mod, nn.Upsample): pass
    else: print(f'{x.shape[-1]/l} \t {x.shape[-1]}')

x = x[:, :, 9:-10]
print(f'{x.shape[-1]/l} \t {x.shape[-1]}')
```

## Fitting LSTM model

```{python}
# Ironing out this class was a bit of a challenge. The key issue here is that the number of layers in the lstm changes the shape of the hidden state, thus chaninging the shape of the embeddings.
# This is okay if there's only one layer but if there are two then expanding the z matrix results in z*num_layers samples. To fix this I changed the embedding to pass the hidden state through a fc
# layer reducing the layer dimension to one and on the other side in the expander I increase this back up up to num_layers. The benefit of doing this this way is that the z matrix can still be 
# repeated to create the [seq, hidden, latent] shaped matrix needed for the input to the decoder. 

class LstmEncoder(nn.Module):
    def __init__(
        self, 
        in_channels, hidden_channels, num_layers, 
        einops_str='b c s -> s b c' 
        ):
        super(LstmEncoder, self).__init__()
        
        self.einops_str =  einops_str
        self.encoder = nn.LSTM(in_channels, hidden_channels, num_layers, bidirectional=False)
        # this is the result of a lot of debugging. basically, since the hidden state is the input to forward here if there is one layer in the lstm then the hidden 
        # state is of shape [batch, channels] but if there are more it's of shape [layers, batch, channels]. This increases the size of the output downstream so 
        # I'm collapsing the layer dim here before reparameterizing. I'm not doing this in encoder 
        self.fc_collapse_layerdim = nn.Sequential(nn.Linear(num_layers, 1))


    def forward(self, x):
        if self.einops_str != None:
            x = einops.rearrange(x, self.einops_str)
        out, hidden = self.encoder(x)
        # return(out, hidden)
        
        x2 = hidden[0]
        x2 = einops.rearrange(x2, 'l b c -> b c l')
        x2 = self.fc_collapse_layerdim(x2)
        x2 = einops.rearrange(x2, 'b c l -> l b c')
        new_hidden = (x2, hidden[1])
        return out, new_hidden

```

```{python}

# new version for lstm (removed flattening)
class LstmReparam(nn.Module):
    def __init__(self, in_channels, latent_channels, num_layers = 1, warning = False):
        super(LstmReparam, self).__init__()
        self.fc_mu = nn.Sequential(nn.Linear(in_channels, latent_channels))
        self.fc_log_var = nn.Sequential(nn.Linear(in_channels, latent_channels))

    def forward(self, res):
        x = res         
        mu = self.fc_mu(x)
        log_var = self.fc_log_var(x)
        return([mu, log_var])
    
```

```{python}
class vaeSample(nn.Module):
    def __init__(self):
        super(vaeSample, self).__init__()

    def forward(self, mu, log_var):
        std = torch.exp(log_var/2)
        p = torch.distributions.Normal(torch.zeros_like(mu), torch.ones_like(std))
        q = torch.distributions.Normal(mu, std)
        z = q.rsample()
        return p, q, z
```

```{python}

class LstmExpand(nn.Module):
    def __init__(self, latent_dims, out_dims, num_layers
                 ):
        super(LstmExpand, self).__init__()
        self.fc_expand = nn.Sequential(nn.Linear(latent_dims, out_dims))

        # opposite of fc_collapse_layerdim
        self.fc_expand_layerdim = nn.Sequential(nn.Linear(1, num_layers))

    def forward(self, res):
        res = self.fc_expand(res)        
        # make sure there are the correct number of dimensions and the dimension for layer is correctly set
        res = res[None, :, :]
        res = einops.rearrange(res, 'l b c -> b c l')
        res = self.fc_expand_layerdim(res)
        res = einops.rearrange(res, 'b c l -> l b c')

        return res        
```

```{python}

class LstmDecoder(nn.Module):
    def __init__(
        self, 
        in_channels, hidden_channels, out_channels, num_layers, 
        einops_str='b c s -> s b c' 
        ):
        super(LstmDecoder, self).__init__()
        
        # rearrange to make things easy
        rev_einops_str = None
        if einops_str != None:
            tmp = einops_str.split('->')
            rev_einops_str = tmp[1]+' -> '+tmp[0]
        self.rev_einops_str = rev_einops_str

        self.decoder = nn.LSTM(in_channels, hidden_channels, num_layers, bidirectional=False)
        self.decoder_fc = nn.Linear(hidden_channels, out_channels)

    def forward(self, z, decoder_state):
        out, hidden = self.decoder(z, decoder_state)
        pred = self.decoder_fc(out)
        if self.rev_einops_str != None:
            pred = einops.rearrange(pred, self.rev_einops_str)
        return pred
    
```

```{python}
class LstmVae(nn.Module):
    def __init__(self, 
                 in_channels, 
                 hidden_channels, 
                 latent_channels, 
                 num_layers, 
                 x_shape, # assumes order batch, channel, seq
                 einops_str = 'b c s -> s b c' # assume batch channel seq. If this isn't the case then extracting shape values won't work right.
                 ):
        super(LstmVae, self).__init__()
        self.in_channels = in_channels
        self.hidden_channels = hidden_channels
        self.latent_channels = latent_channels
        self.num_layers = num_layers
        self.einops_str = einops_str

        batch_size, channel_num, seq_len = x_shape
        # print(batch_size, seq_len, channel_num)
        self.seq_len = seq_len
        self.batch_size = batch_size
        self.channel_num = channel_num

        self.encoder =  LstmEncoder(
            in_channels=in_channels, 
            hidden_channels=hidden_channels, 
            num_layers=num_layers, 
            einops_str=einops_str
            )
        
        self.reparameterizer = LstmReparam(
            in_channels=hidden_channels,
            latent_channels=latent_channels
            )
        
        self.sampler = vaeSample()

        self.expander = LstmExpand(
            latent_dims=latent_channels , 
            out_dims=hidden_channels, 
            num_layers=num_layers
            )

        self.decoder = LstmDecoder(
            in_channels= latent_channels,
            hidden_channels=hidden_channels,
            out_channels=in_channels,
            num_layers=num_layers,
            einops_str=einops_str
            )


    def encode(self, x):
        out, hidden = self.encoder(x)
        # print(f'encode hidden 0 {hidden[0].shape}')
        #              hidden contains (hidden_state, cell)
        #              view is removing a unit dimension in position 0
        # hidden_space = hidden[0].view(self.batch_size, self.hidden_channels) # same a squeeze here.
        hidden_space = hidden[0].squeeze()
        return hidden_space
    
    def decode(self, z):
        estimated_hidden_state = self.expander(z)
        z_in = z.repeat(self.seq_len, 1, 1)

        decoder_state = (
                estimated_hidden_state.contiguous(),
                estimated_hidden_state.contiguous()
            )
        
        pred = self.decoder(z_in, decoder_state)
        return pred

   
    def forward(self, x, return_pqz = False):
        hidden_space = self.encode(x)      
        mu, log_var  = self.reparameterizer(hidden_space)
        p, q, z      = self.sampler(mu, log_var)
        # pred = self.decode(z)
        pred = self.decode(z)
        if return_pqz:
            return pred, p, q, z
        else:
            return pred


model = LstmVae(
    in_channels     = 17, 
    hidden_channels = 16, 
    latent_channels = 15, 
    num_layers      = 3, 
    x_shape         = next(iter(exporatory_dataloader))[0].shape,
    einops_str      = 'b c s -> s b c').to('cuda')

model(next(iter(exporatory_dataloader))[0], return_pqz = True)[0].shape
```

```{python}
class plVAE(pl.LightningModule):
    def __init__(self, vae, kl_coeff = 0.1):
        super().__init__()
        self.vae = vae
        self.kl_coeff = kl_coeff

    def forward(self, x, return_pqz = False):
        output = self.vae(x, return_pqz = return_pqz)
        return output
    
    def training_step(self, batch, batch_idx):
        x = batch[0]
        xhat, p, q, z = self.forward(x, return_pqz = True)
        # calculate losses
        ## reconstruction
        recon_loss = F.mse_loss(xhat, x, reduction='mean')
        ## KLD
        log_qz = q.log_prob(z)
        log_pz = p.log_prob(z)
        kl = log_qz - log_pz
        kl = kl.mean()
        kl *= self.kl_coeff
        loss = kl + recon_loss
        self.log('train_loss', loss)
        return loss#, logs
     
    def configure_optimizers(self, **kwargs):
        optimizer = torch.optim.Adam(self.parameters(), **kwargs)
        return optimizer  


# test_vae = plVAE(vae= model)

# optimizer = test_vae.configure_optimizers()

# max_epoch = 3
# trainer = pl.Trainer(max_epochs=max_epoch, logger=logger)

# trainer.fit(
#     model=test_vae, 
#     train_dataloaders=training_dataloader
#     )
```

#### Experiment with different architectures

```{python}
# tmp = usaMat
# cs_dict = {'histW':calc_cs(tmp)}
# tmp = apply_cs(tmp, cs_dict['histW'])

# training_dataloader = DataLoader(
#     SimBigDataset(
#         lookup_obs= torch.linspace(0, tmp.shape[0], tmp.shape[0])[:, None].to('cuda'), 
#         W = torch.from_numpy(tmp).to('cuda'), 
#         W_type = 'raw'),
#     batch_size = 50,
#     shuffle = True)
```

```{python}
max_epoch = 5
logger = TensorBoardLogger("tb_vae_logs", name="lstm-l1-ch17_17_17")


model = LstmVae(
    in_channels     = 17, 
    hidden_channels = 17, 
    latent_channels = 17, 
    num_layers      = 1, 
    x_shape         = next(iter(exporatory_dataloader))[0].shape,
    einops_str      = 'b c s -> s b c').to('cuda')

test_vae = plVAE(vae= model)

optimizer = test_vae.configure_optimizers()


trainer = pl.Trainer(max_epochs=max_epoch, logger=logger)

trainer.fit(model=test_vae, train_dataloaders=exporatory_dataloader)
```

```{python}
# max_epoch = 10
logger = TensorBoardLogger("tb_vae_logs", name="lstm-l3-ch17_17_17")


model = LstmVae(
    in_channels     = 17, 
    hidden_channels = 17, 
    latent_channels = 17, 
    num_layers      = 3, 
    x_shape         = next(iter(exporatory_dataloader))[0].shape,
    einops_str      = 'b c s -> s b c').to('cuda')

test_vae = plVAE(vae= model)

optimizer = test_vae.configure_optimizers()


trainer = pl.Trainer(max_epochs=max_epoch, logger=logger)

trainer.fit(model=test_vae, train_dataloaders=exporatory_dataloader)
```

```{python}
# max_epoch = 10
logger = TensorBoardLogger("tb_vae_logs", name="lstm-l1-ch17_32_64")


model = LstmVae(
    in_channels     = 17, 
    hidden_channels = 32, 
    latent_channels = 64, 
    num_layers      = 1, 
    x_shape         = next(iter(exporatory_dataloader))[0].shape,
    einops_str      = 'b c s -> s b c').to('cuda')

test_vae = plVAE(vae= model)

optimizer = test_vae.configure_optimizers()


trainer = pl.Trainer(max_epochs=max_epoch, logger=logger)

trainer.fit(model=test_vae, train_dataloaders=exporatory_dataloader)
```

```{python}
# max_epoch = 10
logger = TensorBoardLogger("tb_vae_logs", name="lstm-l1-ch17_128_256")


model = LstmVae(
    in_channels     = 17, 
    hidden_channels = 128, 
    latent_channels = 256, 
    num_layers      = 1, 
    x_shape         = next(iter(exporatory_dataloader))[0].shape,
    einops_str      = 'b c s -> s b c').to('cuda')

test_vae = plVAE(vae= model)

optimizer = test_vae.configure_optimizers()


trainer = pl.Trainer(max_epochs=max_epoch, logger=logger)

trainer.fit(model=test_vae, train_dataloaders=exporatory_dataloader)
```


#### Check Predictions

```{python}
# xs = next(iter(exporatory_dataloader))[0]
# xs = xs.detach().cpu()
# xhats = trainer.model(xs).detach().cpu()
```

```{python}
# px.imshow(
# np.concatenate([
#     reverse_cs(torch.Tensor.numpy(xs[0])[0:16, ], cs_dict['histW']),
#     torch.Tensor.numpy(xs[0])[-1, ][None, ]
#     ], axis = 0)
# )
```

```{python}
# px.imshow(
# np.concatenate([
#     reverse_cs(torch.Tensor.numpy(xhats[0])[0:16, ], cs_dict['histW']),
#     torch.Tensor.numpy(xhats[0])[-1, ][None, ]
#     ], axis = 0)
# )
```

```{python}
# px.imshow(xhats[0]-xs[0])
```

```{python}
# px.line(
#     pd.DataFrame(
#         np.transpose(
#             np.concatenate([
#                 reverse_cs(torch.Tensor.numpy(xhats[0])[0:16, ], cs_dict['histW']),
#                 torch.Tensor.numpy(xhats[0])[-1, ][None, ]
#                 ])), 
#                 columns=list(usa_grid_names[2:18])+['Planting']
#                 ))
```

#### Visualize Embeddings

```{python}
# embeddings = []

# for i, x in enumerate(exporatory_dataloader):
#     with torch.no_grad():
#         embeddings += [trainer.model.vae.encode(x[0].to('cpu'))]

# embeddings = torch.concat(embeddings, axis = 0)
```

```{python}
# X_embedded = TSNE(n_components=2, learning_rate='auto', init='random', perplexity=3).fit_transform(embeddings)
# # 2.34 to run for full dataset
```

```{python}
# temp = pd.concat([pd.DataFrame(training_metadata, columns=['Longitude', 'Latitude', 'Year']), 
#                   pd.DataFrame(X_embedded, columns = ['TSNE1', 'TSNE2'])], axis = 1)

# temp['LatLon']=temp['Latitude'].astype(str)+'_'+temp['Longitude'].astype(str)
```

```{python}
# px.scatter(temp, x = 'Longitude', y = 'Latitude')
```

```{python}
# px.scatter(temp, x = 'TSNE1', y = 'TSNE2', 
#         #    color='LatLon'
#            color = 'Year',
#            hover_data=['LatLon', 'Year']
#            )
```

## Fitting Conv2d model

```{python}
# TODO move to module
class ResBlock(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size = 3, stride = 1, padding = 1, downsample = None):
        super(ResBlock, self).__init__()
        self.conv1 = nn.Sequential(
                        nn.Conv2d(in_channels, out_channels, kernel_size = kernel_size, stride = stride, padding = padding),
                        nn.BatchNorm2d(out_channels),
                        nn.ReLU())
        self.conv2 = nn.Sequential(
                        nn.Conv2d(out_channels, out_channels, kernel_size = kernel_size, stride = stride, padding = padding),
                        nn.BatchNorm2d(out_channels))
        self.downsample = downsample
        self.relu = nn.ReLU()
        self.out_channels = out_channels

    def forward(self, x):
        residual = x
        out = self.conv1(x)     # Note, these are both conv2d
        out = self.conv2(out)   #
        if self.downsample:
            residual = self.downsample(x)
        out += residual
        out = self.relu(out)
        return out
```


```{python}
next(iter(exporatory_dataloader_hilb))[0].shape
```

```{python}
class ResConv2dEncoder(nn.Module):
    def __init__(
        self, 
        input_shape #= torch.Size([40, 1, 28, 28])
        ):
        super(ResConv2dEncoder, self).__init__()
        self.encoder = nn.ModuleList([
            # ResBlock( in_channels=17,   out_channels=17,   kernel_size=3,      stride=1,     padding=1),
            nn.Conv2d(in_channels=17,   out_channels=2*17, kernel_size=(7, 7), stride=(2,2), padding=(3,3), bias=False),

            ResBlock( in_channels=2*17, out_channels=2*17, kernel_size=3,      stride=1,     padding=1),
            nn.Conv2d(in_channels=2*17, out_channels=4*17, kernel_size=(7, 7), stride=(2,2), padding=(3,3), bias=False),

            ResBlock( in_channels=4*17, out_channels=4*17, kernel_size=3,      stride=1,     padding=1),
            nn.Conv2d(in_channels=4*17, out_channels=8*17, kernel_size=(7, 7), stride=(2,2), padding=(3,3), bias=False),

            ResBlock( in_channels=8*17, out_channels=8*17, kernel_size=3,      stride=1,     padding=1),
            nn.Conv2d(in_channels=8*17, out_channels=16*17, kernel_size=(7, 7), stride=(2,2), padding=(3,3), bias=False)            
            ])

        # Find tensor sizes for each block adnd level
        self.input_shape = input_shape
        self.intermediate_shapes = [input_shape]

        x = torch.empty(self.input_shape)
        with torch.no_grad():
            for mod in self.encoder:
                x = mod(x)
                self.intermediate_shapes += [x.shape]
        
        # self.output_shape = self.intermediate_shapes.pop()
        self.output_shape = self.intermediate_shapes[-1]
        self.output_shape_flat = nn.Flatten()(x).shape
        
    def forward(self, x):
        for mod in self.encoder:
            # print(mod)
            x = mod(x)
            # print(x.shape)
        return(x)
    
x = next(iter(exporatory_dataloader_hilb))[0]
vae_en = ResConv2dEncoder(input_shape = next(iter(exporatory_dataloader_hilb))[0].shape ).to('cuda')
res = vae_en(x)
vae_en.intermediate_shapes
```

```{python}

class vaeReparam(nn.Module):
    def __init__(self, in_dims, latent_dims):
        super(vaeReparam, self).__init__()
        self.fc_mu = nn.Sequential(nn.Linear(in_dims, latent_dims))
        self.fc_log_var = nn.Sequential(nn.Linear(in_dims, latent_dims))

    def forward(self, res):
        x = nn.Flatten()(res)
        # print(len(x))
        mu = self.fc_mu(x)
        log_var = self.fc_log_var(x)
        return([mu, log_var])

vae_rp = vaeReparam(vae_en.output_shape_flat[1], 256).to('cuda')
mu, log_var = vae_rp(res)  
mu.shape
```

```{python}

class vaeSample(nn.Module):
    def __init__(self):
        super(vaeSample, self).__init__()

    def forward(self, mu, log_var):
        std = torch.exp(log_var/2)
        p = torch.distributions.Normal(torch.zeros_like(mu), torch.ones_like(std))
        q = torch.distributions.Normal(mu, std)
        z = q.rsample()
        return p, q, z

vae_samp = vaeSample().to('cuda')
p, q, z = vae_samp(mu, log_var)
```

```{python}

class vaeExpand(nn.Module):
    def __init__(self, latent_dims, out_dims
                #  , out_shape
                 ):
        super(vaeExpand, self).__init__()
        self.fc_expand = nn.Sequential(nn.Linear(latent_dims, out_dims))

    def forward(self, res):
        res = self.fc_expand(res)
        return(res)


vae_ex = vaeExpand(256, vae_en.output_shape_flat[1]).to('cuda')
res2 = vae_ex(z)
res2.shape
```

```{python}

class ResConv2dDecoder(nn.Module):
    def __init__(
        self,
        enc_intermediate_shapes    
    ):
        super(ResConv2dDecoder, self).__init__()
        self.decoder = nn.ModuleList([
            
            nn.ConvTranspose2d(in_channels=16*17 , out_channels=8*17, kernel_size=(6, 6), stride=(2,2), padding=(2, 2), bias=False),    
            ResBlock(          in_channels=8*17 , out_channels=8*17, kernel_size=3,      stride=1,     padding=1),
            
            nn.ConvTranspose2d(in_channels=8*17 , out_channels=4*17, kernel_size=(6, 6), stride=(2,2), padding=(2, 2), bias=False),
            ResBlock(          in_channels=4*17 , out_channels=4*17, kernel_size=3,      stride=1,     padding=1),
            
            nn.ConvTranspose2d(in_channels=4*17 , out_channels=2*17, kernel_size=(6, 6), stride=(2,2), padding=(2, 2), bias=False),
            ResBlock(          in_channels=2*17 , out_channels=2*17, kernel_size=3,      stride=1,     padding=1),
            
            nn.ConvTranspose2d(in_channels=2*17,  out_channels=17, kernel_size=(6, 6), stride=(2,2), padding=(2, 2), bias=False),
            ])
        
        # overwrite first value with -1
        vals = [list(e) for e in enc_intermediate_shapes.copy()]
        vals = [tuple([-1]+e[1:]) for e in vals]
        # self.input_shape = vals[-1] #.pop()
        self.input_shape = vals.pop()
        vals.reverse()
        self.intermediate_shapes = vals#[1:] # Onve value here is the input 
        # print(self.intermediate_shapes)
 
    def forward(self, x):
        # x = x.reshape((-1, 8, 4, 4))
        x = x.reshape(self.input_shape)
        # print(x.shape)
        for i in range(len(self.decoder)):
            # upsampling only happens in the nn.ConvTranspos2d layers so if we're running a ResBlock, we'll run it without an output_size
            if 'ResBlock' in str(type(self.decoder[i])):
                x = self.decoder[i](x)
            else:    
                # print(self.intermediate_shapes[i])
                x = self.decoder[i](x, output_size = self.intermediate_shapes[i])
            # print('_')
            # print(x.shape)
        return(x)
  
vae_dc = ResConv2dDecoder(enc_intermediate_shapes= vae_en.intermediate_shapes.copy()).to('cuda')
vae_dc(res).shape

              
```

```{python}
latent_dims = 256

vae_en   = ResConv2dEncoder(input_shape = next(iter(exporatory_dataloader_hilb))[0].shape )
vae_rp   = vaeReparam(vae_en.output_shape_flat[1], latent_dims)
vae_samp = vaeSample()
vae_ex   = vaeExpand(latent_dims, vae_en.output_shape_flat[1])
vae_dc   = ResConv2dDecoder(enc_intermediate_shapes= vae_en.intermediate_shapes.copy())

vae_en = vae_en.to('cuda')
vae_rp = vae_rp.to('cuda')
vae_samp = vae_samp.to('cuda')
vae_ex = vae_ex.to('cuda')
vae_dc = vae_dc.to('cuda')
```

```{python}
# max_epoch = 10
logger = TensorBoardLogger("tb_vae_logs", name="hilb")

test_vae = hWthrVAE(
    enc= vae_en,
    rpm= vae_rp,
    smp= vae_samp,
    ex = vae_ex,
    dcd= vae_dc
)#.to('cuda')

optimizer = test_vae.configure_optimizers()

trainer = pl.Trainer(max_epochs=max_epoch, logger=logger)

trainer.fit(model=test_vae, train_dataloaders=exporatory_dataloader_hilb)
```

#### Check Predictions

```{python}
xs = next(iter(exporatory_dataloader_hilb))[0]
xhats = trainer.model(xs).detach().cpu()
xs = xs.detach().cpu()
```

```{python}
px.imshow(xs[0, 8, :, :])
```

```{python}
px.imshow(xhats[0, 8, :, :])
```

#### Visualize Embeddings

```{python}
# embeddings = []

# for i, x in enumerate(exporatory_dataloader_hilb):
#     with torch.no_grad():
#         embeddings += [trainer.model.embed(x[0].to('cpu'))]

# embeddings = torch.concat(embeddings, axis = 0)
```

```{python}
# X_embedded = TSNE(n_components=2, learning_rate='auto', init='random', perplexity=3).fit_transform(embeddings)
```

```{python}
# temp = pd.concat([pd.DataFrame(training_metadata, columns=['Longitude', 'Latitude', 'Year']), 
#                   pd.DataFrame(X_embedded, columns = ['TSNE1', 'TSNE2'])], axis = 1)

# temp['LatLon']=temp['Latitude'].astype(str)+'_'+temp['Longitude'].astype(str)
```

```{python}
# px.scatter(temp, x = 'TSNE1', y = 'TSNE2', color='LatLon')
```


