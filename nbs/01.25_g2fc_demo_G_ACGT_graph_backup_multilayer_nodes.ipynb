{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6356970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacky way to schedule. Here I'm setting these to sleep until the gpus should be free.\n",
    "# At the end of the notebooks  os._exit(00) will kill the kernel freeing the gpu. \n",
    "#                          Hours to wait\n",
    "# import time; time.sleep( 6 * (60*60))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aadccc0",
   "metadata": {},
   "source": [
    "# G only KEGG based network architecture\n",
    "\n",
    "> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc2f33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "pio.templates.default = \"plotly_white\"\n",
    "\n",
    "# import hilbertcurve\n",
    "# from hilbertcurve.hilbertcurve import HilbertCurve\n",
    "\n",
    "from EnvDL.core import * # includes remove_matching_files\n",
    "from EnvDL.dna import *\n",
    "from EnvDL.dlfn import *\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f0d9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "import torchviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4600ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_path = '../nbs_artifacts/01.25_g2fc_demo_G_ACGT_graph_backup_multilayer_nodes/'\n",
    "ensure_dir_path_exists(dir_path = cache_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce01a01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "dataloader_batch_size = 1000\n",
    "# dataloader_batch_size = 8 #16 #64\n",
    "# run_epochs = 200\n",
    "\n",
    "use_gpu_num = 0\n",
    "\n",
    "# Imports --------------------------------------------------------------------\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if use_gpu_num in [0, 1]: \n",
    "    torch.cuda.set_device(use_gpu_num)\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4a4118",
   "metadata": {},
   "source": [
    "## Simple case: Building a Neural Net from an arbitrary graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234fdf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start by finding the top level -- all those keys which are theselves not values\n",
    "# helper function to get all keys and all value from a dict. Useful for when keys don't have unique values.\n",
    "def find_uniq_keys_values(input_dict):\n",
    "    all_keys = list(input_dict.keys())\n",
    "    all_values = []\n",
    "    for e in all_keys:\n",
    "        all_values.extend(input_dict[e])\n",
    "    all_values = list(set(all_values))\n",
    "\n",
    "    return({'all_keys': all_keys,\n",
    "           'all_values': all_values})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1d05ca",
   "metadata": {},
   "source": [
    "### Find order that nodes in the graph should be called to have all dependencies run when they are called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8e6203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the dependancies for run order from many dependancies to none\n",
    "# wrapper function to find the nodes that aren't any other nodes dependancies.\n",
    "def find_top_nodes(all_key_value_dict):\n",
    "    return([e for e in all_key_value_dict['all_keys'] if e not in all_key_value_dict['all_values']])\n",
    "# wrapper function to find the input nodes. They don't occur in the keys and thus won't be added to the list otherwise.\n",
    "# another way to do this would have been to \n",
    "def find_input_nodes(all_key_value_dict):\n",
    "    return([e for e in all_key_value_dict['all_values'] if e not in all_key_value_dict['all_keys']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46aba7b",
   "metadata": {},
   "source": [
    "### Set up output tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f7d429",
   "metadata": {},
   "source": [
    "### Set up dict of input tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57511313",
   "metadata": {},
   "source": [
    "### Figure out expected input/output shapes\n",
    "\n",
    "_==NOTE! This assumes only dense connections!==_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b348c27",
   "metadata": {},
   "source": [
    "### Set up DataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6046e283",
   "metadata": {},
   "source": [
    "Now we have\n",
    "\n",
    "- A dictionary with the connections: `example_dict`\n",
    "- The expected input sizes for each node: `example_dict_input_size`\n",
    "- A dictionary with the input tensors: `input_tensor_dict`\n",
    "- A list of the input tensors' names: `no_dependants` \n",
    "- A list with the order that each module should be called: `dependancy_order`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6568d72",
   "metadata": {},
   "source": [
    "### Set up NeuralNetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfee2aee",
   "metadata": {},
   "source": [
    "## Tiny test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cafd5a",
   "metadata": {},
   "source": [
    "### Tiny `n`  gene version of the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456c3595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vis_dot_bool = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac137c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kegg_connections = {\n",
    "#     'c1':['b1'],\n",
    "#     'c2':['b2'],\n",
    "#     'b1':['a1_input', 'b2'],\n",
    "#     'b2':['a2_input']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fbc05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # add yhat node to the graph\n",
    "# temp_values = []\n",
    "# for key in kegg_connections.keys():\n",
    "#     temp_values += kegg_connections[key]\n",
    "\n",
    "# kegg_connections['y_hat'] = [key for key in kegg_connections.keys() if key not in temp_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369457ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This is too big to render in full\n",
    "# dot = ''\n",
    "# if vis_dot_bool:\n",
    "#     dot = Digraph()\n",
    "#     for key in tqdm(kegg_connections.keys()):\n",
    "#         dot.node(key)\n",
    "#         for value in kegg_connections[key]:\n",
    "#             # edge takes a head/tail whereas edges takes name pairs concatednated (A, B -> AB)in a list\n",
    "#             dot.edge(value, key)    \n",
    "\n",
    "# dot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb31376",
   "metadata": {},
   "source": [
    "Version with the node names masked for size "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea9ab12",
   "metadata": {},
   "source": [
    "### Setup to build the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f86041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # start by finding the top level -- all those keys which are theselves not values\n",
    "# res = find_uniq_keys_values(input_dict = kegg_connections)\n",
    "# all_keys = res['all_keys']\n",
    "# all_values = res['all_values']\n",
    "\n",
    "# # use the keys to find the input/outputs of the graph\n",
    "# output_nodes = [e for e in all_keys if e not in all_values]\n",
    "# input_nodes = [e for e in all_values if e not in all_keys]\n",
    "\n",
    "# # (output_nodes, input_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67719405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # find the dependancies for run order from many dependancies to none\n",
    "# temp = kegg_connections.copy()\n",
    "\n",
    "# no_dependants = find_input_nodes(all_key_value_dict = find_uniq_keys_values(input_dict = temp))\n",
    "# # first pass. Same as the output nodes identified above\n",
    "# dependancy_order = []\n",
    "# # Then iterate\n",
    "# for ith in range(100): #TODO <- this should be set as a input parameter\n",
    "#     top_nodes = find_top_nodes(all_key_value_dict = find_uniq_keys_values(input_dict = temp))\n",
    "#     if top_nodes == []:\n",
    "#         break\n",
    "#     else:\n",
    "#         dependancy_order += top_nodes    \n",
    "#         # remove nodes from the graph that are at the 'top' level and haven't already been removed\n",
    "#         for key in [e for e in dependancy_order if e in temp.keys()]:\n",
    "#              temp.pop(key)\n",
    "\n",
    "# # dependancy_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ad8a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reverse to get the order that the nodes should be called\n",
    "# dependancy_order.reverse()\n",
    "# # dependancy_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36d7167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Trying out new approach: add a node for the input data tha will only flatten the input.\n",
    "# dependancy_order = input_nodes+dependancy_order\n",
    "\n",
    "# for key in input_nodes:\n",
    "#     kegg_connections[key] = [] #[key] # needs to contain itself so the model's `get_input_node()` function works \n",
    "#                                # or that function needs to change.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08572b1f",
   "metadata": {},
   "source": [
    "-### Set up dict of input tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b377b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_true = torch.from_numpy(np.concatenate([\n",
    "#         np.ones((10, )),\n",
    "#         np.zeros((10, ))], 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ea8e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_tensor_dict = {\n",
    "#     'a1_input': torch.from_numpy(np.concatenate([\n",
    "#         np.zeros((10, 4, 3)),\n",
    "#         np.ones((10, 4, 3))], 0)),\n",
    "#     'a2_input': torch.from_numpy(np.concatenate([\n",
    "#         np.zeros((10, 4, 2)),  \n",
    "#         np.ones((10, 4, 2))], 0))}\n",
    "\n",
    "# x_list_temp = [input_tensor_dict[key].to(torch.float) for key in input_tensor_dict.keys()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97ea5ef",
   "metadata": {},
   "source": [
    "-### Figure out expected input/output shapes\n",
    "\n",
    "_==NOTE! This assumes only dense connections!==_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66adb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This could be replaced by a sort of \"distance from output\" measure\n",
    "# default_output_size = 5\n",
    "# output_size_dict = dict(zip(dependancy_order, \n",
    "#                         [default_output_size for i in range(len(dependancy_order))]))\n",
    "# output_size_dict['y_hat'] = 1 \n",
    "# # output_size_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a03fc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # input nodes will only flatten the output so they have length channels*length.\n",
    "# for key in input_nodes:\n",
    "#     output_size_dict[key] = np.prod(np.array(input_tensor_dict[key].shape[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd928e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_size_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd8c0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CHANNEL AWARE VERSION -----------------------------------------------------------------------------------\n",
    "# input_size_dict = kegg_connections.copy()\n",
    "\n",
    "# # use the expected output sizes from `output_size_dict` to fill in the non-data sizes\n",
    "# tensor_ndim = len(input_tensor_dict[list(input_tensor_dict.keys())[0]].shape)\n",
    "# for e in tqdm(input_size_dict.keys()):\n",
    "#     # overwrite named connections with the output size of those connections\n",
    "#     # if the entry is in no_dependants it's data so it's size needs to be grabbed from the input_tensor_dict\n",
    "    \n",
    "#     # is there no channel dim? (major/minor allele)\n",
    "#     if 2 == tensor_ndim:\n",
    "#         input_size_dict[e] = [\n",
    "#             list(input_tensor_dict[ee].shape)[-1] # <- NOTE! THIS ASSUMES ONLY DENSE CONNECTIONS (i.e. only the 1st dim is needed)\n",
    "#             if ee in no_dependants\n",
    "#             else output_size_dict[ee] for ee in input_size_dict[e]]\n",
    "#     elif 3 == tensor_ndim: # There is a channel dim\n",
    "#         input_size_dict[e] = [\n",
    "#             (list(input_tensor_dict[ee].shape)[1]*list(input_tensor_dict[ee].shape)[2]) # <- NOTE! THIS ASSUMES ONLY DENSE CONNECTIONS (i.e. only the 1st dim is needed)  \n",
    "#             if ee in no_dependants\n",
    "#             else output_size_dict[ee] for ee in input_size_dict[e]]\n",
    "\n",
    "# # Now walk over entries and overwrite with the sum of the inputs\n",
    "# for e in tqdm(input_size_dict.keys()):\n",
    "#     input_size_dict[e] = np.sum(input_size_dict[e])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f325685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dot = ''\n",
    "# if vis_dot_bool:\n",
    "#     dot = Digraph()\n",
    "#     for key in tqdm(kegg_connections.keys()):\n",
    "#         key_label = str(key)+'\\nin: '+str(input_size_dict[key])+'\\nout: '+str(output_size_dict[key])\n",
    "#         dot.node(key, key_label)\n",
    "#         for value in kegg_connections[key]:\n",
    "#             # edge takes a head/tail whereas edges takes name pairs concatednated (A, B -> AB)in a list\n",
    "#             dot.edge(value, key)    \n",
    "\n",
    "# dot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bf0c72",
   "metadata": {},
   "source": [
    "### Set up DataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c05d3e",
   "metadata": {},
   "source": [
    "Now we have\n",
    "\n",
    "- A dictionary with the connections: `example_dict`\n",
    "- The expected input sizes for each node: `example_dict_input_size`\n",
    "- A dictionary with the input tensors: `input_tensor_dict`\n",
    "- A list of the input tensors' names: `no_dependants` \n",
    "- A list with the order that each module should be called: `dependancy_order`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99942db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ListDataset(Dataset): # for any G containing matix with many (phno) to one (geno)\n",
    "    def __init__(self, \n",
    "                 y, \n",
    "                 x_list,\n",
    "                 transform = None, target_transform = None,\n",
    "                 **kwargs \n",
    "                ):\n",
    "        self.device = device\n",
    "        self.y = y \n",
    "        self.x_list = x_list\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform    \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        y_idx =self.y[idx]\n",
    "        \n",
    "        x_idx =[x[idx, ] for x in self.x_list] \n",
    "\n",
    "        \n",
    "        if self.target_transform:\n",
    "            y_idx = self.transform(y_idx)\n",
    "            x_idx = [self.transform(x) for x in x_idx]\n",
    "            \n",
    "        return y_idx, x_idx\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21eb6b9f",
   "metadata": {},
   "source": [
    "To have a fair test of whether the model is working, I want to ensure there is information to learn in the dataset. To this end I'm using just two genotypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d54d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_dataloader = DataLoader(ListDataset(\n",
    "#         y = y_true[:, None].to(torch.float32),\n",
    "#         x_list = [e.to(torch.float32) for e in x_list_temp]\n",
    "#     ),\n",
    "#     batch_size = 2,\n",
    "#     shuffle = True\n",
    "# )\n",
    "\n",
    "# # next(iter(training_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3456a9",
   "metadata": {},
   "source": [
    "### Set up NeuralNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d7717f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f74118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working version ====\n",
    "# Doesn't pass output node through relu\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, \n",
    "                 example_dict, # contains the node (excluding input tensors)\n",
    "                 example_dict_input_size, # contains the input sizes (including the tensors)\n",
    "                 example_dict_output_size,\n",
    "                 input_tensor_names,\n",
    "                 dependancy_order\n",
    "                ):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        def Linear_block(in_size, out_size, drop_pr):\n",
    "            block = nn.Sequential(\n",
    "                nn.Linear(in_size, out_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(drop_pr))\n",
    "            return(block)   \n",
    "        \n",
    "        \n",
    "        # fill in the list in dependancy order. \n",
    "        layer_list = []\n",
    "        for key in dependancy_order:\n",
    "            if key in input_tensor_names:\n",
    "                layer_list += [\n",
    "                    nn.Flatten()\n",
    "                ]\n",
    "            elif key != 'y_hat':\n",
    "                layer_list += [\n",
    "                    Linear_block(in_size=example_dict_input_size[key], \n",
    "                                 out_size=example_dict_output_size[key], \n",
    "                                 drop_pr=0)\n",
    "                              ]\n",
    "            else:\n",
    "                layer_list += [\n",
    "                    nn.Linear(example_dict_input_size[key], \n",
    "                              example_dict_output_size[key])\n",
    "                              ]\n",
    "                \n",
    "\n",
    "        self.nn_layer_list = nn.ModuleList(layer_list)\n",
    "\n",
    "        # things for get_input_node in forward to work.\n",
    "        self.example_dict = example_dict\n",
    "        self.input_tensor_names = input_tensor_names\n",
    "        self.dependancy_order = dependancy_order\n",
    "        \n",
    "        self.input_tensor_lookup = dict(zip(input_tensor_names, \n",
    "                                            [i for i in range(len(input_tensor_names))]))\n",
    "        self.result_list = []\n",
    "        self.result_list_lookup = {}\n",
    "            \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Note: x will be a list. input_tensor_lookup will contain the name: list index pairs.\n",
    "        # I use a dict instead of a list comprehension here because there could be an arbitrarily\n",
    "        # large number of inputs in the list. \n",
    "        def get_input_node(self, input_node, get_x):  \n",
    "#             print(input_node, self.result_list_lookup)\n",
    "            return(self.result_list[self.result_list_lookup[input_node]])\n",
    "        \n",
    "        # trying reinstantiating to get around inplace replacement issue.\n",
    "        self.result_list = []\n",
    "        self.result_list_lookup = {}\n",
    "        for key in self.dependancy_order:\n",
    "            input_nodes = self.example_dict[key]\n",
    "            nn_layer_list_idx = [i for i in range(len(dependancy_order)) if dependancy_order[i]==key][0]\n",
    "            \n",
    "            self.result_list_lookup[key] = len(self.result_list_lookup)                \n",
    "            if key in self.input_tensor_names: # If the input node is an input (flatten) layer\n",
    "                self.result_list = self.result_list + [self.nn_layer_list[nn_layer_list_idx](\n",
    "                    x[self.input_tensor_lookup[key]]\n",
    "                ).clone()]\n",
    "\n",
    "            else:\n",
    "                self.result_list = self.result_list + [self.nn_layer_list[nn_layer_list_idx](torch.concat(\n",
    "                    [get_input_node(self, input_node = e, get_x = x) for e in input_nodes], \n",
    "                    -1)).clone()]\n",
    "\n",
    "        return self.result_list[self.result_list_lookup['y_hat']]\n",
    "    \n",
    "\n",
    "# model = NeuralNetwork(example_dict = kegg_connections, \n",
    "#                       example_dict_input_size = input_size_dict,\n",
    "#                       example_dict_output_size = output_size_dict,\n",
    "#                       input_tensor_names = list(input_tensor_dict.keys()),\n",
    "#                       dependancy_order = dependancy_order) \n",
    "\n",
    "# model(next(iter(training_dataloader))[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c768ef0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model = model.to(device)\n",
    "# model(next(iter(training_dataloader))[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0c7eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the only \n",
    "def train_loop_yx(dataloader, model, loss_fn, optimizer, silent = False):\n",
    "#     import numpy as np\n",
    "#     import pandas as pd\n",
    "    import torch\n",
    "    from torch.utils.data import Dataset\n",
    "    from torch.utils.data import DataLoader\n",
    "#     from torch import nn\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (y_i, xs_i) in enumerate(dataloader):\n",
    "#         print(batch)\n",
    "        # Compute prediction and loss\n",
    "        pred = model(xs_i)\n",
    "                \n",
    "        loss = loss_fn(pred, y_i) # <----------------------------------------\n",
    "\n",
    "        # Backpropagation\n",
    "        torch.autograd.set_detect_anomaly(True)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(y_i) # <----------------\n",
    "            if not silent:\n",
    "                print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d3a462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loop_yx(dataloader = training_dataloader, \n",
    "#               model = model, \n",
    "#               loss_fn = nn.MSELoss(), \n",
    "#               optimizer = torch.optim.SGD(model.parameters(), lr=0.003),\n",
    "#               silent = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800bf8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564d056b",
   "metadata": {},
   "source": [
    "### Train loop functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6b8970",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_error_yx(dataloader, model, loss_fn, silent = False):\n",
    "#     import numpy as np\n",
    "#     import pandas as pd\n",
    "    import torch\n",
    "    from torch.utils.data import Dataset\n",
    "    from torch.utils.data import DataLoader\n",
    "#     from torch import nn\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    train_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for y_i, xs_i in dataloader:\n",
    "            pred = model(xs_i)\n",
    "            \n",
    "            # ensure both are on cuda\n",
    "#             if pred.device.type == 'cpu':\n",
    "#                 pred = pred.to('cuda')\n",
    "#             if y_i.device.type == 'cpu':\n",
    "#                 y_i = y_i.to('cuda')\n",
    "            \n",
    "            train_loss += loss_fn(pred, y_i).item() # <----------------------\n",
    "            \n",
    "    train_loss /= num_batches\n",
    "    return(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1030ef57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop_yx(dataloader, model, loss_fn, silent = False):\n",
    "#     import numpy as np\n",
    "#     import pandas as pd\n",
    "    import torch\n",
    "    from torch.utils.data import Dataset\n",
    "    from torch.utils.data import DataLoader\n",
    "#     from torch import nn\n",
    "\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for y_i, xs_i in dataloader:\n",
    "            pred = model(xs_i)\n",
    "            \n",
    "            # ensure both are on cuda\n",
    "#             if pred.device.type == 'cpu':\n",
    "#                 pred = pred.to('cuda')\n",
    "#             if y_i.device.type == 'cpu':\n",
    "#                 y_i = y_i.to('cuda')\n",
    "                \n",
    "            test_loss += loss_fn(pred, y_i).item() # <-----------------------\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    if not silent:\n",
    "        print(f\"Test Error: Avg loss: {test_loss:>8f}\")\n",
    "    return(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2158d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn_yx(\n",
    "    cache_path,\n",
    "    training_dataloader,\n",
    "    testing_dataloader,\n",
    "    model,\n",
    "    learning_rate = 1e-3,\n",
    "    batch_size = 64,\n",
    "    epochs = 500,\n",
    "    model_prefix = 'model',\n",
    "    save_pt = False\n",
    "):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import torch\n",
    "#     from torch.utils.data import Dataset\n",
    "#     from torch.utils.data import DataLoader\n",
    "    from torch import nn\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    # Initialize the loss function\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    loss_df = pd.DataFrame([i for i in range(epochs)], columns = ['Epoch'])\n",
    "    loss_df['TrainMSE'] = np.nan\n",
    "    loss_df['TestMSE']  = np.nan\n",
    "\n",
    "    for t in tqdm(range(epochs)):        \n",
    "#         print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train_loop_yx(training_dataloader, model, loss_fn, optimizer, silent = True)\n",
    "\n",
    "        loss_df.loc[loss_df.index == t, 'TrainMSE'\n",
    "                   ] = train_error_yx(training_dataloader, model, loss_fn, silent = True)\n",
    "        \n",
    "        loss_df.loc[loss_df.index == t, 'TestMSE'\n",
    "                   ] = test_loop_yx(testing_dataloader, model, loss_fn, silent = True)\n",
    "        \n",
    "        if (t+1)%5 == 0: # Cache in case training is interupted. \n",
    "            # print(loss_df.loc[loss_df.index == t, ['TrainMSE', 'TestMSE']])\n",
    "            if save_pt:\n",
    "                torch.save(model.state_dict(), \n",
    "                           cache_path+'/'+model_prefix+'_'+str(t)+'_'+str(epochs)+'.pt') # convention is to use .pt or .pth\n",
    "        \n",
    "    return([model, loss_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdf2e9f",
   "metadata": {},
   "source": [
    "### Fit Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d786a3f1",
   "metadata": {},
   "source": [
    "# End"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d03773",
   "metadata": {},
   "source": [
    "## KEGG graph "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547c1e1f",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6b569e",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_from = '../nbs_artifacts/01.03_g2fc_prep_matrices/'\n",
    "# phno = pd.read_csv(load_from+'phno.csv')\n",
    "phno_geno = pd.read_csv(load_from+'phno_geno.csv')\n",
    "phno = phno_geno\n",
    "\n",
    "obs_geno_lookup = np.load(load_from+'obs_geno_lookup.npy') # Phno_Idx\tGeno_Idx\tIs_Phno_Idx\n",
    "YMat = np.load(load_from+'YMat.npy')\n",
    "# GMat = np.load(load_from+'GMat.npy')\n",
    "# ACGT_OneHot = np.load(load_from+'ACGT_OneHot.npy')\n",
    "# ACGT = np.load(load_from+'ACGT.npy')\n",
    "# ACGT_hilb = np.load(load_from+'ACGT_hilb.npy')\n",
    "# SMat = np.load(load_from+'SMat3.npy')\n",
    "# WMat = np.load(load_from+'WMat3.npy')\n",
    "# MMat = np.load(load_from+'MMat3.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4299f2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle as pkl\n",
    "# with open('../data/zma/kegg/kegg_gene_entries.pkl', 'rb') as handle:\n",
    "#     parsed_kegg_gene_entries = pkl.load(handle)\n",
    "\n",
    "# Testing using the cleaned up version in '../nbs_artifacts/01.05_g2fc_demo_model/'\n",
    "load_from = '../nbs_artifacts/01.05_g2fc_demo_model/'\n",
    "parsed_kegg_gene_entries = get_cached_result(load_from+'filtered_kegg_gene_entries.pkl')\n",
    "\n",
    "ACGT_gene_slice_list = get_cached_result(load_from+'ACGT_gene_slice_list.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc4d1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(ACGT_gene_slice_list) == len(parsed_kegg_gene_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8edc4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restrict to only those with pathway\n",
    "kegg_gene_brite = [e for e in parsed_kegg_gene_entries if 'BRITE' in e.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb791b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# also require to have a non-empty path\n",
    "kegg_gene_brite = [e for e in kegg_gene_brite if not e['BRITE']['BRITE_PATHS'] == []]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9f3eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retaining 43.53%, 6067/13939 Entries\n"
     ]
    }
   ],
   "source": [
    "print('Retaining '+ str(round(len(kegg_gene_brite)/len(parsed_kegg_gene_entries), 4)*100)+'%, '+str(len(kegg_gene_brite)\n",
    "     )+'/'+str(len(parsed_kegg_gene_entries)\n",
    "     )+' Entries'\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d365781b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['KEGG Orthology (KO) [BR:zma00001]',\n",
       "  '09100 Metabolism',\n",
       "  '09102 Energy metabolism',\n",
       "  '00190 Oxidative phosphorylation',\n",
       "  '100383860'],\n",
       " ['Enzymes [BR:zma01000]',\n",
       "  '7. Translocases',\n",
       "  '7.1  Catalysing the translocation of hydrons',\n",
       "  '7.1.2  Linked to the hydrolysis of a nucleoside triphosphate',\n",
       "  '7.1.2.1  P-type H+-exporting transporter',\n",
       "  '100383860']]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kegg_gene_brite[1]['BRITE']['BRITE_PATHS']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bea378",
   "metadata": {},
   "source": [
    "### Tiny `n`  gene version of the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b38dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 6067/6067 genes.\n"
     ]
    }
   ],
   "source": [
    "n_genes = 200\n",
    "n_genes = 6067 \n",
    "\n",
    "print('Using '+str(n_genes)+'/'+str(len(kegg_gene_brite))+' genes.')\n",
    "\n",
    "# if n_genes is too big, don't visualize.\n",
    "vis_dot_bool = True\n",
    "if n_genes > 10:\n",
    "    vis_dot_bool = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c825d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# The goal here is to have a dict with each node and a list of it's children. \n",
    "# For example, the graph\n",
    "# a--b--d\n",
    "#  |-c--e\n",
    "# Would be parsed into     \n",
    "# {'a':['b', 'c'],\n",
    "#  'b':['d'],\n",
    "#  'c':['e']}\n",
    "# \"\"\"\n",
    "# kegg_connections = {}\n",
    "\n",
    "# # for all genes in list\n",
    "# for i in tqdm(range(n_genes)): # <------------------------- only using 2 inputs genes\n",
    "# # for i in tqdm(range(len(kegg_gene_brite))):    \n",
    "#     temp = kegg_gene_brite[i]['BRITE']['BRITE_PATHS']\n",
    "#     # clean up to make sure that there are no \":\" characters. These can mess up graphviz\n",
    "#     temp = [[temp[j][i].replace(':', '-') for i in range(len(temp[j])) ] for j in range(len(temp))]\n",
    "#     # all paths through graph associated with a gene\n",
    "#     for j in range(len(temp)):\n",
    "#         # steps of the path through the graph\n",
    "#         for k in range(len(temp[j])-1):\n",
    "            \n",
    "            \n",
    "            \n",
    "#             # if this is a new key, add it and add the k+1 entry as it's child\n",
    "#             if temp[j][k] not in kegg_connections.keys():\n",
    "#                 kegg_connections[temp[j][k]] = [temp[j][k+1]]\n",
    "#             else: \n",
    "#                 # Check to see if there's a new child to add   \n",
    "#                 if temp[j][k+1] not in kegg_connections[temp[j][k]]:\n",
    "#                     # make sure that no key contains itself. This was a problem for 'Others' which is now disallowed.\n",
    "#                     if (temp[j][k] != temp[j][k+1]):\n",
    "# #                         if ((temp[j][k] != temp[j][k+1]) & (temp[j][k+1] != 'Others')):\n",
    "#                         # add it.\n",
    "#                         kegg_connections[temp[j][k]].extend([temp[j][k+1]])\n",
    "\n",
    "# # kegg_connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc8ce75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 6067/6067 [00:00<00:00, 50273.11it/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The goal here is to have a dict with each node and a list of it's children. \n",
    "For example, the graph\n",
    "a--b--d\n",
    " |-c--e\n",
    "Would be parsed into     \n",
    "{'a':['b', 'c'],\n",
    " 'b':['d'],\n",
    " 'c':['e']}\n",
    "\"\"\"\n",
    "kegg_connections = {}\n",
    "\n",
    "# for all genes in list\n",
    "for i in tqdm(range(n_genes)): # <------------------------- only using 2 inputs genes\n",
    "# for i in tqdm(range(len(kegg_gene_brite))):    \n",
    "    temp = kegg_gene_brite[i]['BRITE']['BRITE_PATHS']\n",
    "    # clean up to make sure that there are no \":\" characters. These can mess up graphviz\n",
    "    temp = [[temp[j][i].replace(':', '-') for i in range(len(temp[j])) ] for j in range(len(temp))]\n",
    "    # all paths through graph associated with a gene\n",
    "    for j in range(len(temp)):\n",
    "        # steps of the path through the graph\n",
    "        for k in range(len(temp[j])-1):\n",
    "            \n",
    "            # name standardization \n",
    "            temp_jk  = temp[j][k]\n",
    "            temp_jk1 = temp[j][k+1]\n",
    "            temp_jk  = temp_jk.lower().title().replace(' ', '')\n",
    "            temp_jk1 = temp_jk1.lower().title().replace(' ', '')\n",
    "            \n",
    "            # if this is a new key, add it and add the k+1 entry as it's child\n",
    "            if temp_jk  not in kegg_connections.keys():\n",
    "                kegg_connections[temp_jk] = [temp_jk1]\n",
    "            else: \n",
    "                # Check to see if there's a new child to add   \n",
    "                if temp_jk1 not in kegg_connections[temp_jk]:\n",
    "                    # make sure that no key contains itself. This was a problem for 'Others' which is now disallowed.\n",
    "                    if (temp_jk != temp_jk1):\n",
    "#                         if ((temp_jk  != temp_jk1) & (temp_jk1 != 'Others')):\n",
    "                        # add it.\n",
    "                        kegg_connections[temp_jk].extend([temp_jk1])\n",
    "\n",
    "# kegg_connections                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1756038",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc93bad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c110e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed node \"Others\"\n"
     ]
    }
   ],
   "source": [
    "if 'Others' in kegg_connections.keys():\n",
    "    del kegg_connections['Others']\n",
    "    print('Removed node \"Others\"')\n",
    "\n",
    "# remove 'Others' as a possible value\n",
    "for key in kegg_connections.keys():\n",
    "    kegg_connections[key] = [e for e in kegg_connections[key] if e != 'Others']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1c1f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that no list contains it's own key\n",
    "for key in kegg_connections.keys():\n",
    "    kegg_connections[key] = [e for e in kegg_connections[key] if e != key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66ec6db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GProtein-CoupledReceptors[Br-Zma04030]']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [key for key in input_size_dict.keys() if type(input_size_dict[key]) != np.int64]\n",
    "# input_size_dict['Other DSBR factors']\n",
    "\n",
    "# kegg_connections['Other DSBR factors']\n",
    "\n",
    "# start with those that are not connected to any SNPs. Remove them.\n",
    "\n",
    "# rm_list = [key for key in kegg_connections.keys() if kegg_connections[key]==[]]\n",
    "\n",
    "# there might be associations with no dependants and with no dependants except those that have no dependants.\n",
    "# Build up a list with those keys that don't connect back to snps then I'll pass over the connection dict once to remove references to them.\n",
    "rm_list = []\n",
    "rm_list_i = len(rm_list)\n",
    "rm_list_j = -1\n",
    "for i in range(100):\n",
    "    if rm_list_i == rm_list_j:\n",
    "        break\n",
    "    else:\n",
    "        rm_list = [key for key in kegg_connections.keys() if [e for e in kegg_connections[key] if e not in rm_list]\n",
    "     ==[]]\n",
    "        rm_list_j = rm_list_i \n",
    "        rm_list_i = len(rm_list)\n",
    "rm_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057b2e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in rm_list:\n",
    "    del kegg_connections[key]\n",
    "    \n",
    "for key in kegg_connections.keys():\n",
    "    kegg_connections[key] = [e for e in kegg_connections[key] if e not in rm_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c8cc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add yhat node to the graph\n",
    "temp_values = []\n",
    "for key in kegg_connections.keys():\n",
    "    temp_values += kegg_connections[key]\n",
    "\n",
    "kegg_connections['y_hat'] = [key for key in kegg_connections.keys() if key not in temp_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597288ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is too big to render in full\n",
    "dot = ''\n",
    "if vis_dot_bool:\n",
    "    dot = Digraph()\n",
    "    for key in tqdm(kegg_connections.keys()):\n",
    "        dot.node(key)\n",
    "        for value in kegg_connections[key]:\n",
    "            # edge takes a head/tail whereas edges takes name pairs concatednated (A, B -> AB)in a list\n",
    "            dot.edge(value, key)    \n",
    "\n",
    "dot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1e6077",
   "metadata": {},
   "source": [
    "Version with the node names masked for size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ddd86c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot = ''\n",
    "if vis_dot_bool:\n",
    "    name_to_num_dict = dict(zip(list(kegg_connections.keys()),\n",
    "                                [str(i) for i in range(len(list(kegg_connections.keys())))]))\n",
    "\n",
    "    temp = {}\n",
    "    for key in kegg_connections.keys():\n",
    "        temp[name_to_num_dict[key]] = [name_to_num_dict[e] if e in name_to_num_dict.keys() else e for e in kegg_connections[key]]\n",
    "\n",
    "    dot = Digraph()\n",
    "    for key in tqdm(temp.keys()):\n",
    "        dot.node(key)\n",
    "        for value in temp[key]:\n",
    "            # edge takes a head/tail whereas edges takes name pairs concatednated (A, B -> AB)in a list\n",
    "            dot.edge(value, key)    \n",
    "\n",
    "    # dot.render(directory=cache_path, view=True) \n",
    "dot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cb7324",
   "metadata": {},
   "source": [
    "### Setup to build the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d96f8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start by finding the top level -- all those keys which are theselves not values\n",
    "res = find_uniq_keys_values(input_dict = kegg_connections)\n",
    "all_keys = res['all_keys']\n",
    "all_values = res['all_values']\n",
    "\n",
    "# use the keys to find the input/outputs of the graph\n",
    "output_nodes = [e for e in all_keys if e not in all_values]\n",
    "input_nodes = [e for e in all_values if e not in all_keys]\n",
    "\n",
    "# (output_nodes, input_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc6c050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the dependancies for run order from many dependancies to none\n",
    "temp = kegg_connections.copy()\n",
    "\n",
    "no_dependants = find_input_nodes(all_key_value_dict = find_uniq_keys_values(input_dict = temp))\n",
    "# first pass. Same as the output nodes identified above\n",
    "dependancy_order = []\n",
    "# Then iterate\n",
    "for ith in range(100): #TODO <- this should be set as a input parameter\n",
    "    top_nodes = find_top_nodes(all_key_value_dict = find_uniq_keys_values(input_dict = temp))\n",
    "    if top_nodes == []:\n",
    "        break\n",
    "    else:\n",
    "        dependancy_order += top_nodes    \n",
    "        # remove nodes from the graph that are at the 'top' level and haven't already been removed\n",
    "        for key in [e for e in dependancy_order if e in temp.keys()]:\n",
    "             temp.pop(key)\n",
    "\n",
    "# dependancy_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8251172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse to get the order that the nodes should be called\n",
    "dependancy_order.reverse()\n",
    "# dependancy_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272f38f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying out new approach: add a node for the input data tha will only flatten the input.\n",
    "dependancy_order = input_nodes+dependancy_order\n",
    "\n",
    "for key in input_nodes:\n",
    "    kegg_connections[key] = [] #[key] # needs to contain itself so the model's `get_input_node()` function works \n",
    "                               # or that function needs to change."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8001e949",
   "metadata": {},
   "source": [
    "-### Set up dict of input tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53901bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 6067/6067 [00:00<00:00, 963749.52it/s]\n"
     ]
    }
   ],
   "source": [
    "# build a dict to go from the node names in `no_dependants` to the list index in `ACGT_gene_slice_list`\n",
    "brite_node_to_list_idx_dict = {}\n",
    "for i in tqdm(range(len(kegg_gene_brite))):\n",
    "    brite_node_to_list_idx_dict[str(kegg_gene_brite[i]['BRITE']['BRITE_PATHS'][0][-1])] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a00fe4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e209df06",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor_dict = {}\n",
    "for e in no_dependants:\n",
    "    input_tensor_dict[e] = ACGT_gene_slice_list[brite_node_to_list_idx_dict[e]]\n",
    "    \n",
    "# input_tensor_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7b3a3a",
   "metadata": {},
   "source": [
    "-### Figure out expected input/output shapes\n",
    "\n",
    "_==NOTE! This assumes only dense connections!==_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca643b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This could be replaced by a sort of \"distance from output\" measure\n",
    "default_output_size = 50\n",
    "output_size_dict = dict(zip(dependancy_order, \n",
    "                        [default_output_size for i in range(len(dependancy_order))]))\n",
    "output_size_dict['y_hat'] = 1 \n",
    "# output_size_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4703e891",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 8868/8868 [00:00<00:00, 10533.89it/s]\n",
      "100%|███████████████████████████████████| 8868/8868 [00:00<00:00, 335974.71it/s]\n"
     ]
    }
   ],
   "source": [
    "# CHANNEL AWARE VERSION -----------------------------------------------------------------------------------\n",
    "input_size_dict = kegg_connections.copy()\n",
    "\n",
    "# use the expected output sizes from `output_size_dict` to fill in the non-data sizes\n",
    "tensor_ndim = len(input_tensor_dict[list(input_tensor_dict.keys())[0]].shape)\n",
    "for e in tqdm(input_size_dict.keys()):\n",
    "    # overwrite named connections with the output size of those connections\n",
    "    # if the entry is in no_dependants it's data so it's size needs to be grabbed from the input_tensor_dict\n",
    "    \n",
    "    # is there no channel dim? (major/minor allele)\n",
    "    if 2 == tensor_ndim:\n",
    "        input_size_dict[e] = [\n",
    "            list(input_tensor_dict[ee].shape)[-1] # <- NOTE! THIS ASSUMES ONLY DENSE CONNECTIONS (i.e. only the 1st dim is needed)\n",
    "            if ee in no_dependants\n",
    "            else output_size_dict[ee] for ee in input_size_dict[e]]\n",
    "    elif 3 == tensor_ndim: # There is a channel dim\n",
    "        input_size_dict[e] = [\n",
    "            (list(input_tensor_dict[ee].shape)[1]*list(input_tensor_dict[ee].shape)[2]) # <- NOTE! THIS ASSUMES ONLY DENSE CONNECTIONS (i.e. only the 1st dim is needed)  \n",
    "            if ee in no_dependants\n",
    "            else output_size_dict[ee] for ee in input_size_dict[e]]\n",
    "\n",
    "# Now walk over entries and overwrite with the sum of the inputs\n",
    "for e in tqdm(input_size_dict.keys()):\n",
    "    input_size_dict[e] = np.sum(input_size_dict[e])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d9fb35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot = ''\n",
    "if vis_dot_bool:\n",
    "    dot = Digraph()\n",
    "    for key in tqdm(kegg_connections.keys()):\n",
    "        key_label = 'in: '+str(input_size_dict[key])+'\\nout: '+str(output_size_dict[key])\n",
    "        dot.node(key, key_label)\n",
    "        for value in kegg_connections[key]:\n",
    "            # edge takes a head/tail whereas edges takes name pairs concatednated (A, B -> AB)in a list\n",
    "            dot.edge(value, key)    \n",
    "\n",
    "dot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9ebaea",
   "metadata": {},
   "source": [
    "### Set up DataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a550a369",
   "metadata": {},
   "source": [
    "Now we have\n",
    "\n",
    "- A dictionary with the connections: `example_dict`\n",
    "- The expected input sizes for each node: `example_dict_input_size`\n",
    "- A dictionary with the input tensors: `input_tensor_dict`\n",
    "- A list of the input tensors' names: `no_dependants` \n",
    "- A list with the order that each module should be called: `dependancy_order`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75500055",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ListDataset(Dataset): # for any G containing matix with many (phno) to one (geno)\n",
    "    def __init__(self, \n",
    "                 y, \n",
    "                 x_list,\n",
    "                 obs_idxs, # this is a list of the indexes used. It allows us to pass in smaller \n",
    "                           # tensors and then get the right genotype\n",
    "                 obs_geno_lookup,\n",
    "                 transform = None, target_transform = None,\n",
    "                 use_gpu_num = 0,\n",
    "                 device = 'cuda',\n",
    "                 **kwargs \n",
    "                ):\n",
    "        self.device = device\n",
    "        self.y = y \n",
    "        self.x_list = x_list\n",
    "        self.obs_idxs = obs_idxs\n",
    "        self.obs_geno_lookup = obs_geno_lookup\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform    \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        y_idx =self.y[idx]\n",
    "        \n",
    "        idx_geno = obs_geno_lookup[self.obs_idxs[idx], 1]\n",
    "#         print(idx)\n",
    "#         print(self.obs_idxs[idx])\n",
    "#         print(obs_geno_lookup[self.obs_idxs[idx], 1])\n",
    "# #         print([x.shape for x in self.x_list[0]])\n",
    "#         print([e.shape for e in self.x_list])\n",
    "#         print()\n",
    "#         x_idx =[x[idx_geno, ] for x in self.x_list[0]] # must be the 0th entry because it's in a tuple\n",
    "        x_idx =[x[idx_geno, ] for x in self.x_list] \n",
    "        \n",
    "        if self.device == 'cuda':\n",
    "            y_idx.to(self.device)\n",
    "            x_idx = [x.to(self.device) for x in x_idx]\n",
    "\n",
    "        if self.target_transform:\n",
    "            y_idx = self.transform(y_idx)\n",
    "            x_idx = [self.transform(x) for x in x_idx]\n",
    "            \n",
    "        return y_idx, x_idx\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e4e20e",
   "metadata": {},
   "source": [
    "To have a fair test of whether the model is working, I want to ensure there is information to learn in the dataset. To this end I'm using just two genotypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11f5b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # These are the genotypes with the most observations. \n",
    "# yield_xbar_summary = pd.DataFrame(zip(\n",
    "#     [i for i in range(4898)],\n",
    "#     [len(obs_geno_lookup[obs_geno_lookup[:, 1] == i, 0]) for i in range(4898)],\n",
    "#     [np.mean(YMat[obs_geno_lookup[obs_geno_lookup[:, 1] == i, 0]]) for i in range(4898)]\n",
    "# )).sort_values(2)\n",
    "# yield_xbar_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587a364f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # limit by min number of observations\n",
    "# yield_xbar_summary = yield_xbar_summary.loc[(yield_xbar_summary[1] > 300), ].reset_index(drop = True)\n",
    "# yield_xbar_summary = yield_xbar_summary.loc[(\n",
    "#     (yield_xbar_summary.index == 0) | \n",
    "#     (yield_xbar_summary.index == np.max(yield_xbar_summary.index))), ].reset_index(drop = True)\n",
    "# yield_xbar_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c251b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# small_yield_genotype_idxs = list(obs_geno_lookup[obs_geno_lookup[:, 1] == yield_xbar_summary[0][0], 0])\n",
    "# large_yield_genotype_idxs = list(obs_geno_lookup[obs_geno_lookup[:, 1] == yield_xbar_summary[1][0], 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967c5272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # use genotypes with the biggest difference between them (provided both groups have over a 300 observations)\n",
    "# idxs_two_genotypes = small_yield_genotype_idxs+large_yield_genotype_idxs\n",
    "\n",
    "# np.random.shuffle(idxs_two_genotypes)\n",
    "\n",
    "# test_idx = idxs_two_genotypes[0:round(len(idxs_two_genotypes)*.1)]\n",
    "# train_idx = idxs_two_genotypes[round(len(idxs_two_genotypes)*.1):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69804696",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create train/test validate indicies from json\n",
    "load_from = '../nbs_artifacts/01.06_g2fc_cluster_genotypes/'\n",
    "\n",
    "def read_json(json_path):\n",
    "    with open(json_path, 'r') as fp:\n",
    "        dat = json.load(fp)\n",
    "    return(dat)\n",
    "\n",
    "def read_split_info(\n",
    "    load_from = '../nbs_artifacts/01.06_g2fc_cluster_genotypes/',\n",
    "    json_prefix = '2023:9:5:12:8:26'):\n",
    "    jsons = [e for e in os.listdir(load_from) if re.match('^'+json_prefix+'.+\\.json$', e)]\n",
    "    vals = [e for e in jsons if re.match('.+val\\d+\\.json$', e)]\n",
    "    vals.sort()\n",
    "    out = {}\n",
    "    out['test'] = [read_json(json_path = load_from+json_prefix+'-test.json')]\n",
    "    out['test_file'] = [json_prefix+'-test.json']\n",
    "    out['validate'] = [read_json(json_path = load_from+val) for val in vals]\n",
    "    out['validate_files'] = [val for val in vals]\n",
    "    return(out)\n",
    "\n",
    "\n",
    "def find_idxs_split_dict(\n",
    "    obs_df, # assumes presence of Year, Female, Male\n",
    "    split_dict # from read_split_info() output. Should be a test of validate dict.\n",
    "):\n",
    "\n",
    "    temp = obs_df\n",
    "    test_mask = ((temp.Year.isin(split_dict['test_years'])) & \n",
    "                 ((temp.Female.isin(split_dict['test_parents'])) |\n",
    "                  (temp.Male.isin(split_dict['test_parents']))))\n",
    "    temp['Split'] = ''\n",
    "    temp.loc[test_mask, 'Split'] = 'Test'\n",
    "\n",
    "    train_mask = (~(temp.Year.isin(split_dict['test_years'])) & \n",
    "                 (~((temp.Female.isin(split_dict['test_parents'])) |\n",
    "                  (temp.Male.isin(split_dict['test_parents'])))))\n",
    "    temp.loc[train_mask, 'Split'] = 'Train'\n",
    "\n",
    "    temp_test  = (temp.Split == 'Test') # should be the same as with the mask above\n",
    "    temp_train = (temp.Split == 'Train') # should be the same as with the mask above\n",
    "\n",
    "    # Confirm that there's no overlap in parents or years\n",
    "    temp_test_parents  = set(temp.loc[temp_test, 'Female']+temp.loc[temp_test, 'Male'])\n",
    "    temp_train_parents = set(temp.loc[temp_train, 'Female']+temp.loc[temp_train, 'Male'])\n",
    "\n",
    "    temp_test_years  = set(temp.loc[temp_test, 'Year'])\n",
    "    temp_train_years = set(temp.loc[temp_train, 'Year'])\n",
    "\n",
    "    assert [] == [e for e in temp_test_parents if e in temp_train_parents]\n",
    "    assert [] == [e for e in temp_train_parents if e in temp_test_parents]\n",
    "    assert [] == [e for e in temp_test_years if e in temp_train_years]\n",
    "    assert [] == [e for e in temp_train_years if e in temp_test_years]\n",
    "\n",
    "    return({\n",
    "        'test_idx': temp.loc[test_mask, ].index, \n",
    "        'train_idx': temp.loc[train_mask, ].index} )\n",
    "\n",
    "split_info = read_split_info(\n",
    "    load_from = '../nbs_artifacts/01.06_g2fc_cluster_genotypes/',\n",
    "    json_prefix = '2023:9:5:12:8:26')\n",
    "\n",
    "temp = phno.copy()\n",
    "temp[['Female', 'Male']] = temp['Hybrid'].str.split('/', expand = True)\n",
    "\n",
    "test_dict = find_idxs_split_dict(\n",
    "    obs_df = temp, \n",
    "    split_dict = split_info['test'][0]\n",
    ")\n",
    "# test_dict\n",
    "\n",
    "# since this is applying predefined model structure no need for validation.\n",
    "# This is included for my future reference when validation is needed.\n",
    "temp = temp.loc[test_dict['train_idx'], ] # restrict before re-aplying\n",
    "\n",
    "val_dict = find_idxs_split_dict(\n",
    "    obs_df = temp, \n",
    "    split_dict = split_info['validate'][0]\n",
    ")\n",
    "# val_dict\n",
    "\n",
    "test_dict\n",
    "\n",
    "train_idx = test_dict['train_idx']\n",
    "test_idx  = test_dict['test_idx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529581bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_list_temp = [torch.from_numpy(input_tensor_dict[key]).to(torch.float) for key in input_tensor_dict.keys()]\n",
    "[e.shape for e in x_list_temp]\n",
    "y_temp = torch.from_numpy(YMat).to(torch.float)#[:, None]\n",
    "# FIXME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e82908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4926, 4, 13])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_list_temp[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a923c202",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f5c6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataloader = DataLoader(ListDataset(\n",
    "        y = y_temp[train_idx][:, None],\n",
    "        x_list = [e for e in x_list_temp],\n",
    "        obs_idxs = train_idx, \n",
    "        obs_geno_lookup = obs_geno_lookup, \n",
    "        use_gpu_num = 0,\n",
    "        device = 'cuda',\n",
    "    ),\n",
    "    batch_size = dataloader_batch_size,\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "testing_dataloader = DataLoader(ListDataset(\n",
    "        y = y_temp[test_idx][:, None],\n",
    "        x_list = [e for e in x_list_temp],\n",
    "        obs_idxs = test_idx, \n",
    "        obs_geno_lookup = obs_geno_lookup,\n",
    "        use_gpu_num = 0,\n",
    "        device = 'cuda',\n",
    "    ),\n",
    "    batch_size = dataloader_batch_size,\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "# next(iter(training_dataloader))[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c21850c",
   "metadata": {},
   "source": [
    "### Set up NeuralNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed5b11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kegg_connections\n",
    "# input_size_dict\n",
    "# output_size_dict\n",
    "# list(input_tensor_dict.keys())\n",
    "# dependancy_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3061d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working version ====\n",
    "# Doesn't pass output node through relu\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, \n",
    "                 example_dict, # contains the node (excluding input tensors)\n",
    "                 example_dict_input_size, # contains the input sizes (including the tensors)\n",
    "                 example_dict_output_size,\n",
    "                 input_tensor_names,\n",
    "                 dependancy_order\n",
    "                ):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        def Linear_block(in_size, out_size, drop_pr):\n",
    "            block = nn.Sequential(\n",
    "                nn.Linear(in_size, out_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(drop_pr))\n",
    "            return(block)   \n",
    "        \n",
    "        \n",
    "        # fill in the list in dependancy order. \n",
    "        layer_list = []\n",
    "        for key in dependancy_order:\n",
    "            if key in input_tensor_names:\n",
    "                layer_list += [\n",
    "                    nn.Flatten()\n",
    "                ]\n",
    "            elif key != 'y_hat':\n",
    "                layer_list += [\n",
    "                    Linear_block(in_size=example_dict_input_size[key], \n",
    "                                 out_size=example_dict_output_size[key], \n",
    "                                 drop_pr=0)\n",
    "                              ]\n",
    "            else:\n",
    "                layer_list += [\n",
    "                    nn.Linear(example_dict_input_size[key], \n",
    "                              example_dict_output_size[key])\n",
    "                              ]\n",
    "                \n",
    "\n",
    "        self.nn_layer_list = nn.ModuleList(layer_list)\n",
    "\n",
    "        # things for get_input_node in forward to work.\n",
    "        self.example_dict = example_dict\n",
    "        self.input_tensor_names = input_tensor_names\n",
    "        self.dependancy_order = dependancy_order\n",
    "        \n",
    "        self.input_tensor_lookup = dict(zip(input_tensor_names, \n",
    "                                            [i for i in range(len(input_tensor_names))]))\n",
    "        self.result_list = []\n",
    "        self.result_list_lookup = {}\n",
    "            \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Note: x will be a list. input_tensor_lookup will contain the name: list index pairs.\n",
    "        # I use a dict instead of a list comprehension here because there could be an arbitrarily\n",
    "        # large number of inputs in the list. \n",
    "        def get_input_node(self, input_node, get_x):  \n",
    "#             print(input_node, self.result_list_lookup)\n",
    "            return(self.result_list[self.result_list_lookup[input_node]])\n",
    "        \n",
    "        # trying reinstantiating to get around inplace replacement issue.\n",
    "        self.result_list = []\n",
    "        self.result_list_lookup = {}\n",
    "        for key in self.dependancy_order:\n",
    "            input_nodes = self.example_dict[key]\n",
    "            nn_layer_list_idx = [i for i in range(len(dependancy_order)) if dependancy_order[i]==key][0]\n",
    "            \n",
    "            self.result_list_lookup[key] = len(self.result_list_lookup)                \n",
    "            if key in self.input_tensor_names: # If the input node is an input (flatten) layer\n",
    "                self.result_list = self.result_list + [self.nn_layer_list[nn_layer_list_idx](\n",
    "                    x[self.input_tensor_lookup[key]]\n",
    "                ).clone()]\n",
    "\n",
    "            else:\n",
    "                self.result_list = self.result_list + [self.nn_layer_list[nn_layer_list_idx](torch.concat(\n",
    "                    [get_input_node(self, input_node = e, get_x = x) for e in input_nodes], \n",
    "                    -1)).clone()]\n",
    "\n",
    "        return self.result_list[self.result_list_lookup['y_hat']]\n",
    "    \n",
    "\n",
    "model = NeuralNetwork(example_dict = kegg_connections, \n",
    "                      example_dict_input_size = input_size_dict,\n",
    "                      example_dict_output_size = output_size_dict,\n",
    "                      input_tensor_names = list(input_tensor_dict.keys()),\n",
    "                      dependancy_order = dependancy_order) \n",
    "model = model.to(device)\n",
    "# model(next(iter(training_dataloader))[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad949e0",
   "metadata": {},
   "source": [
    "### Train loop functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e3670f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the only \n",
    "def train_loop_yx(dataloader, model, loss_fn, optimizer, silent = False):\n",
    "#     import numpy as np\n",
    "#     import pandas as pd\n",
    "    import torch\n",
    "    from torch.utils.data import Dataset\n",
    "    from torch.utils.data import DataLoader\n",
    "#     from torch import nn\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (y_i, xs_i) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(xs_i)\n",
    "        \n",
    "        # ensure both are on cuda\n",
    "        if pred.device.type == 'cpu':\n",
    "            pred = pred.to('cuda')\n",
    "        if y_i.device.type == 'cpu':\n",
    "            y_i = y_i.to('cuda')\n",
    "        \n",
    "        loss = loss_fn(pred, y_i) # <----------------------------------------\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(y_i) # <----------------\n",
    "            if not silent:\n",
    "                print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6d9be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loop_yx(dataloader = training_dataloader, \n",
    "#               model = model, \n",
    "#               loss_fn = nn.MSELoss(), \n",
    "#               optimizer = torch.optim.SGD(model.parameters(), lr=0.003),\n",
    "#               silent = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e590289b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_error_yx(dataloader, model, loss_fn, silent = False):\n",
    "#     import numpy as np\n",
    "#     import pandas as pd\n",
    "    import torch\n",
    "    from torch.utils.data import Dataset\n",
    "    from torch.utils.data import DataLoader\n",
    "#     from torch import nn\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    train_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for y_i, xs_i in dataloader:\n",
    "            pred = model(xs_i)\n",
    "            \n",
    "            # ensure both are on cuda\n",
    "            if pred.device.type == 'cpu':\n",
    "                pred = pred.to('cuda')\n",
    "            if y_i.device.type == 'cpu':\n",
    "                y_i = y_i.to('cuda')\n",
    "            \n",
    "            train_loss += loss_fn(pred, y_i).item() # <----------------------\n",
    "            \n",
    "    train_loss /= num_batches\n",
    "    return(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84741fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop_yx(dataloader, model, loss_fn, silent = False):\n",
    "#     import numpy as np\n",
    "#     import pandas as pd\n",
    "    import torch\n",
    "    from torch.utils.data import Dataset\n",
    "    from torch.utils.data import DataLoader\n",
    "#     from torch import nn\n",
    "\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for y_i, xs_i in dataloader:\n",
    "            pred = model(xs_i)\n",
    "            \n",
    "            # ensure both are on cuda\n",
    "            if pred.device.type == 'cpu':\n",
    "                pred = pred.to('cuda')\n",
    "            if y_i.device.type == 'cpu':\n",
    "                y_i = y_i.to('cuda')\n",
    "                \n",
    "            test_loss += loss_fn(pred, y_i).item() # <-----------------------\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    if not silent:\n",
    "        print(f\"Test Error: Avg loss: {test_loss:>8f}\")\n",
    "    return(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf9ff14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn_yx(\n",
    "    cache_path,\n",
    "    training_dataloader,\n",
    "    testing_dataloader,\n",
    "    model,\n",
    "    learning_rate = 1e-3,\n",
    "    batch_size = 64,\n",
    "    epochs = 500,\n",
    "    model_prefix = 'model',\n",
    "    save_model = False\n",
    "):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import torch\n",
    "#     from torch.utils.data import Dataset\n",
    "#     from torch.utils.data import DataLoader\n",
    "    from torch import nn\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    # Initialize the loss function\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    loss_df = pd.DataFrame([i for i in range(epochs)], columns = ['Epoch'])\n",
    "    loss_df['TrainMSE'] = np.nan\n",
    "    loss_df['TestMSE']  = np.nan\n",
    "\n",
    "    for t in tqdm(range(epochs)):        \n",
    "#         print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train_loop_yx(training_dataloader, model, loss_fn, optimizer, silent = True)\n",
    "\n",
    "        loss_df.loc[loss_df.index == t, 'TrainMSE'\n",
    "                   ] = train_error_yx(training_dataloader, model, loss_fn, silent = True)\n",
    "        \n",
    "        loss_df.loc[loss_df.index == t, 'TestMSE'\n",
    "                   ] = test_loop_yx(testing_dataloader, model, loss_fn, silent = True)\n",
    "        \n",
    "        if (t+1)%5 == 0: # Cache in case training is interupted. \n",
    "            # print(loss_df.loc[loss_df.index == t, ['TrainMSE', 'TestMSE']])\n",
    "            if save_model:\n",
    "                torch.save(model.state_dict(), \n",
    "                           cache_path+'/'+model_prefix+'_'+str(t)+'_'+str(epochs)+'.pt') # convention is to use .pt or .pth\n",
    "        \n",
    "    return([model, loss_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b357687",
   "metadata": {},
   "source": [
    "### Fit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e8bcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ee = next(iter(training_dataloader))\n",
    "# [e.shape for e in ee[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f34cc2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/1 [2:49:27<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[77], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m run_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      3\u001b[0m epochs_run \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 5\u001b[0m model, loss_df \u001b[38;5;241m=\u001b[39m train_nn_yx(\n\u001b[1;32m      6\u001b[0m     cache_path,\n\u001b[1;32m      7\u001b[0m     training_dataloader,\n\u001b[1;32m      8\u001b[0m     testing_dataloader,\n\u001b[1;32m      9\u001b[0m     model,\n\u001b[1;32m     10\u001b[0m     learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-3\u001b[39m,\n\u001b[1;32m     11\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m dataloader_batch_size,\n\u001b[1;32m     12\u001b[0m     epochs \u001b[38;5;241m=\u001b[39m (run_epochs \u001b[38;5;241m-\u001b[39m epochs_run)\n\u001b[1;32m     13\u001b[0m )\n",
      "Cell \u001b[0;32mIn[75], line 31\u001b[0m, in \u001b[0;36mtrain_nn_yx\u001b[0;34m(cache_path, training_dataloader, testing_dataloader, model, learning_rate, batch_size, epochs, model_prefix, save_model)\u001b[0m\n\u001b[1;32m     27\u001b[0m     loss_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTestMSE\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnan\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(epochs)):        \n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m#         print(f\"Epoch {t+1}\\n-------------------------------\")\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m         train_loop_yx(training_dataloader, model, loss_fn, optimizer, silent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     33\u001b[0m         loss_df\u001b[38;5;241m.\u001b[39mloc[loss_df\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m==\u001b[39m t, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrainMSE\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     34\u001b[0m                    ] \u001b[38;5;241m=\u001b[39m train_error_yx(training_dataloader, model, loss_fn, silent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     36\u001b[0m         loss_df\u001b[38;5;241m.\u001b[39mloc[loss_df\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m==\u001b[39m t, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTestMSE\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     37\u001b[0m                    ] \u001b[38;5;241m=\u001b[39m test_loop_yx(testing_dataloader, model, loss_fn, silent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[71], line 10\u001b[0m, in \u001b[0;36mtrain_loop_yx\u001b[0;34m(dataloader, model, loss_fn, optimizer, silent)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#     from torch import nn\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataloader\u001b[38;5;241m.\u001b[39mdataset)\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch, (y_i, xs_i) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;66;03m# Compute prediction and loss\u001b[39;00m\n\u001b[1;32m     12\u001b[0m         pred \u001b[38;5;241m=\u001b[39m model(xs_i)\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;66;03m# ensure both are on cuda\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/EnvDL/lib/python3.11/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/EnvDL/lib/python3.11/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/EnvDL/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/EnvDL/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[60], line 39\u001b[0m, in \u001b[0;36mListDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     38\u001b[0m     y_idx\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 39\u001b[0m     x_idx \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m x_idx]\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform:\n\u001b[1;32m     42\u001b[0m     y_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(y_idx)\n",
      "Cell \u001b[0;32mIn[60], line 39\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     38\u001b[0m     y_idx\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 39\u001b[0m     x_idx \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m x_idx]\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform:\n\u001b[1;32m     42\u001b[0m     y_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(y_idx)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# dataloader_batch_size = 1000\n",
    "run_epochs = 1\n",
    "epochs_run = 0\n",
    "\n",
    "model, loss_df = train_nn_yx(\n",
    "    cache_path,\n",
    "    training_dataloader,\n",
    "    testing_dataloader,\n",
    "    model,\n",
    "    learning_rate = 1e-3,\n",
    "    batch_size = dataloader_batch_size,\n",
    "    epochs = (run_epochs - epochs_run)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee05aef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.to('cpu')\n",
    "# model(next(iter(training_dataloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c0b20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_df\n",
    "\n",
    "px.line(loss_df, x = 'Epoch', y = 'TrainMSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f442bb58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91327e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "naieve_yhat = training_dataloader.dataset.y.mean()\n",
    "\n",
    "naieve_MSE_Train = np.array(((naieve_yhat - training_dataloader.dataset.y)**2)).mean(),\n",
    "\n",
    "naieve_MSE_Test = np.array(((naieve_yhat - testing_dataloader.dataset.y)**2)).mean(),\n",
    "\n",
    "(naieve_MSE_Train, naieve_MSE_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de98ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=loss_df.Epoch, y=loss_df.TestMSE,\n",
    "                    mode='lines', name='Test'))\n",
    "fig.add_trace(go.Scatter(x=loss_df.Epoch, y=loss_df.TrainMSE,\n",
    "                    mode='lines', name='Train'))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=loss_df.Epoch, y=[list(naieve_MSE_Test)[0]  for e in range(len(loss_df.Epoch))], \n",
    "                         mode='lines', name='Naieve Test'))\n",
    "fig.add_trace(go.Scatter(x=loss_df.Epoch, y=[list(naieve_MSE_Train)[0] for e in range(len(loss_df.Epoch))], \n",
    "                         mode='lines', name='Naieve Train'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30d1a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stophere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56aaad6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d331e55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023bc452",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e804746c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71cdbf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b685d9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_run = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171c02dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_batch_size = 500\n",
    "run_epochs = 100\n",
    "\n",
    "# don't run if either of these exist because there may be cases where we want the results but not the model\n",
    "import re\n",
    "\n",
    "if not os.path.exists(cache_path+'/model.pt'): \n",
    "    # Shared setup (train from scratch and load latest)\n",
    "#     model = NeuralNetwork()\n",
    "\n",
    "#     # find the biggest model to save\n",
    "#     saved_models = os.listdir(cache_path)\n",
    "#     saved_models = [e for e in saved_models if re.match('model*', e)]\n",
    "\n",
    "#     if saved_models == []:\n",
    "#         epochs_run = 0\n",
    "#     else:\n",
    "#         saved_models = [e for e in saved_models if e != 'model.pt']\n",
    "#         # if there are saved models reload and resume training\n",
    "#         saved_models_numbers = [int(e.replace('model_', ''\n",
    "#                                     ).replace('.pt', ''\n",
    "#                                     ).split('_')[0]) for e in saved_models]\n",
    "#         # saved_models\n",
    "#         epochs_run = max(saved_models_numbers)+1 # add 1 to account for 0 index\n",
    "#         latest_model = [e for e in saved_models if re.match(\n",
    "#             '^model_'+str(epochs_run-1)+'_.*\\.pt$', e)][0] # subtract 1 to convert back\n",
    "#         model.load_state_dict(torch.load(cache_path+latest_model))\n",
    "#         print('Resuming Training: '+str(epochs_run)+'/'+str(run_epochs)+' epochs run.')\n",
    "    \n",
    "    model.to(device)    \n",
    "\n",
    "    model, loss_df = train_nn(\n",
    "        cache_path,\n",
    "        training_dataloader,\n",
    "        testing_dataloader,\n",
    "        model,\n",
    "        learning_rate = 1e-3,\n",
    "        batch_size = dataloader_batch_size,\n",
    "        epochs = (run_epochs - epochs_run)\n",
    "    )\n",
    "    \n",
    "    # experimental outputs:\n",
    "    # 1. Model\n",
    "    torch.save(model.state_dict(), cache_path+'/model.pt') # convention is to use .pt or .pth\n",
    "\n",
    "    # 2. loss_df    \n",
    "    # If this is resuming training, load and extend the existing loss dataframe\n",
    "    if os.path.exists(cache_path+'/loss_df.csv'):\n",
    "        loss_df_on_disk = pd.read_csv(cache_path+'/loss_df.csv')\n",
    "        epoch_offset = 1 + loss_df_on_disk['Epoch'].max()\n",
    "        loss_df['Epoch'] = loss_df['Epoch'] + epoch_offset\n",
    "        loss_df = pd.concat([loss_df_on_disk, loss_df])\n",
    "    loss_df.to_csv(cache_path+'/loss_df.csv', index=False)  \n",
    "    \n",
    "    # 3. predictions \n",
    "    yhats = pd.concat([\n",
    "        yhat_loop(testing_dataloader, model).assign(Split = 'Test'),\n",
    "        yhat_loop(training_dataloader, model).assign(Split = 'Train')], axis = 0)\n",
    "\n",
    "    yhats.to_csv(cache_path+'/yhats.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4642a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "stophere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5841e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316c7cd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a495dd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d235c702",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ac4085",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4129310d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2191863",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c37ef3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02205e69",
   "metadata": {},
   "source": [
    "## One Hot Encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f93d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(9230473) # note, must use rng.shuffle(arr) below for this to take effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a91e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_cumsum_thresh = .9\n",
    "\n",
    "# make a df to aid in creating train/test splits\n",
    "# the plan is to shuffle the rows of the df, calculate the cumulative sum of the percents obs, then \n",
    "# the entries above and below a given percent will be the train/test.\n",
    "obs_per_hybrid = phno.assign(n = 1).groupby('Hybrid').count().reset_index()\n",
    "obs_per_hybrid = obs_per_hybrid.loc[:, ['Hybrid', 'n']]\n",
    "obs_per_hybrid['pct'] = obs_per_hybrid.n / obs_per_hybrid.n.sum()\n",
    "obs_per_hybrid['pct_cumsum'] = np.nan\n",
    "obs_per_hybrid['random_order'] = 0\n",
    "\n",
    "obs_per_hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ca1bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in the random values to sort on\n",
    "arr = np.arange(obs_per_hybrid.shape[0])\n",
    "rng.shuffle(arr)\n",
    "obs_per_hybrid.random_order = arr\n",
    "\n",
    "obs_per_hybrid = obs_per_hybrid.sort_values('random_order')\n",
    "obs_per_hybrid['pct_cumsum'] = obs_per_hybrid.pct.cumsum()\n",
    "obs_per_hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bce9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert back into phno indices\n",
    "train_hybrids = list(obs_per_hybrid.loc[(obs_per_hybrid.pct_cumsum <= pct_cumsum_thresh), 'Hybrid'])\n",
    "test_hybrids  = list(obs_per_hybrid.loc[(obs_per_hybrid.pct_cumsum >  pct_cumsum_thresh), 'Hybrid'])\n",
    "\n",
    "train_idx = phno.loc[(phno.Hybrid.isin(train_hybrids)), ].index\n",
    "test_idx  = phno.loc[(phno.Hybrid.isin(test_hybrids)), ].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d0163e",
   "metadata": {},
   "outputs": [],
   "source": [
    "[len(train_idx), len(test_idx)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbaedf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm all observation idxs are have genomic information\n",
    "assert [] == [e for e in list(train_idx)+list(test_idx) if e not in obs_geno_lookup[:, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02004189",
   "metadata": {},
   "outputs": [],
   "source": [
    "YMat_cs = calc_cs(YMat[train_idx])\n",
    "y_cs = apply_cs(YMat, YMat_cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15ee06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_dataloader = DataLoader(\n",
    "#     ACGTDataset(y = torch.from_numpy(y_cs[train_idx])[:, None].to(torch.float), \n",
    "#                 G = torch.from_numpy(ACGT_hilb).to(torch.float), \n",
    "#                 idx_original = torch.from_numpy(np.array(train_idx)),\n",
    "#                 idx_lookup   = torch.from_numpy(np.asarray(obs_geno_lookup)),\n",
    "#                 use_gpu_num = 0,\n",
    "#                 device = 'cuda'\n",
    "#                ),\n",
    "#     batch_size = 50,\n",
    "#     shuffle = True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08eaaff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing_dataloader = DataLoader(\n",
    "#     ACGTDataset(y = torch.from_numpy(y_cs[test_idx])[:, None].to(torch.float), \n",
    "#                 G = torch.from_numpy(ACGT_hilb).to(torch.float), \n",
    "#                 idx_original = torch.from_numpy(np.array(test_idx)),\n",
    "#                 idx_lookup   = torch.from_numpy(np.asarray(obs_geno_lookup)),\n",
    "#                 use_gpu_num = 0,\n",
    "#                 device = 'cuda'\n",
    "#                ),\n",
    "#     batch_size = 50,\n",
    "#     shuffle = True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b8b1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(iter(training_dataloader))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788ba1e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf76c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class NeuralNetwork(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(NeuralNetwork, self).__init__()    \n",
    "\n",
    "#         def Linear_block(in_size, out_size, drop_pr):\n",
    "#             block = nn.Sequential(\n",
    "#                 nn.Linear(in_size, out_size),\n",
    "#                 nn.ReLU(),\n",
    "#                 nn.Dropout(drop_pr)\n",
    "#             )\n",
    "#             return(block)         \n",
    "        \n",
    "        \n",
    "# #         def Conv1D_Max_block(in_channels, out_channels, kernel_size, stride):\n",
    "# #             block = nn.Sequential(\n",
    "# #                 nn.Conv1d(\n",
    "# #                     in_channels= in_channels, # second channel\n",
    "# #                     out_channels= out_channels,\n",
    "# #                     kernel_size= kernel_size,\n",
    "# #                     stride= stride\n",
    "# #                 ), \n",
    "# #                 nn.MaxPool1d((kernel_size,), stride=stride)\n",
    "# #             )\n",
    "# #             return(block)\n",
    "        \n",
    "#         self.x_network = nn.Sequential(\n",
    "#             nn.Conv2d(\n",
    "#                     in_channels= 4, \n",
    "#                     out_channels= 4,\n",
    "#                     kernel_size= (3, 3),\n",
    "#                     stride= 2,\n",
    "#                     padding = 1,\n",
    "#                     bias = True\n",
    "#                 ),\n",
    "#             nn.Conv2d(\n",
    "#                     in_channels= 4, \n",
    "#                     out_channels= 4,\n",
    "#                     kernel_size= (3, 3),\n",
    "#                     stride= 2,\n",
    "#                     padding = 1,\n",
    "#                     bias = True\n",
    "#                 ),\n",
    "#             nn.Conv2d(\n",
    "#                     in_channels= 4, \n",
    "#                     out_channels= 4,\n",
    "#                     kernel_size= (3, 3),\n",
    "#                     stride= 2,\n",
    "#                     padding = 1,\n",
    "#                     bias = True\n",
    "#                 ),\n",
    "#             nn.Conv2d(\n",
    "#                     in_channels= 4, \n",
    "#                     out_channels= 4,\n",
    "#                     kernel_size= (3, 3),\n",
    "#                     stride= 2,\n",
    "#                     padding = 1,\n",
    "#                     bias = True\n",
    "#                 ),\n",
    "#             nn.Conv2d(\n",
    "#                     in_channels= 4, \n",
    "#                     out_channels= 4,\n",
    "#                     kernel_size= (3, 3),\n",
    "#                     stride= 2,\n",
    "#                     padding = 1,\n",
    "#                     bias = True\n",
    "#                 ), \n",
    "#             nn.Flatten(),            \n",
    "# #             Linear_block(in_size = 116, out_size = 32, drop_pr = 0.3),\n",
    "#             nn.Linear(512, 1)\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x_out = self.x_network(x)\n",
    "#         return x_out\n",
    "\n",
    "# model = NeuralNetwork().to(device)\n",
    "\n",
    "# model(next(iter(training_dataloader))[0]).shape\n",
    "\n",
    "# # torch.Size([50, 4, 256, 512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654b3bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # small scale test\n",
    "# model, loss_df = train_nn(\n",
    "#     cache_path,\n",
    "#     training_dataloader,\n",
    "#     testing_dataloader,\n",
    "#     model,\n",
    "#     learning_rate = 1e-3,\n",
    "#     batch_size = 50, #dataloader_batch_size,\n",
    "#     epochs = 1 #(run_epochs - epochs_run)\n",
    "# )\n",
    "\n",
    "# loss_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9cbc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate_iterations(sec_per_it = 95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27248cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove_matching_files(\n",
    "#     cache_path,\n",
    "#     match_regex_list = ['model\\.pt'],\n",
    "#     dry_run = False\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3174cd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader_batch_size = 50\n",
    "# run_epochs = 100\n",
    "\n",
    "# # don't run if either of these exist because there may be cases where we want the results but not the model\n",
    "# import re\n",
    "\n",
    "# if not os.path.exists(cache_path+'/model.pt'): \n",
    "#     # Shared setup (train from scratch and load latest)\n",
    "#     model = NeuralNetwork()\n",
    "\n",
    "#     # find the biggest model to save\n",
    "#     saved_models = os.listdir(cache_path)\n",
    "#     saved_models = [e for e in saved_models if re.match('model*', e)]\n",
    "\n",
    "#     if saved_models == []:\n",
    "#         epochs_run = 0\n",
    "#     else:\n",
    "#         saved_models = [e for e in saved_models if e != 'model.pt']\n",
    "#         # if there are saved models reload and resume training\n",
    "#         saved_models_numbers = [int(e.replace('model_', ''\n",
    "#                                     ).replace('.pt', ''\n",
    "#                                     ).split('_')[0]) for e in saved_models]\n",
    "#         # saved_models\n",
    "#         epochs_run = max(saved_models_numbers)+1 # add 1 to account for 0 index\n",
    "#         latest_model = [e for e in saved_models if re.match(\n",
    "#             '^model_'+str(epochs_run-1)+'_.*\\.pt$', e)][0] # subtract 1 to convert back\n",
    "#         model.load_state_dict(torch.load(cache_path+latest_model))\n",
    "#         print('Resuming Training: '+str(epochs_run)+'/'+str(run_epochs)+' epochs run.')\n",
    "    \n",
    "#     model.to(device)    \n",
    "\n",
    "#     model, loss_df = train_nn(\n",
    "#         cache_path,\n",
    "#         training_dataloader,\n",
    "#         testing_dataloader,\n",
    "#         model,\n",
    "#         learning_rate = 1e-3,\n",
    "#         batch_size = dataloader_batch_size,\n",
    "#         epochs = (run_epochs - epochs_run)\n",
    "#     )\n",
    "    \n",
    "#     # experimental outputs:\n",
    "#     # 1. Model\n",
    "#     torch.save(model.state_dict(), cache_path+'/model.pt') # convention is to use .pt or .pth\n",
    "\n",
    "#     # 2. loss_df    \n",
    "#     # If this is resuming training, load and extend the existing loss dataframe\n",
    "#     if os.path.exists(cache_path+'/loss_df.csv'):\n",
    "#         loss_df_on_disk = pd.read_csv(cache_path+'/loss_df.csv')\n",
    "#         epoch_offset = 1 + loss_df_on_disk['Epoch'].max()\n",
    "#         loss_df['Epoch'] = loss_df['Epoch'] + epoch_offset\n",
    "#         loss_df = pd.concat([loss_df_on_disk, loss_df])\n",
    "#     loss_df.to_csv(cache_path+'/loss_df.csv', index=False)  \n",
    "    \n",
    "#     # 3. predictions \n",
    "#     yhats = pd.concat([\n",
    "#         yhat_loop(testing_dataloader, model).assign(Split = 'Test'),\n",
    "#         yhat_loop(training_dataloader, model).assign(Split = 'Train')], axis = 0)\n",
    "\n",
    "#     yhats.to_csv(cache_path+'/yhats.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4764b227",
   "metadata": {},
   "source": [
    "### Standard Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127430f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f27738",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd9373f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f93f99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34602160",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad680db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale_dict = {'y1':YMat_cs}\n",
    "# import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59662f6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec22f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# naieve_yhat = training_dataloader.dataset.y.mean()\n",
    "\n",
    "# naieve_MSE_Train = reverse_cs( \n",
    "#     np.array(((naieve_yhat - training_dataloader.dataset.y)**2)).mean(),\n",
    "#     scale_dict['y1']\n",
    "# )\n",
    "\n",
    "# naieve_MSE_Test = reverse_cs( \n",
    "#     np.array(((naieve_yhat - testing_dataloader.dataset.y)**2)).mean(),\n",
    "#     scale_dict['y1']\n",
    "# )\n",
    "\n",
    "# naieve_MSE_Train, naieve_MSE_Test\n",
    "\n",
    "\n",
    "\n",
    "# loss_df = pd.read_csv(cache_path+'/loss_df.csv')\n",
    "\n",
    "# loss_df.TrainMSE = reverse_cs(loss_df.TrainMSE, scale_dict['y1'])\n",
    "# loss_df.TestMSE  = reverse_cs(loss_df.TestMSE , scale_dict['y1'])\n",
    "\n",
    "\n",
    "# fig = go.Figure()\n",
    "# fig.add_trace(go.Scatter(x=loss_df.Epoch, y=loss_df.TestMSE,\n",
    "#                     mode='lines', name='Test'))\n",
    "# fig.add_trace(go.Scatter(x=loss_df.Epoch, y=loss_df.TrainMSE,\n",
    "#                     mode='lines', name='Train'))\n",
    "\n",
    "# fig.add_trace(go.Scatter(x=loss_df.Epoch, y=[naieve_MSE_Test  for e in range(len(loss_df.Epoch))], \n",
    "#                          mode='lines', name='Naieve Test'))\n",
    "# fig.add_trace(go.Scatter(x=loss_df.Epoch, y=[naieve_MSE_Train for e in range(len(loss_df.Epoch))], \n",
    "#                          mode='lines', name='Naieve Train'))\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b66997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yhats = pd.read_csv(cache_path+'/yhats.csv')\n",
    "\n",
    "# # px.scatter(yhats, x = 'y_true', y = 'y_pred', color = 'Split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923b6dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yhats.y_true = reverse_cs(yhats.y_true, scale_dict['y1'])\n",
    "# yhats.y_pred = reverse_cs(yhats.y_pred, scale_dict['y1'])\n",
    "\n",
    "# # px.scatter(yhats, x = 'y_true', y = 'y_pred', color = 'Split', trendline=\"ols\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8460bc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yhats['Error'] = yhats.y_pred - yhats.y_true\n",
    "\n",
    "# px.histogram(yhats, x = 'Error', color = 'Split',\n",
    "#              marginal=\"box\", # can be `rug`, `violin`\n",
    "#              nbins= 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21076dbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
