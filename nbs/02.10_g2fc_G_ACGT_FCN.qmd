---
title: G only ACGT Probabilities
jupyter: python3
---


> 


```{python}
# Hacky way to schedule. Here I'm setting these to sleep until the gpus should be free.
# At the end of the notebooks  os._exit(00) will kill the kernel freeing the gpu. 
#                          Hours to wait
# import time; time.sleep( 6 * (60*60))
```

```{python}
import os, re, json
from tqdm import tqdm

import numpy as np
import pandas as pd
pd.set_option('display.max_columns', None)

import plotly.express as px
import plotly.io as pio
pio.templates.default = "plotly_white"

import torch
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
from torch import nn

from EnvDL.core import * 
# includes 
    # remove_matching_files 
    # read_json
from EnvDL.dna import *
from EnvDL.dlfn import * 
# includes
    # read_split_info 
    # find_idxs_split_dict
    # train_loop_yx
    # train_error_yx
    # test_loop_yx
    # train_nn_yx
    # yhat_loop_yx

from tqdm import tqdm
```

```{python}
use_gpu_num = 0

device = "cuda" if torch.cuda.is_available() else "cpu"
if use_gpu_num in [0, 1]: 
    torch.cuda.set_device(use_gpu_num)
print(f"Using {device} device")
```

```{python}
# Imports for the Adaptive Experimentation (Ax) platform
from ax.service.ax_client import AxClient, ObjectiveProperties
from ax.utils.measurement.synthetic_functions import hartmann6
from ax.utils.notebook.plotting import render, init_notebook_plotting

import pickle as pkl
# Because the search space is nested, it's not storable with the builtin methods for saving/reading json (not tested with sql)
# pickling seems to work just fine.

init_notebook_plotting()
```

```{python}
cache_path = '../nbs_artifacts/02.10_g2fc_G_ACGT_FCN/'
ensure_dir_path_exists(dir_path = cache_path)
```

## Load data

```{python}
load_from = '../nbs_artifacts/01.03_g2fc_prep_matrices/'
phno_geno = pd.read_csv(load_from+'phno_geno.csv')
phno = phno_geno

obs_geno_lookup = np.load(load_from+'obs_geno_lookup.npy') # Phno_Idx	Geno_Idx	Is_Phno_Idx
YMat = np.load(load_from+'YMat.npy')
ACGT = np.load(load_from+'ACGT.npy')
```

```{python}
## Create train/test validate indicies from json
load_from = '../nbs_artifacts/01.06_g2fc_cluster_genotypes/'

split_info = read_split_info(
    load_from = '../nbs_artifacts/01.06_g2fc_cluster_genotypes/',
    json_prefix = '2023:9:5:12:8:26')

temp = phno.copy()
temp[['Female', 'Male']] = temp['Hybrid'].str.split('/', expand = True)

test_dict = find_idxs_split_dict(
    obs_df = temp, 
    split_dict = split_info['test'][0]
)
# test_dict

# since this is applying predefined model structure no need for validation.
# This is included for my future reference when validation is needed.
temp = temp.loc[test_dict['train_idx'], ] # restrict before re-aplying
obs_df_ref = temp.copy()

val_dict = find_idxs_split_dict(
    obs_df = obs_df_ref, 
    split_dict = split_info['validate'][0]
)
# val_dict

# test_dict

train_idx = test_dict['train_idx']
test_idx  = test_dict['test_idx']
```

```{python}
# split_info.keys()
# split_info['validate_files']#.keys()
# split_info['validate'][0].keys()
# val_dict
```

## One Hot Encoded

```{python}
# confirm all observation idxs are have genomic information
# assert [] == [e for e in list(train_idx)+list(test_idx) if e not in obs_geno_lookup[:, 0]]
```

```{python}
YMat_cs = calc_cs(YMat[train_idx])
y_cs = apply_cs(YMat, YMat_cs)
```

```{python}
# Debugging Data -----------------------------------------------------------------------------------

train_idx = val_dict['train_idx']
test_idx  = val_dict['test_idx']

training_dataloader = DataLoader(
    ACGTDataset(y = torch.from_numpy(y_cs[train_idx])[:, None].to(torch.float), 
                G = torch.from_numpy(ACGT).to(torch.float), 
                idx_original = torch.from_numpy(np.array(train_idx)),
                idx_lookup   = torch.from_numpy(np.asarray(obs_geno_lookup)),
                use_gpu_num = 0,
                device = 'cuda'
               ),
    batch_size = 50,
    shuffle = True
)

testing_dataloader = DataLoader(
    ACGTDataset(y = torch.from_numpy(y_cs[test_idx])[:, None].to(torch.float), 
                G = torch.from_numpy(ACGT).to(torch.float), 
                idx_original = torch.from_numpy(np.array(test_idx)),
                idx_lookup   = torch.from_numpy(np.asarray(obs_geno_lookup)),
                use_gpu_num = 0,
                device = 'cuda'
               ),
    batch_size = 50,
    shuffle = True
)
```

## Tune with `ax`

```{python}
# needs to be able to take a dictionary with the parameters that can be varied.

# must subclass train_nn to make changes to the optimizer
```

```{python}
2**6
```

```{python}
 np.linspace(2, 32, 12)
```

```{python}
# xs = [i for i in np.linspace(1, 32, 10)]
# ys = [round(e**2) for e in xs]
# px.scatter(
#     pd.DataFrame({'x': xs, 'y': ys}),
#     x ='x', y= 'y')

# [int(2**i) for i in np.linspace(1, 10, 10)]

[round(i**2) for i in np.linspace(1, 32, 10)]
```

```{python}
num_layer_blocks = [1]+[2*(1+i) for i in range(6)]
in_size_int = 4*125891
out_size_choice = [2**(4+i) for i in range(7)] #16 <-> 1024
drop_pr_choice = [(5*e)/100 for e in range(11)] # 0 <-> 50
epoch_choice = [2, 1024]

# See https://github.com/facebook/Ax/issues/1454#issuecomment-1462417398

# num_layer_blocks = [1, 2]
# in_size_int
# out_size_choice = [16, 32]
# drop_pr_choice = [0.0, 0.3]
# epoch_choice

params = [
    {
        "name": "num_layers",
        "type": "choice",
        "is_ordered": True, 
        "values": num_layer_blocks,
        "dependents": {
            num_layers: [
                f"in_{  i + 1}_of_{num_layers}" for i in range(num_layers) if i == 0]+[
                f"out_{ i + 1}_of_{num_layers}" for i in range(num_layers)]+[
                f"drop_{i + 1}_of_{num_layers}" for i in range(num_layers)]+[
                f"epoch_of_{num_layers}" for i in range(num_layers) if i == 0
            ] for num_layers in num_layer_blocks
        }
    },
    *[  # * here puts the entries of the list comprehension into the outer list instead of keeping them as a sub-list
        {
            "name": f"in_{ i + 1}_of_{num_layers}",
            "type": "fixed",
            "value_type": 'int',
            "value": in_size_int
        } for num_layers in num_layer_blocks for i in range(num_layers) if i == 0
        
    ],
    *[
        {
            "name": f"out_{ i + 1}_of_{num_layers}",
            "type": "choice",
            "is_ordered": True, 
            "value_type": 'int',
            "values": out_size_choice
        } for num_layers in num_layer_blocks for i in range(num_layers) 
        
    ],
    *[
        {
            "name": f"drop_{ i + 1}_of_{num_layers}",
            "type": "choice",
            "is_ordered": True, 
            "value_type": 'float',
            "values": drop_pr_choice
        } for num_layers in num_layer_blocks for i in range(num_layers) 
        
    ],
    *[
        {
            "name": f"epoch_of_{num_layers}",
            "type": "range",
#             "is_ordered": True, 
            "value_type": 'int',
            "bounds": epoch_choice
        } for num_layers in num_layer_blocks 
        
    ],
    
]
# params
```

```{python}
# Initialize
ax_client = AxClient()

# Using SAASBO per https://github.com/facebook/Ax/issues/1454#issuecomment-1462417398
# Create an experiment with required arguments: name, parameters, and objective_name.
ax_client.create_experiment(
    name="tune_fcn_layer_number",  # The name of the experiment.
    parameters=params,
    objectives={"rmse": ObjectiveProperties(minimize=True)},  # The objective name and minimization setting.
    # parameter_constraints: Optional, a list of strings of form "p1 >= p2" or "p1 + p2 <= some_bound".
    # outcome_constraints: Optional, a list of strings of form "constrained_metric <= some_bound".
    choose_generation_strategy_kwargs={"use_saasbo": True}
)
```

```{python}
# if there already is a set of test results, then load it.
if os.path.exists(cache_path+'ax_client.pkl'):
    ax_client = None
    with open(cache_path+'ax_client.pkl', 'rb') as f:
        dat = pkl.load(f)
    ax_client = dat
```

```{python}
class NeuralNetwork(nn.Module):
    def __init__(self, parameterization):
        super(NeuralNetwork, self).__init__()    

        def Linear_block(in_size, out_size, drop_pr):
            block = nn.Sequential(
                nn.Linear(in_size, out_size),
                nn.ReLU(),
                nn.Dropout(drop_pr)
            )
            return(block)         
        
        module_list = []

        max_layer = parameterization['num_layers']
        for i in range(max_layer):
            if i  == 0:
                name_in = f"in_{i+1}_of_{max_layer}"
            else:
                name_in = f"out_{i}_of_{max_layer}"
            name_out = f"out_{i+1}_of_{max_layer}"
            name_drop= f"drop_{i+1}_of_{max_layer}"

            if i == 0:
                module_list += [nn.Flatten()]

            module_list += [
                    Linear_block(
                        in_size  = parameterization[name_in], 
                        out_size = parameterization[name_out], 
                        drop_pr  = parameterization[name_drop])]

            if (i+1) == max_layer:
                module_list += [nn.Linear(parameterization[name_out], 1)]
                
        self.x_network = nn.ModuleList(module_list)
        
    def forward(self, x):
        
        for mod in self.x_network:
            x = mod(x)
            
        return x

# model = NeuralNetwork(
#     parameterization = parameterization).to(device)

# model(next(iter(training_dataloader))[0][0:5])
```

```{python}
# Get the parameters and run the trial 
# baseline_parameters = ax_client.get_trial_parameters(trial_index=0)
# ax_client.complete_trial(trial_index=0, raw_data=train_evaluate(baseline_parameters))
```

```{python}
def train_evaluate(parameterization, use_validation_sets = 1):
    dataloader_batch_size = 50
    # run_epochs = 2

    loss_list = []    
    for validate_i in list(np.random.choice(
        [i for i in range(len(split_info['validate']))], 
        use_validation_sets, 
        replace = False)):
        print(f"Running with validation set: {validate_i}")

        val_dict = find_idxs_split_dict(
            obs_df = obs_df_ref, 
            split_dict = split_info['validate'][validate_i]
        )

        train_idx = val_dict['train_idx']
        test_idx  = val_dict['test_idx']

        training_dataloader = DataLoader(
            ACGTDataset(y = torch.from_numpy(y_cs[train_idx])[:, None].to(torch.float), 
                        G = torch.from_numpy(ACGT).to(torch.float), 
                        idx_original = torch.from_numpy(np.array(train_idx)),
                        idx_lookup   = torch.from_numpy(np.asarray(obs_geno_lookup)),
                        use_gpu_num = 0,
                        device = 'cuda'
                       ),
            batch_size = 50,
            shuffle = True
        )

        testing_dataloader = DataLoader(
            ACGTDataset(y = torch.from_numpy(y_cs[test_idx])[:, None].to(torch.float), 
                        G = torch.from_numpy(ACGT).to(torch.float), 
                        idx_original = torch.from_numpy(np.array(test_idx)),
                        idx_lookup   = torch.from_numpy(np.asarray(obs_geno_lookup)),
                        use_gpu_num = 0,
                        device = 'cuda'
                       ),
            batch_size = 50,
            shuffle = True
        )

        model = NeuralNetwork(parameterization = parameterization)    
        model.to(device)    
        model, loss_df = train_nn(
            cache_path,
            training_dataloader,
            testing_dataloader,
            model,
            learning_rate = 1e-3,
            batch_size = dataloader_batch_size,
            epochs = parameterization[f"epoch_of_{parameterization['num_layers']}"]
        )
        loss_list += [list(loss_df['TestMSE'])[-1]]
    
    mean_loss = (sum(loss_list)/len(loss_list))
    print((mean_loss, loss_list))
    return(mean_loss)
```

```{python}
# train_evaluate(parameterization)
```

```{python}
for i in range(10):
    parameters, trial_index = ax_client.get_next_trial()
    # Local evaluation here can be replaced with deployment to external system.
    ax_client.complete_trial(trial_index=trial_index, 
                             raw_data=train_evaluate(parameters))
    
    # Ax has built in functions to save client state to JSON or to a database. The builtins 
    # (save_to_json_file, load_from_json_file) fail, seemingly from having a nested search space. 
    # No debugging information found on Stackoverflow or google so I'm using this work around. 
    # Warning on running ax_client.complete_trial
    #     UserWarning:
    # Cannot flatten observation features ObservationFeatures(parameters={}, trial_index=0) as full parameterization is not recorded in metadata.
    
    with open(cache_path+'ax_client.pkl', 'wb') as f:
        pkl.dump(ax_client, f)
```

```{python}
ax_client.get_trials_data_frame()
```

```{python}
best_parameters, values = ax_client.get_best_parameters()
best_parameters
```

```{python}
mean, covariance = values
mean
```

```{python}
# render(ax_client.get_contour_plot(param_x="drop_1_of_2", 
#                                   param_y="drop_2_of_2", 
#                                   metric_name="rmse"))
```



```{python}
# ax_client.save_to_json_file(filepath = cache_path+'ax_client.json') 
```

```{python}
# restored_ax_client = (
#     AxClient.load_from_json_file(filepath = cache_path+'ax_client.json') 
# )
```



## Train Finalized Model

```{python}
# dataloader_batch_size = 50
# run_epochs = 2#100

# # don't run if either of these exist because there may be cases where we want the results but not the model
# import re

# if not os.path.exists(cache_path+'/model.pt'): 
#     # Shared setup (train from scratch and load latest)
#     model = NeuralNetwork()

#     # find the biggest model to save
#     saved_models = os.listdir(cache_path)
#     saved_models = [e for e in saved_models if re.match('model*', e)]

#     if saved_models == []:
#         epochs_run = 0
#     else:
#         saved_models = [e for e in saved_models if e != 'model.pt']
#         # if there are saved models reload and resume training
#         saved_models_numbers = [int(e.replace('model_', ''
#                                     ).replace('.pt', ''
#                                     ).split('_')[0]) for e in saved_models]
#         # saved_models
#         epochs_run = max(saved_models_numbers)+1 # add 1 to account for 0 index
#         latest_model = [e for e in saved_models if re.match(
#             '^model_'+str(epochs_run-1)+'_.*\.pt$', e)][0] # subtract 1 to convert back
#         model.load_state_dict(torch.load(cache_path+latest_model))
#         print('Resuming Training: '+str(epochs_run)+'/'+str(run_epochs)+' epochs run.')
    
#     model.to(device)    

#     model, loss_df = train_nn(
#         cache_path,
#         training_dataloader,
#         testing_dataloader,
#         model,
#         learning_rate = 1e-3,
#         batch_size = dataloader_batch_size,
#         epochs = (run_epochs - epochs_run)
#     )
    
#     # experimental outputs:
#     # 1. Model
#     torch.save(model.state_dict(), cache_path+'/model.pt') # convention is to use .pt or .pth

#     # 2. loss_df    
#     # If this is resuming training, load and extend the existing loss dataframe
#     if os.path.exists(cache_path+'/loss_df.csv'):
#         loss_df_on_disk = pd.read_csv(cache_path+'/loss_df.csv')
#         epoch_offset = 1 + loss_df_on_disk['Epoch'].max()
#         loss_df['Epoch'] = loss_df['Epoch'] + epoch_offset
#         loss_df = pd.concat([loss_df_on_disk, loss_df])
#     loss_df.to_csv(cache_path+'/loss_df.csv', index=False)  
    
#     # 3. predictions 
#     yhats = pd.concat([
#         yhat_loop(testing_dataloader, model).assign(Split = 'Test'),
#         yhat_loop(training_dataloader, model).assign(Split = 'Train')], axis = 0)

#     yhats.to_csv(cache_path+'/yhats.csv', index=False)
```


### Standard Visualizations




```{python}
scale_dict = {'y1':YMat_cs}
import plotly.graph_objects as go
```

```{python}
naieve_yhat = training_dataloader.dataset.y.mean()

naieve_MSE_Train = reverse_cs( 
    np.array(((naieve_yhat - training_dataloader.dataset.y)**2)).mean(),
    scale_dict['y1']
)

naieve_MSE_Test = reverse_cs( 
    np.array(((naieve_yhat - testing_dataloader.dataset.y)**2)).mean(),
    scale_dict['y1']
)

naieve_MSE_Train, naieve_MSE_Test



loss_df = pd.read_csv(cache_path+'/loss_df.csv')

loss_df.TrainMSE = reverse_cs(loss_df.TrainMSE, scale_dict['y1'])
loss_df.TestMSE  = reverse_cs(loss_df.TestMSE , scale_dict['y1'])


fig = go.Figure()
fig.add_trace(go.Scatter(x=loss_df.Epoch, y=loss_df.TestMSE,
                    mode='lines', name='Test'))
fig.add_trace(go.Scatter(x=loss_df.Epoch, y=loss_df.TrainMSE,
                    mode='lines', name='Train'))

fig.add_trace(go.Scatter(x=loss_df.Epoch, y=[naieve_MSE_Test  for e in range(len(loss_df.Epoch))], 
                         mode='lines', name='Naieve Test'))
fig.add_trace(go.Scatter(x=loss_df.Epoch, y=[naieve_MSE_Train for e in range(len(loss_df.Epoch))], 
                         mode='lines', name='Naieve Train'))
fig.show()
```

```{python}
yhats = pd.read_csv(cache_path+'/yhats.csv')

# px.scatter(yhats, x = 'y_true', y = 'y_pred', color = 'Split')
```

```{python}
yhats.y_true = reverse_cs(yhats.y_true, scale_dict['y1'])
yhats.y_pred = reverse_cs(yhats.y_pred, scale_dict['y1'])

# px.scatter(yhats, x = 'y_true', y = 'y_pred', color = 'Split', trendline="ols")
```

```{python}
yhats['Error'] = yhats.y_pred - yhats.y_true

px.histogram(yhats, x = 'Error', color = 'Split',
             marginal="box", # can be `rug`, `violin`
             nbins= 50)
```

```{python}
# os._exit(00)
```

