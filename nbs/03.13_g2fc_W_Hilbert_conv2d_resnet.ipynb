{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aadccc0",
   "metadata": {},
   "source": [
    "# Test out Hilbert Curve 2d\n",
    "\n",
    "> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# import plotly.express as px\n",
    "# import plotly.io as pio\n",
    "# pio.templates.default = \"plotly_white\"\n",
    "# from sklearn.manifold import TSNE # for visualizing embeddings\n",
    "\n",
    "import hilbertcurve\n",
    "from hilbertcurve.hilbertcurve import HilbertCurve\n",
    "\n",
    "from EnvDL.core import * # includes remove_matching_files\n",
    "from EnvDL.dna  import *\n",
    "from EnvDL.dlfn import * # includes LSUV_\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "use_gpu_num = 0\n",
    "\n",
    "# Imports --------------------------------------------------------------------\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import torch.nn.functional as F # F.mse_loss\n",
    "\n",
    "import einops # for einops.rearrange\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if use_gpu_num in [0, 1]: \n",
    "    torch.cuda.set_device(use_gpu_num)\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6a356c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4600ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_path = '../nbs_artifacts/03.12_g2fc_W_Hilbert_conv2d_resnet.ipynb/'\n",
    "ensure_dir_path_exists(dir_path = cache_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517af1f8",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b17d74",
   "metadata": {},
   "source": [
    "### Weather (Variable in season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6a8976",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_from = '../nbs_artifacts/01.03_g2fc_prep_matrices/'\n",
    "\n",
    "np.load(load_from+'PlantHarvestNames.npy')\n",
    "PlantHarvest = np.load(load_from+'PlantHarvest.npy')\n",
    "\n",
    "# WMat = np.load(load_from+'WMat.npy')\n",
    "WMatNames = np.load(load_from+'WMatNames.npy')\n",
    "\n",
    "WMat_hilb = np.load(load_from+'WMat_hilb.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f244bb4",
   "metadata": {},
   "source": [
    "### Response and lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e59a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_from = '../nbs_artifacts/01.03_g2fc_prep_matrices/'\n",
    "phno_geno = pd.read_csv(load_from+'phno_geno.csv')\n",
    "phno = phno_geno\n",
    "\n",
    "obs_geno_lookup = np.load(load_from+'obs_geno_lookup.npy') # Phno_Idx  Geno_Idx  Is_Phno_Idx\n",
    "obs_env_lookup = np.load(load_from+'obs_env_lookup.npy')   # Phno_Idx  Env_Idx   Is_Phno_Idx\n",
    "YMat = np.load(load_from+'YMat.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6c5870",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27e45421",
   "metadata": {},
   "source": [
    "## Demo retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29cd40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Indexes\n",
    "# idx = 0\n",
    "\n",
    "# geno_idx = obs_geno_lookup[idx, 1]\n",
    "# env_idx = obs_env_lookup[idx, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42df33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Response\n",
    "# YMat[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da133777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Weather (plant in ground) ------------------------------------------------------------------------\n",
    "# # generate planting information\n",
    "# ## Basic tensor\n",
    "# WPlant = np.zeros(365)\n",
    "# WPlant[PlantHarvest[idx, 0]:PlantHarvest[idx, 1]] = 1\n",
    "\n",
    "# ## Hilbert\n",
    "# WPlant_hilb = np_3d_to_hilbert(WPlant[None, :, None])\n",
    "# WPlant_hilb = WPlant_hilb.squeeze(axis = 3)\n",
    "# WPlant_hilb[np.isnan(WPlant_hilb)] = 0\n",
    "# # px.imshow(WPlant_hilb.squeeze())\n",
    "# # compare with\n",
    "# # px.imshow(WMat_hilb[env_idx][8, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf227f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Weather (all) ------------------------------------------------------------------------------------\n",
    "# ## Basic tensor\n",
    "# np.concatenate([WMat[env_idx], WPlant[None, :]], axis = 0).shape\n",
    "# ## Hilbert\n",
    "# np.concatenate([WMat_hilb[env_idx], WPlant_hilb], axis = 0).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8874cdbb",
   "metadata": {},
   "source": [
    "### Set up Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_from = '../nbs_artifacts/01.06_g2fc_cluster_genotypes/'\n",
    "\n",
    "split_info = read_split_info(\n",
    "    load_from = '../nbs_artifacts/01.06_g2fc_cluster_genotypes/',\n",
    "    json_prefix = '2023:9:5:12:8:26')\n",
    "\n",
    "temp = phno.copy()\n",
    "temp[['Female', 'Male']] = temp['Hybrid'].str.split('/', expand = True)\n",
    "\n",
    "test_dict = find_idxs_split_dict(\n",
    "    obs_df = temp, \n",
    "    split_dict = split_info['test'][0]\n",
    ")\n",
    "\n",
    "temp = temp.loc[test_dict['train_idx'], ] # restrict before re-aplying\n",
    "\n",
    "val_dict = find_idxs_split_dict(\n",
    "    obs_df = temp, \n",
    "    split_dict = split_info['validate'][0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5916931e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = val_dict['train_idx']\n",
    "test_idx  = val_dict['test_idx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c90807",
   "metadata": {},
   "outputs": [],
   "source": [
    "YMat_cs = calc_cs(YMat[train_idx])\n",
    "y_cs = apply_cs(YMat, YMat_cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d5f603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging Data -----------------------------------------------------------------------------------\n",
    "\n",
    "train_idx = val_dict['train_idx']\n",
    "test_idx  = val_dict['test_idx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc27b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataloader = DataLoader(BigDataset(\n",
    "    y = torch.from_numpy(                                 y_cs[train_idx])[:, None].to(torch.float).to('cuda'),\n",
    "    lookup_obs = torch.from_numpy(np.asarray([i for i in range(train_idx.size)])),\n",
    "    lookup_env = torch.from_numpy(   np.asarray(obs_env_lookup[train_idx])),\n",
    "    W = torch.from_numpy(WMat_hilb).to(torch.float).to('cuda'),\n",
    "    P = torch.from_numpy(PlantHarvest).to(torch.int),\n",
    "    W_type = 'hilbert'\n",
    "    ),\n",
    "    batch_size = 50,\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "validation_dataloader = DataLoader(BigDataset(\n",
    "    y = torch.from_numpy(                                 y_cs[test_idx])[:, None].to(torch.float).to('cuda'),\n",
    "    lookup_obs = torch.from_numpy(np.asarray([i for i in range(test_idx.size)])),\n",
    "    lookup_env = torch.from_numpy(np.asarray(   obs_env_lookup[test_idx])),\n",
    "    W = torch.from_numpy(WMat_hilb).to(torch.float).to('cuda'),\n",
    "    P = torch.from_numpy(PlantHarvest),\n",
    "    W_type = 'hilbert'\n",
    "    ),\n",
    "    batch_size = 50,\n",
    "    shuffle = False \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21aaeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ResNet2d(\n",
    "#         block = BasicBlock2d, #: Type[Union[BasicBlock, Bottleneck]],\n",
    "#         layers = [2, 2, 2, 2], #: List[int],\n",
    "#         # num_classes: int = 1000,\n",
    "#         zero_init_residual = False,\n",
    "#         groups = 1,\n",
    "#         width_per_group = 64,\n",
    "#         replace_stride_with_dilation = None,\n",
    "#         norm_layer = None,\n",
    "#         input_channels = 17\n",
    "#     ).to('cuda')(next(iter(training_dataloader))[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cdc46f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69476140",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Module for training subnetworks.\n",
    "class plDNN_wthr(pl.LightningModule):\n",
    "    def __init__(self, mod):\n",
    "        super().__init__()\n",
    "        self.mod = mod\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        y_i, w_i = batch\n",
    "        pred = self.mod(w_i)\n",
    "        loss = F.mse_loss(pred, y_i)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            weight_list=[(name, param) for name, param in model.named_parameters() if name.split('.')[-1] == 'weight']\n",
    "            for l in weight_list:\n",
    "                self.log((\"train_mean\"+l[0]), l[1].mean())\n",
    "                self.log((\"train_std\"+l[0]), l[1].std())        \n",
    "        return(loss)\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        y_i, w_i = batch\n",
    "        pred = self.mod(w_i)\n",
    "        loss = F.mse_loss(pred, y_i)\n",
    "        self.log('val_loss', loss)        \n",
    "     \n",
    "    def configure_optimizers(self, **kwargs):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), **kwargs)\n",
    "        return optimizer    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fafb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet2d(\n",
    "        block = BasicBlock2d, #: Type[Union[BasicBlock, Bottleneck]],\n",
    "        layers = [2, 2, 2, 2], #: List[int],\n",
    "        # num_classes: int = 1000,\n",
    "        zero_init_residual = False,\n",
    "        groups = 1,\n",
    "        width_per_group = 64,\n",
    "        replace_stride_with_dilation = None,\n",
    "        norm_layer = None,\n",
    "        input_channels = 17\n",
    "    ).to('cuda')\n",
    "\n",
    "\n",
    "max_epoch = 10\n",
    "DNNW = plDNN_wthr(model)     \n",
    "optimizer = DNNW.configure_optimizers()\n",
    "\n",
    "logger = TensorBoardLogger(\"tb_logs\", name=\"w-hilb-res-4rep2-from-pytorch\")\n",
    "trainer = pl.Trainer(max_epochs=max_epoch, logger=logger)\n",
    "\n",
    "trainer.fit(model=DNNW, train_dataloaders=training_dataloader, val_dataloaders=validation_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b1ba16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kickd/miniconda3/envs/fastai/lib/python3.11/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/kickd/miniconda3/envs/fastai/lib/python3.11/si ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Missing logger folder: tb_logs/w-hilb-res-4rep4-from-pytorch\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name | Type     | Params\n",
      "----------------------------------\n",
      "0 | mod  | ResNet2d | 23.8 M\n",
      "----------------------------------\n",
      "23.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "23.8 M    Total params\n",
      "95.049    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22a1efe9d65e4013b06dcb47caba62e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kickd/miniconda3/envs/fastai/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "/home/kickd/miniconda3/envs/fastai/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "779a216caf474bdf950169cacbacbf86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2e5e682c57c44a5971b634b66c84c2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a44724755c64ca0ac15d516ce1c1f1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84f1a878189d4964a292b6f451995569",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "521abb323cdd4c00895bd830a5767472",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1c8713a370d4e6e8e3f7f4b5968d7ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e3ec492052b41d79713f267bc615055",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c5b2674ba6c4971a6df72563c08055c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35dfc5b34ee0463492dbcd89ad8f1b68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa7a16e86bc1481a8e6e5659cb190e4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6043be7abc314b3b87ec1722dcfaf49b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "model = ResNet2d(\n",
    "        block = BasicBlock2d, #: Type[Union[BasicBlock, Bottleneck]],\n",
    "        layers = [4, 4, 4, 4], #: List[int],\n",
    "        # num_classes: int = 1000,\n",
    "        zero_init_residual = False,\n",
    "        groups = 1,\n",
    "        width_per_group = 64,\n",
    "        replace_stride_with_dilation = None,\n",
    "        norm_layer = None,\n",
    "        input_channels = 17\n",
    "    ).to('cuda')\n",
    "\n",
    "\n",
    "max_epoch = 10\n",
    "DNNW = plDNN_wthr(model)     \n",
    "optimizer = DNNW.configure_optimizers()\n",
    "\n",
    "logger = TensorBoardLogger(\"tb_logs\", name=\"w-hilb-res-4rep4-from-pytorch\")\n",
    "trainer = pl.Trainer(max_epochs=max_epoch, logger=logger)\n",
    "\n",
    "trainer.fit(model=DNNW, train_dataloaders=training_dataloader, val_dataloaders=validation_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
