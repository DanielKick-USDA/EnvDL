{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6356970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacky way to schedule. Here I'm setting these to sleep until the gpus should be free.\n",
    "# At the end of the notebooks  os._exit(00) will kill the kernel freeing the gpu. \n",
    "#                          Hours to wait\n",
    "# import time; time.sleep( 3 * (24*60*60))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aadccc0",
   "metadata": {},
   "source": [
    "# G only KEGG based network architecture\n",
    "\n",
    "> The important change here is to use two linear/drop/relu repeats per module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc2f33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, json\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "pio.templates.default = \"plotly_white\"\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import torch.nn.functional as F # F.mse_loss\n",
    "\n",
    "from EnvDL.core import * \n",
    "# includes \n",
    "    # remove_matching_files \n",
    "    # read_json\n",
    "from EnvDL.dna import *\n",
    "from EnvDL.dlfn import * \n",
    "# includes\n",
    "    # read_split_info \n",
    "    # find_idxs_split_dict\n",
    "    # train_loop_yx\n",
    "    # train_error_yx\n",
    "    # test_loop_yx\n",
    "    # train_nn_yx\n",
    "    # yhat_loop_yx\n",
    "    \n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "\n",
    "from graphviz import Digraph\n",
    "# import torchviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce01a01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu_num = 0\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if use_gpu_num in [0, 1]: \n",
    "    torch.cuda.set_device(use_gpu_num)\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fba3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_path = '../nbs_artifacts/02.30_g2fc_G_ACGT_VNN.1/'\n",
    "save_prefix = [e for e in cache_path.split('/') if e != ''][-1]\n",
    "ensure_dir_path_exists(dir_path = cache_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f56cc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings:\n",
    "## For graph linear blocks\n",
    "default_output_size = 50\n",
    "default_dropout_pr = 0.1\n",
    "default_block_reps = 2\n",
    "\n",
    "## Training settings\n",
    "run_epochs = 15\n",
    "epochs_run = 0\n",
    "dataloader_batch_size = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff84bdd0",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547c1e1f",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6b569e",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_from = '../nbs_artifacts/01.03_g2fc_prep_matrices/'\n",
    "phno_geno = pd.read_csv(load_from+'phno_geno.csv')\n",
    "phno = phno_geno\n",
    "\n",
    "obs_geno_lookup = np.load(load_from+'obs_geno_lookup.npy') # Phno_Idx\tGeno_Idx\tIs_Phno_Idx\n",
    "YMat = np.load(load_from+'YMat.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4299f2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing using the cleaned up version in '../nbs_artifacts/01.05_g2fc_demo_model/'\n",
    "load_from = '../nbs_artifacts/01.05_g2fc_demo_model/'\n",
    "parsed_kegg_gene_entries = get_cached_result(load_from+'filtered_kegg_gene_entries.pkl')\n",
    "\n",
    "ACGT_gene_slice_list = get_cached_result(load_from+'ACGT_gene_slice_list.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc4d1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(ACGT_gene_slice_list) == len(parsed_kegg_gene_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d62f9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create train/test validate indicies from json\n",
    "load_from = '../nbs_artifacts/01.06_g2fc_cluster_genotypes/'\n",
    "\n",
    "split_info = read_split_info(\n",
    "    load_from = '../nbs_artifacts/01.06_g2fc_cluster_genotypes/',\n",
    "    json_prefix = '2023:9:5:12:8:26')\n",
    "\n",
    "temp = phno.copy()\n",
    "temp[['Female', 'Male']] = temp['Hybrid'].str.split('/', expand = True)\n",
    "\n",
    "test_dict = find_idxs_split_dict(\n",
    "    obs_df = temp, \n",
    "    split_dict = split_info['test'][0]\n",
    ")\n",
    "# test_dict\n",
    "\n",
    "# since this is applying predefined model structure no need for validation.\n",
    "# This is included for my future reference when validation is needed.\n",
    "temp = temp.loc[test_dict['train_idx'], ] # restrict before re-aplying\n",
    "\n",
    "val_dict = find_idxs_split_dict(\n",
    "    obs_df = temp, \n",
    "    split_dict = split_info['validate'][0]\n",
    ")\n",
    "# val_dict\n",
    "\n",
    "# test_dict\n",
    "\n",
    "train_idx = test_dict['train_idx']\n",
    "test_idx  = test_dict['test_idx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fa7327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # FIXME\n",
    "# train_idx = train_idx[0:100] # 81169 total\n",
    "# test_idx = test_idx[0:100]\n",
    "# # 1/100th\n",
    "# train_idx = train_idx[0:3000] # 81169 total\n",
    "# test_idx = test_idx[0:3000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bfb7bb",
   "metadata": {},
   "source": [
    "## Generate Graph for DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6afc90",
   "metadata": {},
   "source": [
    "### Functions for Graph Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b924e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Building a Neural Net from an arbitrary graph\n",
    "# start by finding the top level -- all those keys which are theselves not values\n",
    "# helper function to get all keys and all value from a dict. Useful for when keys don't have unique values.\n",
    "def find_uniq_keys_values(input_dict):\n",
    "    all_keys = list(input_dict.keys())\n",
    "    all_values = []\n",
    "    for e in all_keys:\n",
    "        all_values.extend(input_dict[e])\n",
    "    all_values = list(set(all_values))\n",
    "\n",
    "    return({'all_keys': all_keys,\n",
    "           'all_values': all_values})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd645ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Find order that nodes in the graph should be called to have all dependencies run when they are called.\n",
    "# find the dependancies for run order from many dependancies to none\n",
    "# wrapper function to find the nodes that aren't any other nodes dependancies.\n",
    "def find_top_nodes(all_key_value_dict):\n",
    "    return([e for e in all_key_value_dict['all_keys'] if e not in all_key_value_dict['all_values']])\n",
    "# wrapper function to find the input nodes. They don't occur in the keys and thus won't be added to the list otherwise.\n",
    "# another way to do this would have been to \n",
    "def find_input_nodes(all_key_value_dict):\n",
    "    return([e for e in all_key_value_dict['all_values'] if e not in all_key_value_dict['all_keys']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd67297",
   "metadata": {},
   "source": [
    "### Process paths into graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8edc4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restrict to only those with pathway\n",
    "kegg_gene_brite = [e for e in parsed_kegg_gene_entries if 'BRITE' in e.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb791b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# also require to have a non-empty path\n",
    "kegg_gene_brite = [e for e in kegg_gene_brite if not e['BRITE']['BRITE_PATHS'] == []]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9f3eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Retaining '+ str(round(len(kegg_gene_brite)/len(parsed_kegg_gene_entries), 4)*100)+'%, '+str(len(kegg_gene_brite)\n",
    "     )+'/'+str(len(parsed_kegg_gene_entries)\n",
    "     )+' Entries'\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d365781b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kegg_gene_brite[1]['BRITE']['BRITE_PATHS']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bea378",
   "metadata": {},
   "source": [
    "### Tiny `n`  gene version of the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b38dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_genes = 6067 \n",
    "\n",
    "print('Using '+str(n_genes)+'/'+str(len(kegg_gene_brite))+' genes.')\n",
    "\n",
    "# if n_genes is too big, don't visualize.\n",
    "vis_dot_bool = True\n",
    "if n_genes > 10:\n",
    "    vis_dot_bool = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc8ce75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The goal here is to have a dict with each node and a list of it's children. \n",
    "For example, the graph\n",
    "a--b--d\n",
    " |-c--e\n",
    "Would be parsed into     \n",
    "{'a':['b', 'c'],\n",
    " 'b':['d'],\n",
    " 'c':['e']}\n",
    "\"\"\"\n",
    "kegg_connections = {}\n",
    "\n",
    "# for all genes in list\n",
    "for i in tqdm(range(n_genes)): \n",
    "# for i in tqdm(range(len(kegg_gene_brite))):    \n",
    "    temp = kegg_gene_brite[i]['BRITE']['BRITE_PATHS']\n",
    "    # clean up to make sure that there are no \":\" characters. These can mess up graphviz\n",
    "    temp = [[temp[j][i].replace(':', '-') for i in range(len(temp[j])) ] for j in range(len(temp))]\n",
    "    # all paths through graph associated with a gene\n",
    "    for j in range(len(temp)):\n",
    "        # steps of the path through the graph\n",
    "        for k in range(len(temp[j])-1):\n",
    "            \n",
    "            # name standardization \n",
    "            temp_jk  = temp[j][k]\n",
    "            temp_jk1 = temp[j][k+1]\n",
    "            temp_jk  = temp_jk.lower().title().replace(' ', '')\n",
    "            temp_jk1 = temp_jk1.lower().title().replace(' ', '')\n",
    "            \n",
    "            # if this is a new key, add it and add the k+1 entry as it's child\n",
    "            if temp_jk  not in kegg_connections.keys():\n",
    "                kegg_connections[temp_jk] = [temp_jk1]\n",
    "            else: \n",
    "                # Check to see if there's a new child to add   \n",
    "                if temp_jk1 not in kegg_connections[temp_jk]:\n",
    "                    # make sure that no key contains itself. This was a problem for 'Others' which is now disallowed.\n",
    "                    if (temp_jk != temp_jk1):\n",
    "#                         if ((temp_jk  != temp_jk1) & (temp_jk1 != 'Others')):\n",
    "                        # add it.\n",
    "                        kegg_connections[temp_jk].extend([temp_jk1])\n",
    "\n",
    "# kegg_connections                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c110e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Others' in kegg_connections.keys():\n",
    "    del kegg_connections['Others']\n",
    "    print('Removed node \"Others\"')\n",
    "\n",
    "# remove 'Others' as a possible value\n",
    "for key in kegg_connections.keys():\n",
    "    kegg_connections[key] = [e for e in kegg_connections[key] if e != 'Others']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1c1f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that no list contains it's own key\n",
    "for key in kegg_connections.keys():\n",
    "    kegg_connections[key] = [e for e in kegg_connections[key] if e != key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66ec6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there might be associations with no dependants and with no dependants except those that have no dependants.\n",
    "# Build up a list with those keys that don't connect back to snps then I'll pass over the connection dict once to remove references to them.\n",
    "rm_list = []\n",
    "rm_list_i = len(rm_list)\n",
    "rm_list_j = -1\n",
    "for i in range(100):\n",
    "    if rm_list_i == rm_list_j:\n",
    "        break\n",
    "    else:\n",
    "        rm_list = [key for key in kegg_connections.keys() if [e for e in kegg_connections[key] if e not in rm_list]\n",
    "     ==[]]\n",
    "        rm_list_j = rm_list_i \n",
    "        rm_list_i = len(rm_list)\n",
    "rm_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057b2e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in rm_list:\n",
    "    del kegg_connections[key]\n",
    "    \n",
    "for key in kegg_connections.keys():\n",
    "    kegg_connections[key] = [e for e in kegg_connections[key] if e not in rm_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c8cc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add yhat node to the graph\n",
    "temp_values = []\n",
    "for key in kegg_connections.keys():\n",
    "    temp_values += kegg_connections[key]\n",
    "\n",
    "kegg_connections['y_hat'] = [key for key in kegg_connections.keys() if key not in temp_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597288ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is too big to render in full\n",
    "dot = ''\n",
    "if vis_dot_bool:\n",
    "    dot = Digraph()\n",
    "    for key in tqdm(kegg_connections.keys()):\n",
    "        dot.node(key)\n",
    "        for value in kegg_connections[key]:\n",
    "            # edge takes a head/tail whereas edges takes name pairs concatednated (A, B -> AB)in a list\n",
    "            dot.edge(value, key)    \n",
    "\n",
    "dot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1e6077",
   "metadata": {},
   "source": [
    "Version with the node names masked for size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ddd86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dot = ''\n",
    "if vis_dot_bool:\n",
    "    name_to_num_dict = dict(zip(list(kegg_connections.keys()),\n",
    "                                [str(i) for i in range(len(list(kegg_connections.keys())))]))\n",
    "\n",
    "    temp = {}\n",
    "    for key in kegg_connections.keys():\n",
    "        temp[name_to_num_dict[key]] = [name_to_num_dict[e] if e in name_to_num_dict.keys() else e for e in kegg_connections[key]]\n",
    "\n",
    "    dot = Digraph()\n",
    "    for key in tqdm(temp.keys()):\n",
    "        dot.node(key)\n",
    "        for value in temp[key]:\n",
    "            # edge takes a head/tail whereas edges takes name pairs concatednated (A, B -> AB)in a list\n",
    "            dot.edge(value, key)    \n",
    "\n",
    "    # dot.render(directory=cache_path, view=True) \n",
    "dot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cb7324",
   "metadata": {},
   "source": [
    "### Setup to build the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d96f8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start by finding the top level -- all those keys which are theselves not values\n",
    "res = find_uniq_keys_values(input_dict = kegg_connections)\n",
    "all_keys = res['all_keys']\n",
    "all_values = res['all_values']\n",
    "\n",
    "# use the keys to find the input/outputs of the graph\n",
    "output_nodes = [e for e in all_keys if e not in all_values]\n",
    "input_nodes = [e for e in all_values if e not in all_keys]\n",
    "\n",
    "# (output_nodes, input_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc6c050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the dependancies for run order from many dependancies to none\n",
    "temp = kegg_connections.copy()\n",
    "\n",
    "no_dependants = find_input_nodes(all_key_value_dict = find_uniq_keys_values(input_dict = temp))\n",
    "# first pass. Same as the output nodes identified above\n",
    "dependancy_order = []\n",
    "# Then iterate\n",
    "for ith in range(100): #TODO <- this should be set as a input parameter\n",
    "    top_nodes = find_top_nodes(all_key_value_dict = find_uniq_keys_values(input_dict = temp))\n",
    "    if top_nodes == []:\n",
    "        break\n",
    "    else:\n",
    "        dependancy_order += top_nodes    \n",
    "        # remove nodes from the graph that are at the 'top' level and haven't already been removed\n",
    "        for key in [e for e in dependancy_order if e in temp.keys()]:\n",
    "             temp.pop(key)\n",
    "\n",
    "# dependancy_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8251172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse to get the order that the nodes should be called\n",
    "dependancy_order.reverse()\n",
    "# dependancy_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272f38f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying out new approach: add a node for the input data tha will only flatten the input.\n",
    "dependancy_order = input_nodes+dependancy_order\n",
    "\n",
    "for key in input_nodes:\n",
    "    kegg_connections[key] = [] #[key] # needs to contain itself so the model's `get_input_node()` function works \n",
    "                               # or that function needs to change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53901bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a dict to go from the node names in `no_dependants` to the list index in `ACGT_gene_slice_list`\n",
    "brite_node_to_list_idx_dict = {}\n",
    "for i in tqdm(range(len(kegg_gene_brite))):\n",
    "    brite_node_to_list_idx_dict[str(kegg_gene_brite[i]['BRITE']['BRITE_PATHS'][0][-1])] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e209df06",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor_dict = {}\n",
    "for e in no_dependants:\n",
    "    input_tensor_dict[e] = ACGT_gene_slice_list[brite_node_to_list_idx_dict[e]]\n",
    "    \n",
    "# input_tensor_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca643b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure out expected input/output shapes\n",
    "#==NOTE! This assumes only dense connections!==\n",
    "\n",
    "# This could be replaced by a sort of \"distance from output\" measure\n",
    "output_size_dict = dict(zip(dependancy_order, \n",
    "                        [default_output_size for i in range(len(dependancy_order))]))\n",
    "output_size_dict['y_hat'] = 1 \n",
    "# output_size_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e945e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup dropout % dictionary\n",
    "dropout_pr_dict = dict(zip(dependancy_order, \n",
    "                        [default_dropout_pr for i in range(len(dependancy_order))]))\n",
    "dropout_pr_dict['y_hat'] = 0 # not required, output node is purely linear without dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adedd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup replicates of layers dictionary\n",
    "block_rep_dict = dict(zip(dependancy_order, \n",
    "                        [default_block_reps for i in range(len(dependancy_order))]))\n",
    "block_rep_dict['y_hat'] = 1 # not required, output node is purely linear. Not a linear block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4703e891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANNEL AWARE VERSION -----------------------------------------------------------------------------------\n",
    "input_size_dict = kegg_connections.copy()\n",
    "\n",
    "# use the expected output sizes from `output_size_dict` to fill in the non-data sizes\n",
    "tensor_ndim = len(input_tensor_dict[list(input_tensor_dict.keys())[0]].shape)\n",
    "for e in tqdm(input_size_dict.keys()):\n",
    "    # overwrite named connections with the output size of those connections\n",
    "    # if the entry is in no_dependants it's data so it's size needs to be grabbed from the input_tensor_dict\n",
    "    \n",
    "    # is there no channel dim? (major/minor allele)\n",
    "    if 2 == tensor_ndim:\n",
    "        input_size_dict[e] = [\n",
    "            list(input_tensor_dict[ee].shape)[-1] # <- NOTE! THIS ASSUMES ONLY DENSE CONNECTIONS (i.e. only the 1st dim is needed)\n",
    "            if ee in no_dependants\n",
    "            else output_size_dict[ee] for ee in input_size_dict[e]]\n",
    "    elif 3 == tensor_ndim: # There is a channel dim\n",
    "        input_size_dict[e] = [\n",
    "            (list(input_tensor_dict[ee].shape)[1]*list(input_tensor_dict[ee].shape)[2]) # <- NOTE! THIS ASSUMES ONLY DENSE CONNECTIONS (i.e. only the 1st dim is needed)  \n",
    "            if ee in no_dependants\n",
    "            else output_size_dict[ee] for ee in input_size_dict[e]]\n",
    "\n",
    "# Now walk over entries and overwrite with the sum of the inputs\n",
    "for e in tqdm(input_size_dict.keys()):\n",
    "    input_size_dict[e] = np.sum(input_size_dict[e])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d9fb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "dot = ''\n",
    "if vis_dot_bool:\n",
    "    dot = Digraph()\n",
    "    for key in tqdm(kegg_connections.keys()):\n",
    "        key_label = 'in: '+str(input_size_dict[key])+'\\nout: '+str(output_size_dict[key])\n",
    "        dot.node(key, key_label)\n",
    "        for value in kegg_connections[key]:\n",
    "            # edge takes a head/tail whereas edges takes name pairs concatednated (A, B -> AB)in a list\n",
    "            dot.edge(value, key)    \n",
    "\n",
    "dot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9ebaea",
   "metadata": {},
   "source": [
    "## Set up DataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a550a369",
   "metadata": {},
   "source": [
    "Now we have\n",
    "\n",
    "- A dictionary with the connections: `example_dict`\n",
    "- The expected input sizes for each node: `example_dict_input_size`\n",
    "- A dictionary with the input tensors: `input_tensor_dict`\n",
    "- A list of the input tensors' names: `no_dependants` \n",
    "- A list with the order that each module should be called: `dependancy_order`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e4e20e",
   "metadata": {},
   "source": [
    "To have a fair test of whether the model is working, I want to ensure there is information to learn in the dataset. To this end I'm using just two genotypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5656cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_list_temp = [torch.from_numpy(input_tensor_dict[key]).to(torch.float) for key in input_tensor_dict.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b93408",
   "metadata": {},
   "outputs": [],
   "source": [
    "YMat_cs = calc_cs(YMat[train_idx])\n",
    "y_cs = apply_cs(YMat, YMat_cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529581bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_temp = torch.from_numpy(y_cs).to(torch.float)#[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e82908",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_list_temp[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c21850c",
   "metadata": {},
   "source": [
    "## Set up NeuralNetwork, Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb35a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working version ====\n",
    "# Doesn't pass output node through relu\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, \n",
    "                 example_dict, # contains the node (excluding input tensors)\n",
    "                 example_dict_input_size, # contains the input sizes (including the tensors)\n",
    "                 example_dict_output_size,\n",
    "                 example_dict_dropout_pr,\n",
    "                 example_block_rep_dict,\n",
    "                 input_tensor_names,\n",
    "                 dependancy_order\n",
    "                ):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        def Linear_block(in_size, out_size, drop_pr, block_reps):\n",
    "            block_list = []\n",
    "            for i in range(block_reps):\n",
    "                if i == 0:\n",
    "                    block_list += [\n",
    "                        nn.Linear(in_size, out_size),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(drop_pr)]\n",
    "                else:\n",
    "                    block_list += [\n",
    "                        nn.Linear(out_size, out_size),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(drop_pr)]\n",
    "        \n",
    "            block = nn.ModuleList(block_list)\n",
    "            return(block)           \n",
    "        \n",
    "        # fill in the list in dependancy order. \n",
    "        layer_list = []\n",
    "        for key in dependancy_order:\n",
    "            if key in input_tensor_names:\n",
    "                layer_list += [\n",
    "                    nn.Flatten()\n",
    "                ]\n",
    "            elif key != 'y_hat':\n",
    "                layer_list += [\n",
    "                    Linear_block(in_size=example_dict_input_size[key], \n",
    "                                 out_size=example_dict_output_size[key], \n",
    "                                 drop_pr=example_dict_dropout_pr[key],\n",
    "                                 block_reps=example_block_rep_dict[key])\n",
    "                              ]\n",
    "            else:\n",
    "                layer_list += [\n",
    "                    nn.Linear(example_dict_input_size[key], \n",
    "                              example_dict_output_size[key])\n",
    "                              ]\n",
    "                \n",
    "\n",
    "        self.nn_layer_list = nn.ModuleList(layer_list)\n",
    "\n",
    "        # things for get_input_node in forward to work.\n",
    "        self.example_dict = example_dict\n",
    "        self.input_tensor_names = input_tensor_names\n",
    "        self.dependancy_order = dependancy_order\n",
    "        \n",
    "        self.input_tensor_lookup = dict(zip(input_tensor_names, \n",
    "                                            [i for i in range(len(input_tensor_names))]))\n",
    "        self.result_list = []\n",
    "        self.result_list_lookup = {}\n",
    "            \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Note: x will be a list. input_tensor_lookup will contain the name: list index pairs.\n",
    "        # I use a dict instead of a list comprehension here because there could be an arbitrarily\n",
    "        # large number of inputs in the list. \n",
    "        def get_input_node(self, input_node, get_x):  \n",
    "#             print(input_node, self.result_list_lookup)\n",
    "            return(self.result_list[self.result_list_lookup[input_node]])\n",
    "        \n",
    "        # trying reinstantiating to get around inplace replacement issue.\n",
    "        self.result_list = []\n",
    "        self.result_list_lookup = {}\n",
    "        for key in self.dependancy_order:\n",
    "            input_nodes = self.example_dict[key]\n",
    "            nn_layer_list_idx = [i for i in range(len(dependancy_order)) if dependancy_order[i]==key][0]\n",
    "            \n",
    "            self.result_list_lookup[key] = len(self.result_list_lookup)                \n",
    "            if key in self.input_tensor_names: # If the input node is an input (flatten) layer\n",
    "                self.result_list = self.result_list + [self.nn_layer_list[nn_layer_list_idx](\n",
    "                    x[self.input_tensor_lookup[key]]\n",
    "                ).clone()]\n",
    "\n",
    "            elif key != 'y_hat':\n",
    "                # refactored to handle module lists (even if module list contains only one entry)\n",
    "                out = torch.concat(\n",
    "                    [get_input_node(self, input_node = e, get_x = x) for e in input_nodes], \n",
    "                    -1)\n",
    "            \n",
    "                for module in self.nn_layer_list[nn_layer_list_idx]:\n",
    "                    out = module(out)\n",
    "        \n",
    "                self.result_list = self.result_list + [out] \n",
    "            \n",
    "            else:\n",
    "                self.result_list = self.result_list + [self.nn_layer_list[nn_layer_list_idx](torch.concat(\n",
    "                    [get_input_node(self, input_node = e, get_x = x) for e in input_nodes], \n",
    "                    -1)).clone()]            \n",
    "\n",
    "        return self.result_list[self.result_list_lookup['y_hat']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3061d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = NeuralNetwork(example_dict = kegg_connections, \n",
    "#                       example_dict_input_size = input_size_dict,\n",
    "#                       example_dict_output_size = output_size_dict,\n",
    "#                       example_dict_dropout_pr= dropout_pr_dict,\n",
    "#                       example_block_rep_dict = block_rep_dict,\n",
    "#                       input_tensor_names = list(input_tensor_dict.keys()),\n",
    "#                       dependancy_order = dependancy_order) \n",
    "# model = model.to(device)\n",
    "# model(next(iter(training_dataloader))[1])\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b56467",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ListDataset(Dataset): # for any G containing matix with many (phno) to one (geno)\n",
    "    def __init__(self, \n",
    "                 y, \n",
    "                 x_list,\n",
    "                 obs_idxs, # this is a list of the indexes used. It allows us to pass in smaller \n",
    "                           # tensors and then get the right genotype\n",
    "                 obs_geno_lookup,\n",
    "                 transform = None, target_transform = None,\n",
    "                 **kwargs \n",
    "                ):\n",
    "        self.device = device\n",
    "        self.y = y \n",
    "        self.x_list = x_list\n",
    "        self.obs_idxs = obs_idxs\n",
    "        self.obs_geno_lookup = obs_geno_lookup\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform    \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        y_idx =self.y[idx]\n",
    "        \n",
    "        idx_geno = obs_geno_lookup[self.obs_idxs[idx], 1]\n",
    "        x_idx =[x[idx_geno, ] for x in self.x_list] \n",
    "        \n",
    "        if self.target_transform:\n",
    "            y_idx = self.transform(y_idx)\n",
    "            x_idx = [self.transform(x) for x in x_idx]\n",
    "            \n",
    "        return y_idx, x_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b34f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataloader = DataLoader(ListDataset(\n",
    "        y = y_temp[train_idx][:, None].to('cuda'),\n",
    "        x_list = [e.to('cuda') for e in x_list_temp],\n",
    "        obs_idxs = train_idx, \n",
    "        obs_geno_lookup = obs_geno_lookup\n",
    "    ),\n",
    "    batch_size = dataloader_batch_size,\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "validation_dataloader = DataLoader(ListDataset(\n",
    "        y = y_temp[test_idx][:, None].to('cuda'),\n",
    "        x_list = [e.to('cuda') for e in x_list_temp],\n",
    "        obs_idxs = test_idx, \n",
    "        obs_geno_lookup = obs_geno_lookup\n",
    "    ),\n",
    "    batch_size = dataloader_batch_size,\n",
    "    shuffle = False \n",
    ")\n",
    "\n",
    "# next(iter(training_dataloader))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996c1c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(iter(training_dataloader))[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db51906",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(example_dict = kegg_connections, \n",
    "                      example_dict_input_size = input_size_dict,\n",
    "                      example_dict_output_size = output_size_dict,\n",
    "                      example_dict_dropout_pr= dropout_pr_dict,\n",
    "                      example_block_rep_dict = block_rep_dict,\n",
    "                      input_tensor_names = list(input_tensor_dict.keys()),\n",
    "                      dependancy_order = dependancy_order)\n",
    "\n",
    "\n",
    "model.to('cuda')\n",
    "# LSUV_(model, data = next(iter(training_dataloader))[1])\n",
    "# model(next(iter(training_dataloader))[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640349cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad25d25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class plVNN(pl.LightningModule):\n",
    "    def __init__(self, mod):\n",
    "        super().__init__()\n",
    "        self.mod = mod\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        y_i, xs_i = batch\n",
    "#         pred, out = self.mod(xs_i)\n",
    "        pred = self.mod(xs_i)\n",
    "        loss = F.mse_loss(pred, y_i)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             weight_list=[(name, param) for name, param in model.named_parameters() if name.split('.')[-1] == 'weight']\n",
    "#             for l in weight_list:\n",
    "#                 self.log((\"train_mean\"+l[0]), l[1].mean())\n",
    "#                 self.log((\"train_std\"+l[0]), l[1].std())        \n",
    "        return(loss)\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        y_i, xs_i = batch\n",
    "#         pred, out = self.mod(xs_i)\n",
    "        pred = self.mod(xs_i)\n",
    "        loss = F.mse_loss(pred, y_i)\n",
    "        self.log('val_loss', loss)        \n",
    "     \n",
    "    def configure_optimizers(self, **kwargs):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), **kwargs)\n",
    "        return optimizer    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2116f9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2 epochs took 58 minutes so a 3 day weekend would be \n",
    "# 24*3*2=144 epochs\n",
    "max_epoch = 150\n",
    "max_epoch = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199df0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "VNN = plVNN(model)                          # 1. Update\n",
    "# optimizer = VNN.configure_optimizers(lr = lr, betas=(beta1, beta2)) # 2. Update\n",
    "optimizer = VNN.configure_optimizers() # 2. Update\n",
    "\n",
    "logger = TensorBoardLogger(\"tb_vnn_logs\", name=save_prefix+'_no_LSUV')\n",
    "trainer = pl.Trainer(max_epochs=max_epoch, logger=logger)\n",
    "\n",
    "trainer.fit(model=VNN, train_dataloaders=training_dataloader, val_dataloaders=validation_dataloader)       # 4. Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e34e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(VNN.mod, cache_path+'vnn'+'.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71a34c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "os._exit(00)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
