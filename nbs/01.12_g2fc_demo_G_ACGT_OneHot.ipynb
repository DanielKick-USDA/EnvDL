{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aadccc0",
   "metadata": {},
   "source": [
    "# G only ACGT OneHot Encoded\n",
    "\n",
    "> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e18fbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacky way to schedule. Here I'm setting these to sleep until the gpus should be free.\n",
    "# At the end of the notebooks  os._exit(00) will kill the kernel freeing the gpu. \n",
    "#                          Hours to wait\n",
    "# import time; time.sleep( 6 * (60*60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ca844b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "pio.templates.default = \"plotly_white\"\n",
    "\n",
    "import hilbertcurve\n",
    "from hilbertcurve.hilbertcurve import HilbertCurve\n",
    "\n",
    "from EnvDL.core import * # includes remove_matching_files\n",
    "from EnvDL.dna import *\n",
    "from EnvDL.dlfn import *\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4600ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_path = '../nbs_artifacts/01.12_g2fc_demo_G_ACGT_OneHot/'\n",
    "ensure_dir_path_exists(dir_path = cache_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517af1f8",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e59a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_from = '../nbs_artifacts/01.03_g2fc_prep_matrices/'\n",
    "# phno = pd.read_csv(load_from+'phno.csv')\n",
    "phno_geno = pd.read_csv(load_from+'phno_geno.csv')\n",
    "phno = phno_geno\n",
    "\n",
    "obs_geno_lookup = np.load(load_from+'obs_geno_lookup.npy') # Phno_Idx\tGeno_Idx\tIs_Phno_Idx\n",
    "YMat = np.load(load_from+'YMat.npy')\n",
    "# GMat = np.load(load_from+'GMat.npy')\n",
    "ACGT_OneHot = np.load(load_from+'ACGT_OneHot.npy')\n",
    "# ACGT = np.load(load_from+'ACGT.npy')\n",
    "# ACGT_hilb = np.load(load_from+'ACGT_hilb.npy')\n",
    "# SMat = np.load(load_from+'SMat3.npy')\n",
    "# WMat = np.load(load_from+'WMat3.npy')\n",
    "# MMat = np.load(load_from+'MMat3.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5238dd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader_batch_size = 8 #16 #64\n",
    "# run_epochs = 200\n",
    "\n",
    "use_gpu_num = 0\n",
    "\n",
    "# Imports --------------------------------------------------------------------\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if use_gpu_num in [0, 1]: \n",
    "    torch.cuda.set_device(use_gpu_num)\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02205e69",
   "metadata": {},
   "source": [
    "## One Hot Encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7731c169",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(9230473) # note, must use rng.shuffle(arr) below for this to take effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a91e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_cumsum_thresh = .9\n",
    "\n",
    "# make a df to aid in creating train/test splits\n",
    "# the plan is to shuffle the rows of the df, calculate the cumulative sum of the percents obs, then \n",
    "# the entries above and below a given percent will be the train/test.\n",
    "obs_per_hybrid = phno.assign(n = 1).groupby('Hybrid').count().reset_index()\n",
    "obs_per_hybrid = obs_per_hybrid.loc[:, ['Hybrid', 'n']]\n",
    "obs_per_hybrid['pct'] = obs_per_hybrid.n / obs_per_hybrid.n.sum()\n",
    "obs_per_hybrid['pct_cumsum'] = np.nan\n",
    "obs_per_hybrid['random_order'] = 0\n",
    "\n",
    "obs_per_hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ca1bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in the random values to sort on\n",
    "arr = np.arange(obs_per_hybrid.shape[0])\n",
    "rng.shuffle(arr)\n",
    "obs_per_hybrid.random_order = arr\n",
    "\n",
    "obs_per_hybrid = obs_per_hybrid.sort_values('random_order')\n",
    "obs_per_hybrid['pct_cumsum'] = obs_per_hybrid.pct.cumsum()\n",
    "obs_per_hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bce9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert back into phno indices\n",
    "train_hybrids = list(obs_per_hybrid.loc[(obs_per_hybrid.pct_cumsum <= pct_cumsum_thresh), 'Hybrid'])\n",
    "test_hybrids  = list(obs_per_hybrid.loc[(obs_per_hybrid.pct_cumsum >  pct_cumsum_thresh), 'Hybrid'])\n",
    "\n",
    "train_idx = phno.loc[(phno.Hybrid.isin(train_hybrids)), ].index\n",
    "test_idx  = phno.loc[(phno.Hybrid.isin(test_hybrids)), ].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d0163e",
   "metadata": {},
   "outputs": [],
   "source": [
    "[len(train_idx), len(test_idx)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbaedf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm all observation idxs are have genomic information\n",
    "assert [] == [e for e in list(train_idx)+list(test_idx) if e not in obs_geno_lookup[:, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02004189",
   "metadata": {},
   "outputs": [],
   "source": [
    "YMat_cs = calc_cs(YMat[train_idx])\n",
    "y_cs = apply_cs(YMat, YMat_cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d9cd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataloader = DataLoader(\n",
    "    ACGTDataset(y = torch.from_numpy(y_cs[train_idx])[:, None].to(torch.float), \n",
    "                G = torch.from_numpy(ACGT_OneHot).to(torch.float), \n",
    "                idx_original = torch.from_numpy(np.array(train_idx)),\n",
    "                idx_lookup   = torch.from_numpy(np.asarray(obs_geno_lookup)),\n",
    "                use_gpu_num = 0,\n",
    "                device = 'cuda'\n",
    "               ),\n",
    "    batch_size = 50,\n",
    "    shuffle = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08eaaff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_dataloader = DataLoader(\n",
    "    ACGTDataset(y = torch.from_numpy(y_cs[test_idx])[:, None].to(torch.float), \n",
    "                G = torch.from_numpy(ACGT_OneHot).to(torch.float), \n",
    "                idx_original = torch.from_numpy(np.array(test_idx)),\n",
    "                idx_lookup   = torch.from_numpy(np.asarray(obs_geno_lookup)),\n",
    "                use_gpu_num = 0,\n",
    "                device = 'cuda'\n",
    "               ),\n",
    "    batch_size = 50,\n",
    "    shuffle = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b8b1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(training_dataloader))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788ba1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()    \n",
    "\n",
    "        def Linear_block(in_size, out_size, drop_pr):\n",
    "            block = nn.Sequential(\n",
    "                nn.Linear(in_size, out_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(drop_pr)\n",
    "            )\n",
    "            return(block)         \n",
    "        \n",
    "        self.x_network = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            Linear_block(in_size = 13*125891, out_size = 512, drop_pr = 0.5),\n",
    "            Linear_block(in_size = 512, out_size = 256, drop_pr = 0.5),\n",
    "            Linear_block(in_size = 256, out_size = 128, drop_pr = 0.5),\n",
    "            Linear_block(in_size = 128, out_size = 32, drop_pr = 0.5),\n",
    "            Linear_block(in_size = 32, out_size = 16, drop_pr = 0.5),\n",
    "            nn.Linear(16, 1)\n",
    "        \n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_out = self.x_network(x)\n",
    "        return x_out\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "\n",
    "model(next(iter(training_dataloader))[0][0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654b3bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # small scale test\n",
    "# model, loss_df = train_nn(\n",
    "#     cache_path,\n",
    "#     training_dataloader,\n",
    "#     testing_dataloader,\n",
    "#     model,\n",
    "#     learning_rate = 1e-3,\n",
    "#     batch_size = 50, #dataloader_batch_size,\n",
    "#     epochs = 1 #(run_epochs - epochs_run)\n",
    "# )\n",
    "\n",
    "# loss_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9cbc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimate_iterations(sec_per_it = 161)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3174cd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_batch_size = 50\n",
    "run_epochs = 100\n",
    "\n",
    "# don't run if either of these exist because there may be cases where we want the results but not the model\n",
    "import re\n",
    "\n",
    "if not os.path.exists(cache_path+'/model.pt'): \n",
    "    # Shared setup (train from scratch and load latest)\n",
    "    model = NeuralNetwork()\n",
    "\n",
    "    # find the biggest model to save\n",
    "    saved_models = os.listdir(cache_path)\n",
    "    saved_models = [e for e in saved_models if re.match('model*', e)]\n",
    "\n",
    "    if saved_models == []:\n",
    "        epochs_run = 0\n",
    "    else:\n",
    "        saved_models = [e for e in saved_models if e != 'model.pt']\n",
    "        # if there are saved models reload and resume training\n",
    "        saved_models_numbers = [int(e.replace('model_', ''\n",
    "                                    ).replace('.pt', ''\n",
    "                                    ).split('_')[0]) for e in saved_models]\n",
    "        # saved_models\n",
    "        epochs_run = max(saved_models_numbers)+1 # add 1 to account for 0 index\n",
    "        latest_model = [e for e in saved_models if re.match(\n",
    "            '^model_'+str(epochs_run-1)+'_.*\\.pt$', e)][0] # subtract 1 to convert back\n",
    "        model.load_state_dict(torch.load(cache_path+latest_model))\n",
    "        print('Resuming Training: '+str(epochs_run)+'/'+str(run_epochs)+' epochs run.')\n",
    "    \n",
    "    model.to(device)    \n",
    "\n",
    "    model, loss_df = train_nn(\n",
    "        cache_path,\n",
    "        training_dataloader,\n",
    "        testing_dataloader,\n",
    "        model,\n",
    "        learning_rate = 1e-3,\n",
    "        batch_size = dataloader_batch_size,\n",
    "        epochs = (run_epochs - epochs_run)\n",
    "    )\n",
    "    \n",
    "    # experimental outputs:\n",
    "    # 1. Model\n",
    "    torch.save(model.state_dict(), cache_path+'/model.pt') # convention is to use .pt or .pth\n",
    "\n",
    "    # 2. loss_df    \n",
    "    # If this is resuming training, load and extend the existing loss dataframe\n",
    "    if os.path.exists(cache_path+'/loss_df.csv'):\n",
    "        loss_df_on_disk = pd.read_csv(cache_path+'/loss_df.csv')\n",
    "        epoch_offset = 1 + loss_df_on_disk['Epoch'].max()\n",
    "        loss_df['Epoch'] = loss_df['Epoch'] + epoch_offset\n",
    "        loss_df = pd.concat([loss_df_on_disk, loss_df])\n",
    "    loss_df.to_csv(cache_path+'/loss_df.csv', index=False)  \n",
    "    \n",
    "    # 3. predictions \n",
    "    yhats = pd.concat([\n",
    "        yhat_loop(testing_dataloader, model).assign(Split = 'Test'),\n",
    "        yhat_loop(training_dataloader, model).assign(Split = 'Train')], axis = 0)\n",
    "\n",
    "    yhats.to_csv(cache_path+'/yhats.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4764b227",
   "metadata": {},
   "source": [
    "### Standard Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad680db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_dict = {'y1':YMat_cs}\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec22f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "naieve_yhat = training_dataloader.dataset.y.mean()\n",
    "\n",
    "naieve_MSE_Train = reverse_cs( \n",
    "    np.array(((naieve_yhat - training_dataloader.dataset.y)**2)).mean(),\n",
    "    scale_dict['y1']\n",
    ")\n",
    "\n",
    "naieve_MSE_Test = reverse_cs( \n",
    "    np.array(((naieve_yhat - testing_dataloader.dataset.y)**2)).mean(),\n",
    "    scale_dict['y1']\n",
    ")\n",
    "\n",
    "naieve_MSE_Train, naieve_MSE_Test\n",
    "\n",
    "\n",
    "\n",
    "loss_df = pd.read_csv(cache_path+'/loss_df.csv')\n",
    "\n",
    "loss_df.TrainMSE = reverse_cs(loss_df.TrainMSE, scale_dict['y1'])\n",
    "loss_df.TestMSE  = reverse_cs(loss_df.TestMSE , scale_dict['y1'])\n",
    "\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=loss_df.Epoch, y=loss_df.TestMSE,\n",
    "                    mode='lines', name='Test'))\n",
    "fig.add_trace(go.Scatter(x=loss_df.Epoch, y=loss_df.TrainMSE,\n",
    "                    mode='lines', name='Train'))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=loss_df.Epoch, y=[naieve_MSE_Test  for e in range(len(loss_df.Epoch))], \n",
    "                         mode='lines', name='Naieve Test'))\n",
    "fig.add_trace(go.Scatter(x=loss_df.Epoch, y=[naieve_MSE_Train for e in range(len(loss_df.Epoch))], \n",
    "                         mode='lines', name='Naieve Train'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b66997",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhats = pd.read_csv(cache_path+'/yhats.csv')\n",
    "\n",
    "# px.scatter(yhats, x = 'y_true', y = 'y_pred', color = 'Split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923b6dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhats.y_true = reverse_cs(yhats.y_true, scale_dict['y1'])\n",
    "yhats.y_pred = reverse_cs(yhats.y_pred, scale_dict['y1'])\n",
    "\n",
    "# px.scatter(yhats, x = 'y_true', y = 'y_pred', color = 'Split', trendline=\"ols\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8460bc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhats['Error'] = yhats.y_pred - yhats.y_true\n",
    "\n",
    "px.histogram(yhats, x = 'Error', color = 'Split',\n",
    "             marginal=\"box\", # can be `rug`, `violin`\n",
    "             nbins= 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156b32d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os._exit(00)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
