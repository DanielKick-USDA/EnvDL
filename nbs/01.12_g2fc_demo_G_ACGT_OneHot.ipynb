{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aadccc0",
   "metadata": {},
   "source": [
    "# G only ACGT OneHot Encoded\n",
    "\n",
    "> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e18fbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacky way to schedule. Here I'm setting these to sleep until the gpus should be free.\n",
    "# At the end of the notebooks  os._exit(00) will kill the kernel freeing the gpu. \n",
    "#                          Hours to wait\n",
    "# import time; time.sleep( 6 * (60*60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ca844b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "pio.templates.default = \"plotly_white\"\n",
    "\n",
    "import hilbertcurve\n",
    "from hilbertcurve.hilbertcurve import HilbertCurve\n",
    "\n",
    "from EnvDL.core import * # includes remove_matching_files\n",
    "from EnvDL.dna import *\n",
    "from EnvDL.dlfn import *\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4600ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_path = '../nbs_artifacts/01.12_g2fc_demo_G_ACGT_OneHot/'\n",
    "ensure_dir_path_exists(dir_path = cache_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517af1f8",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e59a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_from = '../nbs_artifacts/01.03_g2fc_prep_matrices/'\n",
    "# phno = pd.read_csv(load_from+'phno.csv')\n",
    "phno_geno = pd.read_csv(load_from+'phno_geno.csv')\n",
    "phno = phno_geno\n",
    "\n",
    "obs_geno_lookup = np.load(load_from+'obs_geno_lookup.npy') # Phno_Idx\tGeno_Idx\tIs_Phno_Idx\n",
    "YMat = np.load(load_from+'YMat.npy')\n",
    "# GMat = np.load(load_from+'GMat.npy')\n",
    "ACGT_OneHot = np.load(load_from+'ACGT_OneHot.npy')\n",
    "# ACGT = np.load(load_from+'ACGT.npy')\n",
    "# ACGT_hilb = np.load(load_from+'ACGT_hilb.npy')\n",
    "# SMat = np.load(load_from+'SMat3.npy')\n",
    "# WMat = np.load(load_from+'WMat3.npy')\n",
    "# MMat = np.load(load_from+'MMat3.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5238dd15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# dataloader_batch_size = 8 #16 #64\n",
    "# run_epochs = 200\n",
    "\n",
    "use_gpu_num = 0\n",
    "\n",
    "# Imports --------------------------------------------------------------------\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if use_gpu_num in [0, 1]: \n",
    "    torch.cuda.set_device(use_gpu_num)\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02205e69",
   "metadata": {},
   "source": [
    "## One Hot Encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7731c169",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(9230473) # note, must use rng.shuffle(arr) below for this to take effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a91e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hybrid</th>\n",
       "      <th>n</th>\n",
       "      <th>pct</th>\n",
       "      <th>pct_cumsum</th>\n",
       "      <th>random_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2369/DK3IIH6</td>\n",
       "      <td>113</td>\n",
       "      <td>0.000852</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2369/PHN82</td>\n",
       "      <td>62</td>\n",
       "      <td>0.000468</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2369/PHZ51</td>\n",
       "      <td>108</td>\n",
       "      <td>0.000814</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2FACC/DK3IIH6</td>\n",
       "      <td>40</td>\n",
       "      <td>0.000302</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4N506/DK3IIH6</td>\n",
       "      <td>37</td>\n",
       "      <td>0.000279</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4921</th>\n",
       "      <td>Z037E0054/LH162</td>\n",
       "      <td>10</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4922</th>\n",
       "      <td>Z037E0054/PHZ51</td>\n",
       "      <td>23</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4923</th>\n",
       "      <td>Z038E0057/DK3IIH6</td>\n",
       "      <td>37</td>\n",
       "      <td>0.000279</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4924</th>\n",
       "      <td>Z038E0057/LH162</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4925</th>\n",
       "      <td>Z038E0057/PHZ51</td>\n",
       "      <td>36</td>\n",
       "      <td>0.000271</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4926 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Hybrid    n       pct  pct_cumsum  random_order\n",
       "0          2369/DK3IIH6  113  0.000852         NaN             0\n",
       "1            2369/PHN82   62  0.000468         NaN             0\n",
       "2            2369/PHZ51  108  0.000814         NaN             0\n",
       "3         2FACC/DK3IIH6   40  0.000302         NaN             0\n",
       "4         4N506/DK3IIH6   37  0.000279         NaN             0\n",
       "...                 ...  ...       ...         ...           ...\n",
       "4921    Z037E0054/LH162   10  0.000075         NaN             0\n",
       "4922    Z037E0054/PHZ51   23  0.000173         NaN             0\n",
       "4923  Z038E0057/DK3IIH6   37  0.000279         NaN             0\n",
       "4924    Z038E0057/LH162   11  0.000083         NaN             0\n",
       "4925    Z038E0057/PHZ51   36  0.000271         NaN             0\n",
       "\n",
       "[4926 rows x 5 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pct_cumsum_thresh = .9\n",
    "\n",
    "# make a df to aid in creating train/test splits\n",
    "# the plan is to shuffle the rows of the df, calculate the cumulative sum of the percents obs, then \n",
    "# the entries above and below a given percent will be the train/test.\n",
    "obs_per_hybrid = phno.assign(n = 1).groupby('Hybrid').count().reset_index()\n",
    "obs_per_hybrid = obs_per_hybrid.loc[:, ['Hybrid', 'n']]\n",
    "obs_per_hybrid['pct'] = obs_per_hybrid.n / obs_per_hybrid.n.sum()\n",
    "obs_per_hybrid['pct_cumsum'] = np.nan\n",
    "obs_per_hybrid['random_order'] = 0\n",
    "\n",
    "obs_per_hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ca1bfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hybrid</th>\n",
       "      <th>n</th>\n",
       "      <th>pct</th>\n",
       "      <th>pct_cumsum</th>\n",
       "      <th>random_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1738</th>\n",
       "      <td>MOG_MO45-074-1-1-1-1-B/LH185</td>\n",
       "      <td>13</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1196</th>\n",
       "      <td>M0276/LH198</td>\n",
       "      <td>21</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>0.000256</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>CG102/DK3IIH6</td>\n",
       "      <td>64</td>\n",
       "      <td>0.000483</td>\n",
       "      <td>0.000739</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4735</th>\n",
       "      <td>Z022E0023/PB80</td>\n",
       "      <td>13</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000837</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1647</th>\n",
       "      <td>MO44_PHW65_0416/LH195</td>\n",
       "      <td>47</td>\n",
       "      <td>0.000354</td>\n",
       "      <td>0.001192</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>BGEM-0211-N/LH195</td>\n",
       "      <td>45</td>\n",
       "      <td>0.000339</td>\n",
       "      <td>0.999555</td>\n",
       "      <td>4921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>B73_PHG39-15/LH82</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.999615</td>\n",
       "      <td>4922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2682</th>\n",
       "      <td>PHW52/PHK76</td>\n",
       "      <td>28</td>\n",
       "      <td>0.000211</td>\n",
       "      <td>0.999827</td>\n",
       "      <td>4923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3249</th>\n",
       "      <td>W10001_0013/PHB47</td>\n",
       "      <td>12</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.999917</td>\n",
       "      <td>4924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1330</th>\n",
       "      <td>MO44_LH145_0081/PB80</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4925</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4926 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Hybrid   n       pct  pct_cumsum  random_order\n",
       "1738  MOG_MO45-074-1-1-1-1-B/LH185  13  0.000098    0.000098             0\n",
       "1196                   M0276/LH198  21  0.000158    0.000256             1\n",
       "256                  CG102/DK3IIH6  64  0.000483    0.000739             2\n",
       "4735                Z022E0023/PB80  13  0.000098    0.000837             3\n",
       "1647         MO44_PHW65_0416/LH195  47  0.000354    0.001192             4\n",
       "...                            ...  ..       ...         ...           ...\n",
       "194              BGEM-0211-N/LH195  45  0.000339    0.999555          4921\n",
       "106              B73_PHG39-15/LH82   8  0.000060    0.999615          4922\n",
       "2682                   PHW52/PHK76  28  0.000211    0.999827          4923\n",
       "3249             W10001_0013/PHB47  12  0.000090    0.999917          4924\n",
       "1330          MO44_LH145_0081/PB80  11  0.000083    1.000000          4925\n",
       "\n",
       "[4926 rows x 5 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fill in the random values to sort on\n",
    "arr = np.arange(obs_per_hybrid.shape[0])\n",
    "rng.shuffle(arr)\n",
    "obs_per_hybrid.random_order = arr\n",
    "\n",
    "obs_per_hybrid = obs_per_hybrid.sort_values('random_order')\n",
    "obs_per_hybrid['pct_cumsum'] = obs_per_hybrid.pct.cumsum()\n",
    "obs_per_hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bce9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert back into phno indices\n",
    "train_hybrids = list(obs_per_hybrid.loc[(obs_per_hybrid.pct_cumsum <= pct_cumsum_thresh), 'Hybrid'])\n",
    "test_hybrids  = list(obs_per_hybrid.loc[(obs_per_hybrid.pct_cumsum >  pct_cumsum_thresh), 'Hybrid'])\n",
    "\n",
    "train_idx = phno.loc[(phno.Hybrid.isin(train_hybrids)), ].index\n",
    "test_idx  = phno.loc[(phno.Hybrid.isin(test_hybrids)), ].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d0163e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[119339, 13264]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(train_idx), len(test_idx)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbaedf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm all observation idxs are have genomic information\n",
    "assert [] == [e for e in list(train_idx)+list(test_idx) if e not in obs_geno_lookup[:, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02004189",
   "metadata": {},
   "outputs": [],
   "source": [
    "YMat_cs = calc_cs(YMat[train_idx])\n",
    "y_cs = apply_cs(YMat, YMat_cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d9cd98",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataLoader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m training_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m      2\u001b[0m     ACGTDataset(y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(y_cs[train_idx])[:, \u001b[38;5;28;01mNone\u001b[39;00m]\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat), \n\u001b[1;32m      3\u001b[0m                 G \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(ACGT_OneHot)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat), \n\u001b[1;32m      4\u001b[0m                 idx_original \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39marray(train_idx)),\n\u001b[1;32m      5\u001b[0m                 idx_lookup   \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39masarray(obs_geno_lookup)),\n\u001b[1;32m      6\u001b[0m                 use_gpu_num \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m      7\u001b[0m                 device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      8\u001b[0m                ),\n\u001b[1;32m      9\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m,\n\u001b[1;32m     10\u001b[0m     shuffle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     11\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DataLoader' is not defined"
     ]
    }
   ],
   "source": [
    "training_dataloader = DataLoader(\n",
    "    ACGTDataset(y = torch.from_numpy(y_cs[train_idx])[:, None].to(torch.float), \n",
    "                G = torch.from_numpy(ACGT_OneHot).to(torch.float), \n",
    "                idx_original = torch.from_numpy(np.array(train_idx)),\n",
    "                idx_lookup   = torch.from_numpy(np.asarray(obs_geno_lookup)),\n",
    "                use_gpu_num = 0,\n",
    "                device = 'cuda'\n",
    "               ),\n",
    "    batch_size = 50,\n",
    "    shuffle = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08eaaff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_dataloader = DataLoader(\n",
    "    ACGTDataset(y = torch.from_numpy(y_cs[test_idx])[:, None].to(torch.float), \n",
    "                G = torch.from_numpy(ACGT_OneHot).to(torch.float), \n",
    "                idx_original = torch.from_numpy(np.array(test_idx)),\n",
    "                idx_lookup   = torch.from_numpy(np.asarray(obs_geno_lookup)),\n",
    "                use_gpu_num = 0,\n",
    "                device = 'cuda'\n",
    "               ),\n",
    "    batch_size = 50,\n",
    "    shuffle = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b8b1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(training_dataloader))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788ba1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()    \n",
    "\n",
    "        def Linear_block(in_size, out_size, drop_pr):\n",
    "            block = nn.Sequential(\n",
    "                nn.Linear(in_size, out_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(drop_pr)\n",
    "            )\n",
    "            return(block)         \n",
    "        \n",
    "        self.x_network = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            Linear_block(in_size = 13*125891, out_size = 512, drop_pr = 0.5),\n",
    "            Linear_block(in_size = 512, out_size = 256, drop_pr = 0.5),\n",
    "            Linear_block(in_size = 256, out_size = 128, drop_pr = 0.5),\n",
    "            Linear_block(in_size = 128, out_size = 32, drop_pr = 0.5),\n",
    "            Linear_block(in_size = 32, out_size = 16, drop_pr = 0.5),\n",
    "            nn.Linear(16, 1)\n",
    "        \n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_out = self.x_network(x)\n",
    "        return x_out\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "\n",
    "model(next(iter(training_dataloader))[0][0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654b3bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # small scale test\n",
    "# model, loss_df = train_nn(\n",
    "#     cache_path,\n",
    "#     training_dataloader,\n",
    "#     testing_dataloader,\n",
    "#     model,\n",
    "#     learning_rate = 1e-3,\n",
    "#     batch_size = 50, #dataloader_batch_size,\n",
    "#     epochs = 1 #(run_epochs - epochs_run)\n",
    "# )\n",
    "\n",
    "# loss_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9cbc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimate_iterations(sec_per_it = 161)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3174cd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_batch_size = 50\n",
    "run_epochs = 100\n",
    "\n",
    "# don't run if either of these exist because there may be cases where we want the results but not the model\n",
    "import re\n",
    "\n",
    "if not os.path.exists(cache_path+'/model.pt'): \n",
    "    # Shared setup (train from scratch and load latest)\n",
    "    model = NeuralNetwork()\n",
    "\n",
    "    # find the biggest model to save\n",
    "    saved_models = os.listdir(cache_path)\n",
    "    saved_models = [e for e in saved_models if re.match('model*', e)]\n",
    "\n",
    "    if saved_models == []:\n",
    "        epochs_run = 0\n",
    "    else:\n",
    "        saved_models = [e for e in saved_models if e != 'model.pt']\n",
    "        # if there are saved models reload and resume training\n",
    "        saved_models_numbers = [int(e.replace('model_', ''\n",
    "                                    ).replace('.pt', ''\n",
    "                                    ).split('_')[0]) for e in saved_models]\n",
    "        # saved_models\n",
    "        epochs_run = max(saved_models_numbers)+1 # add 1 to account for 0 index\n",
    "        latest_model = [e for e in saved_models if re.match(\n",
    "            '^model_'+str(epochs_run-1)+'_.*\\.pt$', e)][0] # subtract 1 to convert back\n",
    "        model.load_state_dict(torch.load(cache_path+latest_model))\n",
    "        print('Resuming Training: '+str(epochs_run)+'/'+str(run_epochs)+' epochs run.')\n",
    "    \n",
    "    model.to(device)    \n",
    "\n",
    "    model, loss_df = train_nn(\n",
    "        cache_path,\n",
    "        training_dataloader,\n",
    "        testing_dataloader,\n",
    "        model,\n",
    "        learning_rate = 1e-3,\n",
    "        batch_size = dataloader_batch_size,\n",
    "        epochs = (run_epochs - epochs_run)\n",
    "    )\n",
    "    \n",
    "    # experimental outputs:\n",
    "    # 1. Model\n",
    "    torch.save(model.state_dict(), cache_path+'/model.pt') # convention is to use .pt or .pth\n",
    "\n",
    "    # 2. loss_df    \n",
    "    # If this is resuming training, load and extend the existing loss dataframe\n",
    "    if os.path.exists(cache_path+'/loss_df.csv'):\n",
    "        loss_df_on_disk = pd.read_csv(cache_path+'/loss_df.csv')\n",
    "        epoch_offset = 1 + loss_df_on_disk['Epoch'].max()\n",
    "        loss_df['Epoch'] = loss_df['Epoch'] + epoch_offset\n",
    "        loss_df = pd.concat([loss_df_on_disk, loss_df])\n",
    "    loss_df.to_csv(cache_path+'/loss_df.csv', index=False)  \n",
    "    \n",
    "    # 3. predictions \n",
    "    yhats = pd.concat([\n",
    "        yhat_loop(testing_dataloader, model).assign(Split = 'Test'),\n",
    "        yhat_loop(training_dataloader, model).assign(Split = 'Train')], axis = 0)\n",
    "\n",
    "    yhats.to_csv(cache_path+'/yhats.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4764b227",
   "metadata": {},
   "source": [
    "### Standard Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad680db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_dict = {'y1':YMat_cs}\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec22f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "naieve_yhat = training_dataloader.dataset.y.mean()\n",
    "\n",
    "naieve_MSE_Train = reverse_cs( \n",
    "    np.array(((naieve_yhat - training_dataloader.dataset.y)**2)).mean(),\n",
    "    scale_dict['y1']\n",
    ")\n",
    "\n",
    "naieve_MSE_Test = reverse_cs( \n",
    "    np.array(((naieve_yhat - testing_dataloader.dataset.y)**2)).mean(),\n",
    "    scale_dict['y1']\n",
    ")\n",
    "\n",
    "naieve_MSE_Train, naieve_MSE_Test\n",
    "\n",
    "\n",
    "\n",
    "loss_df = pd.read_csv(cache_path+'/loss_df.csv')\n",
    "\n",
    "loss_df.TrainMSE = reverse_cs(loss_df.TrainMSE, scale_dict['y1'])\n",
    "loss_df.TestMSE  = reverse_cs(loss_df.TestMSE , scale_dict['y1'])\n",
    "\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=loss_df.Epoch, y=loss_df.TestMSE,\n",
    "                    mode='lines', name='Test'))\n",
    "fig.add_trace(go.Scatter(x=loss_df.Epoch, y=loss_df.TrainMSE,\n",
    "                    mode='lines', name='Train'))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=loss_df.Epoch, y=[naieve_MSE_Test  for e in range(len(loss_df.Epoch))], \n",
    "                         mode='lines', name='Naieve Test'))\n",
    "fig.add_trace(go.Scatter(x=loss_df.Epoch, y=[naieve_MSE_Train for e in range(len(loss_df.Epoch))], \n",
    "                         mode='lines', name='Naieve Train'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b66997",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhats = pd.read_csv(cache_path+'/yhats.csv')\n",
    "\n",
    "# px.scatter(yhats, x = 'y_true', y = 'y_pred', color = 'Split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923b6dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhats.y_true = reverse_cs(yhats.y_true, scale_dict['y1'])\n",
    "yhats.y_pred = reverse_cs(yhats.y_pred, scale_dict['y1'])\n",
    "\n",
    "# px.scatter(yhats, x = 'y_true', y = 'y_pred', color = 'Split', trendline=\"ols\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8460bc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhats['Error'] = yhats.y_pred - yhats.y_true\n",
    "\n",
    "px.histogram(yhats, x = 'Error', color = 'Split',\n",
    "             marginal=\"box\", # can be `rug`, `violin`\n",
    "             nbins= 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156b32d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "os._exit(00)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
