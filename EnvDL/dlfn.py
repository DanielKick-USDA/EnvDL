# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/00.02_core_dlfn_Deep_Learning_Convenience_Functions.ipynb.

# %% auto 0
__all__ = ['calc_cs', 'apply_cs', 'reverse_cs', 'read_split_info', 'find_idxs_split_dict', 'train_loop', 'train_error',
           'test_loop', 'yhat_loop', 'train_nn', 'estimate_iterations', 'ACGTDataset', 'train_loop_yx',
           'train_error_yx', 'test_loop_yx', 'train_nn_yx', 'yhat_loop_yx']

# %% ../nbs/00.02_core_dlfn_Deep_Learning_Convenience_Functions.ipynb 3
def calc_cs(x # numeric array
           ): 
    "Calculate nan mean and nan std of an array. Returned as list"
    import numpy as np
    return [np.nanmean(x, axis = 0), np.nanstd(x, axis = 0)]

# %% ../nbs/00.02_core_dlfn_Deep_Learning_Convenience_Functions.ipynb 4
def apply_cs(xs, 
             cs_dict_entry # list of length 2 containing mean and s
            ): return ((xs - cs_dict_entry[0]) / cs_dict_entry[1])

# %% ../nbs/00.02_core_dlfn_Deep_Learning_Convenience_Functions.ipynb 5
def reverse_cs(xs, cs_dict_entry): return (cs_dict_entry[1] * xs) + cs_dict_entry[0]

# %% ../nbs/00.02_core_dlfn_Deep_Learning_Convenience_Functions.ipynb 7
def read_split_info(
    load_from = '../nbs_artifacts/01.06_g2fc_cluster_genotypes/',
    json_prefix = '2023:9:5:12:8:26'):
    ""
    import os, re
    from EnvDL.core import read_json
    
    jsons = [e for e in os.listdir(load_from) if re.match('^'+json_prefix+'.+\.json$', e)]
    vals = [e for e in jsons if re.match('.+val\d+\.json$', e)]
    vals.sort()
    out = {}
    out['test'] = [read_json(json_path = load_from+json_prefix+'-test.json')]
    out['test_file'] = [json_prefix+'-test.json']
    out['validate'] = [read_json(json_path = load_from+val) for val in vals]
    out['validate_files'] = [val for val in vals]
    return(out)


# %% ../nbs/00.02_core_dlfn_Deep_Learning_Convenience_Functions.ipynb 8
def find_idxs_split_dict(
    obs_df, # assumes presence of Year, Female, Male
    split_dict # from read_split_info() output. Should be a test of validate dict.
):
    import pandas as pd
    temp = obs_df
    test_mask = ((temp.Year.isin(split_dict['test_years'])) & 
                 ((temp.Female.isin(split_dict['test_parents'])) |
                  (temp.Male.isin(split_dict['test_parents']))))
    temp['Split'] = ''
    temp.loc[test_mask, 'Split'] = 'Test'

    train_mask = (~(temp.Year.isin(split_dict['test_years'])) & 
                 (~((temp.Female.isin(split_dict['test_parents'])) |
                  (temp.Male.isin(split_dict['test_parents'])))))
    temp.loc[train_mask, 'Split'] = 'Train'

    temp_test  = (temp.Split == 'Test') # should be the same as with the mask above
    temp_train = (temp.Split == 'Train') # should be the same as with the mask above

    # Confirm that there's no overlap in parents or years
    temp_test_parents  = set(temp.loc[temp_test, 'Female']+temp.loc[temp_test, 'Male'])
    temp_train_parents = set(temp.loc[temp_train, 'Female']+temp.loc[temp_train, 'Male'])

    temp_test_years  = set(temp.loc[temp_test, 'Year'])
    temp_train_years = set(temp.loc[temp_train, 'Year'])

    assert [] == [e for e in temp_test_parents if e in temp_train_parents]
    assert [] == [e for e in temp_train_parents if e in temp_test_parents]
    assert [] == [e for e in temp_test_years if e in temp_train_years]
    assert [] == [e for e in temp_train_years if e in temp_test_years]

    return({
        'test_idx': temp.loc[test_mask, ].index, 
        'train_idx': temp.loc[train_mask, ].index} )

# %% ../nbs/00.02_core_dlfn_Deep_Learning_Convenience_Functions.ipynb 11
def train_loop(dataloader, model, loss_fn, optimizer, silent = False):
#     import numpy as np
#     import pandas as pd
    import torch
    from torch.utils.data import Dataset
    from torch.utils.data import DataLoader
#     from torch import nn
    size = len(dataloader.dataset)
    for batch, (xs_i, y_i) in enumerate(dataloader):
        # Compute prediction and loss
        pred = model(xs_i)
        loss = loss_fn(pred, y_i) # <----------------------------------------

        # Backpropagation
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if batch % 100 == 0:
            loss, current = loss.item(), batch * len(y_i) # <----------------
            if not silent:
                print(f"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]")


# %% ../nbs/00.02_core_dlfn_Deep_Learning_Convenience_Functions.ipynb 12
def train_error(dataloader, model, loss_fn, silent = False):
#     import numpy as np
#     import pandas as pd
    import torch
    from torch.utils.data import Dataset
    from torch.utils.data import DataLoader
#     from torch import nn
    size = len(dataloader.dataset)
    num_batches = len(dataloader)
    train_loss = 0

    with torch.no_grad():
        for xs_i, y_i in dataloader:
            pred = model(xs_i)
            train_loss += loss_fn(pred, y_i).item() # <----------------------
            
    train_loss /= num_batches
    return(train_loss) 

# %% ../nbs/00.02_core_dlfn_Deep_Learning_Convenience_Functions.ipynb 13
def test_loop(dataloader, model, loss_fn, silent = False):
#     import numpy as np
#     import pandas as pd
    import torch
    from torch.utils.data import Dataset
    from torch.utils.data import DataLoader
#     from torch import nn

    size = len(dataloader.dataset)
    num_batches = len(dataloader)
    test_loss = 0

    with torch.no_grad():
        for xs_i, y_i in dataloader:
            pred = model(xs_i)
            test_loss += loss_fn(pred, y_i).item() # <-----------------------

    test_loss /= num_batches
    if not silent:
        print(f"Test Error: Avg loss: {test_loss:>8f}")
    return(test_loss) 

# %% ../nbs/00.02_core_dlfn_Deep_Learning_Convenience_Functions.ipynb 14
def yhat_loop(dataloader, model):
    import numpy as np
    import pandas as pd
    import torch
#     from torch.utils.data import Dataset
#     from torch.utils.data import DataLoader
#     from torch import nn

    
    size = len(dataloader.dataset)
    num_batches = len(dataloader)
    
    y_true = np.array([])
    y_pred = np.array([])
    
    with torch.no_grad():
        for xs_i, y_i in dataloader:
            yhat_i = model(xs_i)
            y_pred = np.append(y_pred, np.array(yhat_i.cpu()))
            y_true = np.append(y_true, np.array(y_i.cpu()))
    
    out = np.concatenate([y_true[:, None], y_pred[:, None]], axis = 1) 
    out = pd.DataFrame(out, columns = ['y_true', 'y_pred'])
    return(out)

# %% ../nbs/00.02_core_dlfn_Deep_Learning_Convenience_Functions.ipynb 15
def train_nn(
    cache_path,
    training_dataloader,
    testing_dataloader,
    model,
    learning_rate = 1e-3,
    batch_size = 64,
    epochs = 500,
    model_prefix = 'model'
):
    import numpy as np
    import pandas as pd
    import torch
#     from torch.utils.data import Dataset
#     from torch.utils.data import DataLoader
    from torch import nn
    from tqdm import tqdm
    
    # Initialize the loss function
    loss_fn = nn.MSELoss()
    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)

    loss_df = pd.DataFrame([i for i in range(epochs)], columns = ['Epoch'])
    loss_df['TrainMSE'] = np.nan
    loss_df['TestMSE']  = np.nan

    for t in tqdm(range(epochs)):        
        # print(f"Epoch {t+1}\n-------------------------------")
        train_loop(training_dataloader, model, loss_fn, optimizer, silent = True)

        loss_df.loc[loss_df.index == t, 'TrainMSE'
                   ] = train_error(training_dataloader, model, loss_fn, silent = True)
        
        loss_df.loc[loss_df.index == t, 'TestMSE'
                   ] = test_loop(testing_dataloader, model, loss_fn, silent = True)
        
        if (t+1)%5 == 0: # Cache in case training is interupted. 
            # print(loss_df.loc[loss_df.index == t, ['TrainMSE', 'TestMSE']])
            torch.save(model.state_dict(), 
                       cache_path+'/'+model_prefix+'_'+str(t)+'_'+str(epochs)+'.pt') # convention is to use .pt or .pth
        
    return([model, loss_df])

# %% ../nbs/00.02_core_dlfn_Deep_Learning_Convenience_Functions.ipynb 18
def estimate_iterations(sec_per_it = 161):
    import math
    import pandas as pd
    hours = [1, 2, 4, 8, 12, 24]
    res = pd.DataFrame(zip(hours, 
    [math.floor(
        ((i)*(60*60))/sec_per_it
    ) for i in hours]), columns = ['Hours', 'Iterations'])
    return(res)

# %% ../nbs/00.02_core_dlfn_Deep_Learning_Convenience_Functions.ipynb 20
import torch
from torch.utils.data import Dataset
class ACGTDataset(Dataset): # for any G containing matix with many (phno) to one (geno)
    def __init__(self, 
                 y, 
                 G, # not on gpu
                 idx_original,
                 idx_lookup,
                 transform = None, target_transform = None,
                 use_gpu_num = 0,
                 device = 'cuda',
                 **kwargs 
                ):

        self.device = device
        self.y = y 
        self.G = G
        self.idx_original = idx_original
        self.idx_lookup = idx_lookup
        self.transform = transform
        self.target_transform = target_transform    
        
    def __len__(self):
        return len(self.y)
    
    def __getitem__(self, idx):
        y_idx =self.y[idx]
            
        #                 |array containing correct index in deduplicated g 
        #                 |               index in phno    
        uniq_g_idx = self.idx_lookup[self.idx_original[idx], 1]
        g_idx = self.G[uniq_g_idx, :, :]
        
        # send all to gpu        
        if (self.device != 'cpu'):
            if y_idx.device.type == 'cpu':
                y_idx = y_idx.to(self.device) 
                
            if g_idx.device.type == 'cpu':
                g_idx = g_idx.to(self.device) 
        
        
        if self.transform:
            g_idx = self.transform(g_idx)
            
        if self.target_transform:
            y_idx = self.transform(y_idx)
        return g_idx, y_idx

# %% ../nbs/00.02_core_dlfn_Deep_Learning_Convenience_Functions.ipynb 22
def train_loop_yx(dataloader, model, loss_fn, optimizer, silent = False):
    import torch
    from torch.utils.data import Dataset
    from torch.utils.data import DataLoader
    size = len(dataloader.dataset)
    for batch, (y_i, xs_i) in enumerate(dataloader):
        # Compute prediction and loss
        pred = model(xs_i)
        
        # ensure both are on cuda
        if pred.device.type == 'cpu':
            pred = pred.to('cuda')
        if y_i.device.type == 'cpu':
            y_i = y_i.to('cuda')
        
        loss = loss_fn(pred, y_i)
        
        # Backpropagation
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if batch % 100 == 0:
            loss, current = loss.item(), batch * len(y_i) 
            if not silent:
                print(f"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]")

# %% ../nbs/00.02_core_dlfn_Deep_Learning_Convenience_Functions.ipynb 23
def train_error_yx(dataloader, model, loss_fn, silent = False):
    import torch
    from torch.utils.data import Dataset
    from torch.utils.data import DataLoader
    size = len(dataloader.dataset)
    num_batches = len(dataloader)
    train_loss = 0

    with torch.no_grad():
        for y_i, xs_i in dataloader:
            pred = model(xs_i)
            
            # ensure both are on cuda
            if pred.device.type == 'cpu':
                pred = pred.to('cuda')
            if y_i.device.type == 'cpu':
                y_i = y_i.to('cuda')
            
            train_loss += loss_fn(pred, y_i).item()
            
    train_loss /= num_batches
    return(train_loss)

# %% ../nbs/00.02_core_dlfn_Deep_Learning_Convenience_Functions.ipynb 24
def test_loop_yx(dataloader, model, loss_fn, silent = False):
    import torch
    from torch.utils.data import Dataset
    from torch.utils.data import DataLoader
    
    size = len(dataloader.dataset)
    num_batches = len(dataloader)
    test_loss = 0

    with torch.no_grad():
        for y_i, xs_i in dataloader:
            pred = model(xs_i)
            
            # ensure both are on cuda
            if pred.device.type == 'cpu':
                pred = pred.to('cuda')
            if y_i.device.type == 'cpu':
                y_i = y_i.to('cuda')
                
            test_loss += loss_fn(pred, y_i).item() 

    test_loss /= num_batches
    if not silent:
        print(f"Test Error: Avg loss: {test_loss:>8f}")
    return(test_loss)

# %% ../nbs/00.02_core_dlfn_Deep_Learning_Convenience_Functions.ipynb 25
def train_nn_yx(
    cache_path,
    training_dataloader,
    testing_dataloader,
    model,
    batch_size = 64,
    epochs = 500,
    model_prefix = 'model',
    save_model = False,
    **kwargs # can include 'silent' for train loop or 'save_on' for saving frequency
):
    import numpy as np
    import pandas as pd
    import torch
    from torch import nn
    from tqdm import tqdm
    
    if 'optimizer' not in kwargs:
        optimizer = torch.optim.SGD(model.parameters(), lr=kwargs['learning_rate'])
    else:
        optimizer = kwargs['optimizer']
        
    if 'save_on' in kwargs:
        save_on = kwargs['save_on']
    else:
        save_on = 5       
    
    # Initialize the loss function
    loss_fn = nn.MSELoss()     

    loss_df = pd.DataFrame([i for i in range(epochs)], columns = ['Epoch'])
    loss_df['TrainMSE'] = np.nan
    loss_df['TestMSE']  = np.nan

    for t in tqdm(range(epochs)):        
        if 'silent' in kwargs:
            train_loop_yx(training_dataloader, model, loss_fn, optimizer, silent = kwargs['silent'])
        else:
            train_loop_yx(training_dataloader, model, loss_fn, optimizer, silent = True)

        loss_df.loc[loss_df.index == t, 'TrainMSE'
                   ] = train_error_yx(training_dataloader, model, loss_fn, silent = True)
        
        loss_df.loc[loss_df.index == t, 'TestMSE'
                   ] = test_loop_yx(testing_dataloader, model, loss_fn, silent = True)
        
        if (t+1)%save_on == 0: # Cache in case training is interupted. 
            if save_model:
                torch.save(model.state_dict(), 
                           cache_path+'/'+model_prefix+'_'+str(t)+'_'+str(epochs)+'.pt') # convention is to use .pt or .pth
        
    return([model, loss_df])

# %% ../nbs/00.02_core_dlfn_Deep_Learning_Convenience_Functions.ipynb 26
def yhat_loop_yx(dataloader, model):
    import numpy as np
    import pandas as pd
    import torch

    size = len(dataloader.dataset)
    num_batches = len(dataloader)
    
    y_true = np.array([])
    y_pred = np.array([])
    
    with torch.no_grad():
        for y_i, xs_i in dataloader:
            yhat_i = model(xs_i)
            y_pred = np.append(y_pred, np.array(yhat_i.cpu()))
            y_true = np.append(y_true, np.array(y_i.cpu()))
    
    out = np.concatenate([y_true[:, None], y_pred[:, None]], axis = 1) 
    out = pd.DataFrame(out, columns = ['y_true', 'y_pred'])
    return(out)
